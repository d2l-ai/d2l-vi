%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,11pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax
%% turn off hyperref patch of \index as sphinx.xdy xindy module takes care of
%% suitable \hyperpage mark-up, working around hyperref-xindy incompatibility
\PassOptionsToPackage{hyperindex=false}{hyperref}

\PassOptionsToPackage{warn}{textcomp}

\catcode`^^^^00a0\active\protected\def^^^^00a0{\leavevmode\nobreak\ }
\usepackage{cmap}
\usepackage{fontspec}
\usepackage{amsmath,amssymb,amstext}
\usepackage{polyglossia}
\setmainlanguage{english}



\setmainfont{FreeSerif}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Italic,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldItalic
]
\setsansfont{FreeSans}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]
\setmonofont{FreeMono}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]


\usepackage[Bjarne]{fncychap}
\usepackage[,numfigreset=2,mathnumfig]{sphinx}
\sphinxsetup{verbatimwithframe=false, verbatimsep=2mm, VerbatimColor={rgb}{.95,.95,.95}}
\fvset{fontsize=\small}
\usepackage{geometry}

% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\usepackage{sphinxmessages}
\setcounter{tocdepth}{0}



% Page size
\setlength{\voffset}{-14mm}
\addtolength{\textheight}{16mm}

% Chapter title style
\usepackage{titlesec, blindtext, color}
\definecolor{gray75}{gray}{0.75}
\newcommand{\hsp}{\hspace{20pt}}
\titleformat{\chapter}[hang]{\Huge\bfseries}{\thechapter\hsp\textcolor{gray75}{|}\hsp}{0pt}{\Huge\bfseries}

% So some large pictures won't get the full page
\renewcommand{\floatpagefraction}{.8}

\setcounter{tocdepth}{1}
% Use natbib's citation style, e.g. (Li and Smola, 16)
\usepackage{natbib}
\protected\def\sphinxcite{\citep}





% Remove top header
\usepackage[draft]{minted}
\fvset{breaklines=true, breakanywhere=true}
\setlength{\headheight}{13.6pt}
\makeatletter
    \fancypagestyle{normal}{
        \fancyhf{}
        \fancyfoot[LE,RO]{{\py@HeaderFamily\thepage}}
        \fancyfoot[LO]{{\py@HeaderFamily\nouppercase{\rightmark}}}
        \fancyfoot[RE]{{\py@HeaderFamily\nouppercase{\leftmark}}}
        \fancyhead[LE,RO]{{\py@HeaderFamily }}
     }
\makeatother


\title{Đắm mình vào Học Sâu}
\date{Jan 04, 2020}
\release{0.7.0}
\author{Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{Release}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}



\chapter{Giới thiệu từ nhóm dịch}
\label{\detokenize{intro_vn:gioi-thieu-tu-nhom-dich}}\label{\detokenize{intro_vn::doc}}

\section{Mục tiêu của dự án}
\label{\detokenize{intro_vn:muc-tieu-cua-du-an}}
Trong những năm gần đây, học sâu là một trong các lĩnh vực được quan tâm
nhiều nhất trong các trường đại học cũng như các công ty công nghệ. Ngày
càng nhiều các diễn đàn liên quan đến học máy và học sâu với lượng thành
viên và chủ đề trao đổi ngày một tăng. Một trong các diễn đàn tiếng Việt
nổi bật nhất là \sphinxhref{https://www.facebook.com/groups/machinelearningcoban/}{Forum Machine Learning cơ
bản}%
\begin{footnote}[1]\sphinxAtStartFootnote
\sphinxnolinkurl{https://www.facebook.com/groups/machinelearningcoban/}
%
\end{footnote} và \sphinxhref{https://forum.machinelearningcoban.com/}{Diễn
đàn Machine Learning cơ bản}%
\begin{footnote}[2]\sphinxAtStartFootnote
\sphinxnolinkurl{https://forum.machinelearningcoban.com/}
%
\end{footnote}
với hơn 35 ngàn thành viên và hàng chục chủ đề mới mỗi ngày.

Qua các diễn đàn đó, chúng tôi nhận ra rằng nhu cầu tìm hiểu lĩnh vực
này ngày một tăng trong khi lượng tài liệu tiếng Việt còn rất hạn chế.
Đặc biệt, các tài liệu tiếng Việt còn chưa nhất quán trong cách dịch,
khiến độc giả bối rối trước quá nhiều thông tin nhưng lại quá ít thông
tin đầy đủ. Việc này thúc đẩy chúng tôi tìm và dịch những cuốn sách được
quan tâm nhiều về lĩnh vực này.

Nhóm dịch đã bước đầu thành công khi dịch cuốn \sphinxhref{https://github.com/aivivn/Machine-Learning-Yearning-Vietnamese-Translation/blob/master/chapters/all\_chapters.md}{Machine Learning
Yearning}%
\begin{footnote}[3]\sphinxAtStartFootnote
\sphinxnolinkurl{https://github.com/aivivn/Machine-Learning-Yearning-Vietnamese-Translation/blob/master/chapters/all\_chapters.md}
%
\end{footnote}
của tác giả Andrew Ng. Cuốn sách này đề cập đến các vấn đề cần lưu ý khi
xây dựng các hệ thống học máy, trong đó đề cập đến nhiều kiến thức thực
tế khi thực hiện dự án. Tuy nhiên, cuốn sách này phần nào hướng tới
những người đã có những kinh nghiệm nhất định đã đang tham gia các dự án
học máy. Chúng tôi vẫn khao khát được mang một tài liệu đầy đủ hơn với
đủ kiến thức toán nền tảng, cách triển khai các công thức toán bằng mã
nguồn, cùng với cách triển khai một hệ thống thực tế trên một nền tảng
học sâu được nhiều người sử dụng. Và quan trọng hơn, các kiến thức này
phải cập nhật các xu hướng học máy mới nhất.

Sau nhiều ngày tìm kiếm các cuốn sách về học máy/học sâu được các trường
đại học lớn trên thế giới sử dụng trong quá trình giảng dạy, chúng tôi
quyết định dịch cuốn \sphinxhref{https://www.d2l.ai/}{Dive into Deep Learning}%
\begin{footnote}[4]\sphinxAtStartFootnote
\sphinxnolinkurl{https://www.d2l.ai/}
%
\end{footnote}
của nhóm tác giả từ công ty Amazon. Cuốn này hội tụ đủ các yếu tố: có
giải thích toán dễ hiểu, có code đi kèm cho những bạn muốn thực hành
ngay khi học xong lý thuyết, cập nhật đầy đủ những khía cạnh của học
sâu, và quan trọng nhất là không đòi hỏi bản quyền để dịch. Chúng tôi đã
liên hệ với nhóm tác giả và họ rất vui mừng khi cuốn sách sắp được phổ
biến rộng rãi hơn nữa.

Hiện cuốn sách vẫn đang được thực hiện và sắp ra mắt phiên bản 0.7.0.
Nhóm tác giả có lời khuyên chúng tôi có thể dịch bản 0.7.0 này ở branch
\sphinxhref{https://github.com/d2l-ai/d2l-en/tree/numpy2}{numpy2}%
\begin{footnote}[5]\sphinxAtStartFootnote
\sphinxnolinkurl{https://github.com/d2l-ai/d2l-en/tree/numpy2}
%
\end{footnote} và có thể cập
nhật khi cuốn sách được xuất bản. Chúng tôi cũng chọn bản này vì nó sử
dụng thư viện chính là \sphinxcode{\sphinxupquote{numpy}} (tích hợp trong MXNet), một thư viện xử
lý mảng nhiều chiều phổ biến mà theo chúng tôi, người làm về học máy,
học sâu và khoa học dữ liệu cần biết.

Để có thể thực hiện dịch dự án dịch cuốn sách hơn 800 trang này, chúng
tôi rất cần sự chung tay của cộng đồng. Mọi sự đóng góp đều đáng quý và
sẽ được ghi nhận. Chúng tôi hy vọng cuốn sách sẽ được hoàn thành trong
năm 2020. Và sau đó nó có thể trở thành giáo trình trong các trường đại
học. Hy vọng một ngày chúng ta có thể nhìn thấy một trường của Việt Nam
trong danh sách này:

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics{{48a3d9f792e07e4d02dbc930551d3c826119f62b}.png}
\caption{Danh sách các trường đại học sử dụng cuốn sách này}\label{\detokenize{intro_vn:id1}}\end{figure}


\subsection{Trình tự dịch}
\label{\detokenize{intro_vn:trinh-tu-dich}}
Đây cũng là các nội dung được đề cập trong cuốn sách:
\begin{itemize}
\item {} 
Preface

\item {} 
Introduction

\item {} 
Preliminaries

\item {} 
Mathematics for Deep Learning

\item {} 
Tools for Deep Learning

\item {} 
Linear Neural Networks

\item {} 
Multilayer Perceptrons

\item {} 
Deep Learning Computation

\item {} 
Convolutional Neural Networks

\item {} 
Modern Convolutional Networks

\item {} 
Recurrent Neural Networks

\item {} 
Modern Recurrent Networks

\item {} 
Attention Mechanisms

\item {} 
Optimization Algorithms

\item {} 
Computational Performance

\item {} 
Computer Vision

\item {} 
Natural Language Processing

\item {} 
Recommender Systems

\item {} 
Generative Adversarial Networks

\end{itemize}


\subsection{Diễn đàn}
\label{\detokenize{intro_vn:dien-dan}}
Nội dung cuốn sách này rất phong phú và có nhiều bài tập ở cuối mỗi
phần. Các bạn có thể tham gia thảo luận nội dung và bài tập của cuốn
sách \sphinxhref{https://forum.machinelearningcoban.com/c/d2l}{tại đây}%
\begin{footnote}[6]\sphinxAtStartFootnote
\sphinxnolinkurl{https://forum.machinelearningcoban.com/c/d2l}
%
\end{footnote}.


\section{Hướng dẫn đóng góp}
\label{\detokenize{intro_vn:huong-dan-dong-gop}}
Những việc bạn có thể làm để đóng góp vào dự án:
\begin{itemize}
\item {} 
Tham gia dịch thông qua các Pull Request

\item {} 
Tham gia review các Pull Request

\item {} 
Hỗ trợ kỹ thuật

\item {} 
Sửa các lỗi chính tả, ngữ pháp, những điểm chưa nhất quán trong cách
dịch

\item {} 
Start GitHub repo của dự án

\item {} 
Chia sẻ dự án tới nhiều người hơn

\end{itemize}

Bạn có thể tham gia thảo luận tại \sphinxhref{https://docs.google.com/forms/d/e/1FAIpQLScYforPRBn0oDhqSV\_zTpzkxCAf0F7Cke13QS2tqXrJ8LxisQ/viewform?usp=sf\_link}{Slack của nhóm
dịch}%
\begin{footnote}[7]\sphinxAtStartFootnote
\sphinxnolinkurl{https://docs.google.com/forms/d/e/1FAIpQLScYforPRBn0oDhqSV\_zTpzkxCAf0F7Cke13QS2tqXrJ8LxisQ/viewform?usp=sf\_link}
%
\end{footnote}
hoặc đóng góp trực tiếp trên \sphinxhref{https://github.com/aivivn/d2l-vn}{GitHub
repo}%
\begin{footnote}[8]\sphinxAtStartFootnote
\sphinxnolinkurl{https://github.com/aivivn/d2l-vn}
%
\end{footnote}.

Dưới đây là chi tiết về ba việc quan trọng nhất:


\subsection{Dịch}
\label{\detokenize{intro_vn:dich}}
Mỗi Pull Request liên quan tới việc dịch chỉ dịch một phần của một file
\sphinxcode{\sphinxupquote{.md}} nằm giữa hai dòng:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}!\PYGZhy{}\PYGZhy{} =================== Bắt đầu dịch Phần x ================================ \PYGZhy{}\PYGZhy{}\PYGZgt{}
\end{sphinxVerbatim}

và

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}!\PYGZhy{}\PYGZhy{} =================== Kết thúc dịch Phần x ================================ \PYGZhy{}\PYGZhy{}\PYGZgt{}
\end{sphinxVerbatim}

Việc chia nhỏ một file ra nhiều phần khiến một Pull Request mất không
quá nhiều thời gian trong cả việc thực hiện lẫn review.

(xem ví dụ \sphinxhref{https://github.com/aivivn/d2l-vn/blame/master/chapter\_preface/index\_vn.md\#L1-L47}{tại
đây}%
\begin{footnote}[9]\sphinxAtStartFootnote
\sphinxnolinkurl{https://github.com/aivivn/d2l-vn/blame/master/chapter\_preface/index\_vn.md\#L1-L47}
%
\end{footnote}.)

\sphinxstylestrong{Các bước thực hiện khi dịch một *phần* của một file {}`{}`.md{}`{}`:}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Tham khảo cách \sphinxhref{https://codetot.net/contribute-github/}{đóng góp vào một dự án
GitHub}%
\begin{footnote}[10]\sphinxAtStartFootnote
\sphinxnolinkurl{https://codetot.net/contribute-github/}
%
\end{footnote}

\item {} 
Luôn luôn giữ bản forked của mình cập nhật với bản chính trong repo
này

\item {} 
Tìm các issues liên quan đến việc dịch \sphinxhref{https://github.com/aivivn/d2l-vn/issues}{tại
đây}%
\begin{footnote}[11]\sphinxAtStartFootnote
\sphinxnolinkurl{https://github.com/aivivn/d2l-vn/issues}
%
\end{footnote}.

\item {} 
Dịch và tạo một Pull Request.

\item {} 
Trả lời các bình luận từ các reviewers

\item {} 
Điền tên mình và tên các reviewer có các phản hồi hữu ích (từ góc
nhìn người dịch chính) vào cuối file, mục “Những người thực hiện”.

\end{enumerate}

\sphinxstylestrong{Lưu ý:}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Thuật ngữ Luôn luôn bám sát \sphinxhref{https://github.com/aivivn/d2l-vn/blob/master/glossary.md}{Bảng thuật
ngữ}%
\begin{footnote}[12]\sphinxAtStartFootnote
\sphinxnolinkurl{https://github.com/aivivn/d2l-vn/blob/master/glossary.md}
%
\end{footnote} khi
dịch. Nếu một từ/cụm chưa có trong Bảng thuật ngữ, hãy tạo một Pull
Request riêng đề xuất cách dịch từ/cụm đó.

\item {} 
Giữ đúng format của bản gốc:
\begin{itemize}
\item {} 
Các phần in nghiêng, in đậm

\item {} 
Tiêu đề (số lượng dấu \sphinxcode{\sphinxupquote{\#}} đầu dòng)

\item {} 
Bảng biểu, chú thích cho bảng (dòng phía trên mỗi bảng bắt đầu
bằng dấu \sphinxcode{\sphinxupquote{:}})

\item {} 
Dịch các từ trong hình vẽ nếu cần. Các dòng có hình có dạng:
\sphinxcode{\sphinxupquote{!{[}caption{]}(path)}}

\item {} 
Dịch các chú thích hình vẽ (thay các cụm
\sphinxcode{\sphinxupquote{*dịch chú thích ảnh phía trên*}} bằng bản dịch tiếng Việt)

\item {} 
Không dịch các phần code (nằm giữa hai dấu {}`{}`{}`{}`{}`)

\item {} 
Copy các công thức toán từ bản gốc (các đoạn có \sphinxcode{\sphinxupquote{\$}})

\item {} 
Giữ các dòng gán nhãn (bắt đầu với \sphinxcode{\sphinxupquote{:label:}})

\item {} 
Không tự thêm bớt các dòng trắng

\end{itemize}

\item {} 
Xuống dòng sau mỗi câu. Markdown sẽ coi những dòng liền nhau không có
dòng trắng là một đoạn văn. Việc này giúp công đoạn review được thuận
tiện hơn.

\end{enumerate}


\subsection{Review}
\label{\detokenize{intro_vn:review}}
Chọn một Pull Request trong \sphinxhref{https://github.com/aivivn/d2l-vn/pulls}{danh sách
này}%
\begin{footnote}[13]\sphinxAtStartFootnote
\sphinxnolinkurl{https://github.com/aivivn/d2l-vn/pulls}
%
\end{footnote} và bắt đầu review.

Khi Review, bạn có thể đề xuất thay đổi cách dịch mỗi dòng trực tiếp như
trong hình dưới đây: \begin{center}\sphinxincludegraphics{{5538651c7e70ad6b546215518bfc9f98ba7147c1}.png}\end{center}

Nếu bạn có những phản hồi hữu ích, tên của bạn sẽ được tác giả chính của
Pull Request đó điền vào cuối file mục “Những người thực hiện”.


\subsection{Hỗ trợ kỹ thuật}
\label{\detokenize{intro_vn:ho-tro-ky-thuat}}
Để phục vụ cho việc dịch trên quy mô lớn, nhóm dịch cần một số bạn hỗ
trợ kỹ thuật cho một số việc dưới đây:
\begin{itemize}
\item {} 
Lấy các bản gốc từ \sphinxhref{https://github.com/d2l-ai/d2l-en/tree/numpy2}{bản tiếng
Anh}%
\begin{footnote}[14]\sphinxAtStartFootnote
\sphinxnolinkurl{https://github.com/d2l-ai/d2l-en/tree/numpy2}
%
\end{footnote}. Vì bản này tác
giả vẫn cập nhật nên dịch đến đâu chúng ta sẽ cập nhật đến đó.

\item {} 
Tự động thêm comment vào các bản gốc (\sphinxcode{\sphinxupquote{\textless{}!-{-}}} và \sphinxcode{\sphinxupquote{-{-}\textgreater{}}}) để các
phần này không hiển thị trên \sphinxhref{https://d2l.aivivn.com/}{trang web
chính}%
\begin{footnote}[15]\sphinxAtStartFootnote
\sphinxnolinkurl{https://d2l.aivivn.com/}
%
\end{footnote}. Phần thêm này có thể được thực
hiện tự động bằng cách chạy:

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{python3} \PYG{n}{utils} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{convert} \PYG{o}{\PYGZlt{}}\PYG{n}{path\PYGZus{}to\PYGZus{}file}\PYG{o}{\PYGZgt{}}\PYG{o}{.}\PYG{n}{md}
\end{sphinxVerbatim}

và tạo ra file \sphinxcode{\sphinxupquote{\textless{}path\_to\_file\textgreater{}\_vn.md}}.
\begin{itemize}
\item {} 
Chia các file lớn thành các mục nhỏ như trong \sphinxhref{https://github.com/aivivn/d2l-vn/blame/master/chapter\_preface/index\_vn.md}{ví dụ
này}%
\begin{footnote}[16]\sphinxAtStartFootnote
\sphinxnolinkurl{https://github.com/aivivn/d2l-vn/blame/master/chapter\_preface/index\_vn.md}
%
\end{footnote}.
Phần này cần thực hiện bằng tay. Mỗi phần trong file nên bao gồm
những mục cụ thể, không bắt đầu và kết thúc giữa chừng của một mục.

\item {} 
Dịch các chữ trong hình vẽ theo Bảng thuật ngữ. Sẵn sàng đổi các bản
dịch này nếu Bảng thuật ngữ thay đổi

\item {} 
Hỗ trợ quản lý project trên github, slack và diễn đàn.

\end{itemize}

\sphinxstylestrong{Lưu ý:} Chỉ bắt đầu thực hiện công việc nếu đã có một issue tương ứng
được tạo. Nếu bạn thấy một việc cần thiết, hãy tạo issue và thảo luận
trước khi thực hiện. Tránh việc dẫm lên chân nhau.






\chapter{Lời nói đầu}
\label{\detokenize{chapter_preface/index_vn:loi-noi-dau}}\label{\detokenize{chapter_preface/index_vn::doc}}


Chỉ một vài năm trước, không có nhiều nhà khoa học học sâu (\sphinxstyleemphasis{deep
learning}) phát triển các sản phẩm và dịch vụ thông minh tại các công ty
lớn và công ty khởi nghiệp. Khi người trẻ nhất trong nhóm tác giả chúng
tôi tiến vào lĩnh vực này, học máy (\sphinxstyleemphasis{machine learning}) còn chưa xuất
hiện thường xuyên trên truyền thông. Cha mẹ chúng tôi từng không có ý
niệm gì về học máy chứ chưa nói đến việc hiểu tại sao chúng tôi theo
đuổi lĩnh vực này thay vì y khoa hay luật khoa. Học máy từng là một lĩnh
vực nghiên cứu với chỉ một tập nhỏ các ứng dụng thực tế. Và những ứng
dụng đó, chẳng hạn nhận dạng giọng nói (\sphinxstyleemphasis{speech recognition}) hay thị
giác máy tính (\sphinxstyleemphasis{computer vision}), đòi hỏi quá nhiều kiến thức chuyên
biệt khiến chúng thường được phân thành các lĩnh vực hoàn toàn riêng mà
trong đó học máy chỉ là một thành phần nhỏ. Các mạng nơ-ron (\sphinxstyleemphasis{neural
network}), tiền đề của các mô hình học sâu mà chúng ta tập trung vào
trong cuốn sách này, từng được coi là các công cụ lỗi thời.



Trong chỉ khoảng năm năm gần đây, học sâu đã mang đến nhiều bất ngờ trên
quy mô toàn cầu, dẫn đường cho những tiến triển nhanh chóng trong nhiều
lĩnh vực khác nhau như thị giác máy tính, xử lý ngôn ngữ tự nhiên
(\sphinxstyleemphasis{natural language processing}), nhận dạng giọng nói tự động (\sphinxstyleemphasis{automatic
speech recognition}), học tăng cường (\sphinxstyleemphasis{reinforcement learning}), và mô
hình hoá thống kê (\sphinxstyleemphasis{statistical modeling}). Với những tiến bộ này, chúng
ta bây giờ có thể xây dựng xe tự lái với mức độ tự động ngày càng cao
(nhưng chưa nhiều tới mức như vài công ty đang tuyên bố), hệ thống trả
lời tự động, giúp con người đào sâu vào cả núi email, và các phần mềm
chiến thắng những người giỏi nhất trong các môn cờ như cờ vây, một kỳ
tích từng được xem là không thể đạt được trong nhiều thập kỷ tới. Những
công cụ này đã và đang gây ảnh hưởng rộng rãi tới các ngành công nghiệp
và đời sống xã hội, thay đổi cách tạo ra các bộ phim, cách chẩn đoán
bệnh, đóng một vài trò ngày càng tăng trong các ngành khoa học cơ bản \textendash{}
từ vật lý thiên văn tới sinh học.








\section{Về cuốn sách này}
\label{\detokenize{chapter_preface/index_vn:ve-cuon-sach-nay}}


Cuốn sách này được viết với mong muốn học sâu dễ tiếp cận hơn, dạy bạn
từ \sphinxstyleemphasis{khái niệm}, \sphinxstyleemphasis{bối cảnh}, tới \sphinxstyleemphasis{lập trình}.




\subsection{Một phương tiện truyền tải kết hợp Code, Toán, và HTML}
\label{\detokenize{chapter_preface/index_vn:mot-phuong-tien-truyen-tai-ket-hop-code-toan-va-html}}


Để bất kỳ kỹ thuật tính toán nào đạt được tầm ảnh hưởng sâu rộng, nó
phải dễ hiểu, có tài liệu đầy đủ, và được hỗ trợ bởi nhưng công cụ cấp
tiến được “bảo trì” thường xuyên. Các ý tưởng chính cần được chắt lọc rõ
ràng, tối thiểu thời gian chuẩn bị cần thiết để trang bị kiến thức đương
thời cho những người mới bắt đầu. Các thư viện cấp tiến nên tự động hoá
các tác vụ đơn giản, và các đoạn mã nguồn ví dụ cần phải đơn giản với
những người mới bắt đầu sao cho họ có thể dễ dàng chỉnh sửa, áp dụng, và
mở rộng những ứng dụng thông thường thành các ứng dụng họ cần. Lấy ứng
dụng các trang web động làm ví dụ. Mặc dù các công ty công nghệ lớn, như
Amazon, phát triển thành công các ứng dụng web định hướng bởi cơ sở dữ
liệu từ những năm 1990, tiềm năng của công nghệ này để hỗ trợ các doanh
nghiệp sáng tạo chỉ được nhân rộng lên ở một tầm cao mới từ khoảng mười
năm nay, nhờ vào sự phát triển của các nền tảng mạnh mẽ và với tài liệu
đầy đủ.



Kiểm định tiềm năng của học sâu có những thách thức riêng biệt vì bất kỳ
ứng dụng riêng lẻ nào cũng bao gồm nhiều lĩnh vực khác nhau. Ứng dụng
học sâu đòi hỏi những hiểu biết đồng thời (i) động lực để biến đổi một
bài toán theo một hướng cụ thể; (ii) kiến thức toán học của một hướng
tiếp cận mô hình hoá; (iii) những thuật toán tối ưu cho việc khớp mô
hình với dữ liệu; và (iv) phần kỹ thuật yêu cầu để huấn luyện mô hình
một cách hiệu quả, xử lý những khó khăn trong tính toán và tận dụng thật
tốt phần cứng hiện có. Đào tạo kỹ năng suy nghĩ thấu đáo cần thiết để
định hình bài toán, kiến thức toán để giải chúng, và các công cụ phần
mềm để triển khai những giải pháp đó, tất cả trong một nơi, hàm chứa
nhiều thách thức lớn. Mục tiêu của chúng tôi trong cuốn sách này là
trình bày một nguồn tài liệu tổng hợp giúp những học viên nhanh chóng
bắt kịp.



Chúng tôi bắt đầu dự án sách này từ tháng 7/2017 khi cần trình bày giao
diện MXNet Gluon (khi đó còn mới) tới người dùng. Tại thời điểm đó,
không có một nguồn tài liệu nào vừa đồng thời (i) cập nhật; (ii) bao gồm
đầy đủ các khía cạnh của học máy hiện đại với đầy đủ chiều sâu kỹ thuật;
và (iii) xem kẽ các giải trình mà người ta mong đợi từ một cuốn sách
giáo trình với mã có thể thực thi, điều thường được tìm thấy trong các
bài hướng dẫn thực hành. Chúng tôi tìm thấy một lượng lớn các đoạn mã ví
dụ về việc sử dụng một nền tảng học sâu (ví dụ làm thế nào để thực hiện
các phép toán cơ bản với ma trận trên TensorFlow) hoặc để triển khai
những kỹ thuật cụ thể (ví dụ các đoạn mã cho LeNet, AlexNet, ResNet,…)
dưới dạng một bài blog hoặc trên GitHub. Tuy nhiên, những ví dụ này
thường tập trung vào khía cạnh \sphinxstyleemphasis{làm thế nào} để triển khai một hướng
tiếp cận cho trước, mà bỏ qua các thảo luận về việc \sphinxstyleemphasis{tại sao} một thuật
toán được tạo như thế. Trong khi các chủ đề lẻ tẻ đã được đề cập trong
các bài blog, ví dụ trên trang web \sphinxhref{http://distill.pub}{Distill}%
\begin{footnote}[17]\sphinxAtStartFootnote
\sphinxnolinkurl{http://distill.pub}
%
\end{footnote} hoặc
các blog cá nhân, họ chỉ đề cập đến một vài chủ đề được chọn về học sâu,
và thường thiếu mã nguồn đi kèm. Một mặt khác, trong khi nhiều sách giáo
trình đã ra đời, đáng chú ý nhất là
\DUrole{bibtex}{{[}Goodfellow.Bengio.Courville.2016{]}} (cuốn này cung cấp một bản
khảo sát xuất sắc về các khái niệm phía sau học sâu), những nguồn tài
liệu này lại không đi kèm với việc diễn giải dưới dạng mã nguồn để hiểu
rõ hơn về các khái niệm. Điều này khiến người đọc đôi khi mù tịt về cách
thực thi chúng. Bên cạnh đó, rất nhiều tài liệu lại được cung cấp dưới
dạng các khoá học tốn phí.



Chúng tôi đặt mục tiêu tạo ra một tài liệu mà có thể (1) miễn phí cho
mọi người; (2) cung cấp chiều sâu kỹ thuật đầy đủ tạo điểm bắt đầu cho
con đường trở thành một nhà khoa học học máy ứng dụng; (3) bao gồm mã
thực thi được, trình bày cho người đọc \sphinxstyleemphasis{làm thế nào} giải quyết các bài
toán trên thực tế; (4) tài liệu này có thể cập nhật một cách nhanh
chóng, bằng cả chúng tôi và cộng động ở quy mô lớn; và (5) được bổ sung
bởi một \sphinxhref{http://discuss.mxnet.io}{diễn đàn}%
\begin{footnote}[18]\sphinxAtStartFootnote
\sphinxnolinkurl{http://discuss.mxnet.io}
%
\end{footnote} (và \sphinxhref{https://forum.machinelearningcoban.com/c/d2l}{diễn đàn tiếng
Việt}%
\begin{footnote}[19]\sphinxAtStartFootnote
\sphinxnolinkurl{https://forum.machinelearningcoban.com/c/d2l}
%
\end{footnote} của nhóm dịch)
cho những thảo luận nhanh chóng các chi tiết kỹ thuật và hỏi đáp.



Những mục tiêu này từng có xung đột. Các công thức, định lý, và các
trích dẫn được quản lý tốt nhất trên LaTex. Mã được giải thích tốt nhất
bằng Python. Và trang web phù hợp với HTML và JavaScript. Hơn nữa, chúng
tôi muốn nội dung vừa có thể truy cập được bằng mã nguồn có thể thực
thi, bằng một cuốn sách như một tập tin PDF tải về được, và ở trên
internet như một trang web. Hiện tại không tồn tại công cụ nào phù hợp
một cách hoàn hảo cho những nhu cầu này, bởi vậy chúng tôi phải tự tạo
công cụ cho riêng mình. Chúng tôi mô tả hướng tiếp cận một cách chi tiết
trong \sphinxcode{\sphinxupquote{chapter\_contribute}}. Chúng tôi tổ chức dự án trên
GitHub để chia sẻ mã nguồn và cho phép sửa đổi, Jupyter notebook để kết
hợp mã, các phương trình và nội dung chữ, Sphinx như một bộ máy tạo
nhiều tập tin đầu ra, và Discourse để tạo diễn đàn. Trong khi hệ thống
này còn chưa hoàn hảo, những sự lựa chọn này cung cấp một giải pháp chấp
nhận được trong số các giải pháp tương tự. Chúng tôi tin rằng đây có thể
là cuốn sách đầu tiên được xuất bản dưới dạng kết hợp này.








\subsection{Học bằng cách thực hành}
\label{\detokenize{chapter_preface/index_vn:hoc-bang-cach-thuc-hanh}}


Có nhiều cuốn sách dạy rất chi tiết một chuỗi các chủ đề khác nhau. Ví
dụ như, cuốn sách tuyệt vời \DUrole{bibtex}{{[}Bishop.2006{]}} của Bishop, dạy mỗi
chủ đề rất kỹ lưỡng tới nỗi để đến được chương hồi quy tuyến tính cũng
đòi hỏi công sức bỏ ra không hề nhỏ. Trong khi các chuyên gia yêu thích
quyển sách này chính vì sự kỹ lưỡng của nó, nhưng với những người mới
bắt đầu, thì đặc điểm này lại hạn chế việc dùng cuốn sách như tài liệu
nhập môn.



Trong quyển sách này, chúng tôi sẽ dạy hầu hết các khái niệm \sphinxstyleemphasis{ở mức vừa
đủ}. Hay nói cách khác, bạn sẽ chỉ học các khái niệm tại ngay thời điểm
cần thiết để hoàn tất phần thực hành. Trong khi chúng tôi sẽ dành một
chút thời gian để dạy kiến thức căn bản sơ bộ, như là đại số tuyến tính
và xác suất, chúng tôi muốn các bạn được tận hưởng cảm giác mãn nguyện
của việc huấn luyện mô hình đầu tiên trước khi bận tâm tới các lý thuyết
phân phối xác suất.



Bên cạnh một vài notebook cơ bản cung cấp một khoá học cấp tốc về nền
tảng toán học, mỗi chương tiếp theo sẽ giới thiệu một lượng hợp lý các
khái niệm mới và đồng thời cung cấp các ví dụ đơn hoàn chỉnh—sử dụng các
tập dữ liệu thực tế. Và đây là cả thách thức về cách tổ chức nội dung.
Một vài mô hình có thể được nhóm lại một cách có logic trong một
notebook riêng lẻ. Và một vài ý tưởng có thể được dạy tốt nhất bằng cách
thực thi một số mô hình kế tiếp nhau. Mặt khác, có một lợi thế lớn về
việc tuân thủ theo chính sách \sphinxstyleemphasis{mỗi notebook là một ví dụ hoàn chỉnh}:
Điều này giúp bạn bắt đầu các dự án nghiên cứu của mình một cách dễ dàng
nhất có thể bằng cách tận dụng mã nguồn của chúng tôi. Bạn chỉ cần sao
chép một notebook và bắt đầu sửa đổi trên đó.



Chúng tôi sẽ xen kẽ mã nguồn có thể thực thi với kiến thức nền tảng khi
cần thiết. Thông thường, chúng tôi sẽ tập trung vào việc tạo ra những
công cụ trước khi giải thích chúng đầy đủ (và chúng tôi sẽ theo sát bằng
cách giải thích phần kiến thức nền tảng sau). Ví dụ, chúng tôi có thể sử
dụng \sphinxstyleemphasis{hạ gradient ngẫu nhiên} trước khi giải thích đầy đủ tại sao nó lại
hữu ích hoặc tại sao nó lại hoạt động. Điều này giúp cung cấp cho người
thực hành những phương tiện cần thiết để giải quyết vấn đề nhanh chóng
và đòi hỏi người đọc phải tin tưởng vào một số quyết định triển khai của
chúng tôi.



Xuyên cuốn sách, chúng ta sẽ làm việc với thư viện MXNet, một thư viện
với một đặc tính hiếm có, đó là vừa đủ linh hoạt để nghiên cứu và đủ
nhanh để tạo ra sản phẩm. Cuốn sách này sẽ dạy về khái niệm học sâu từ
đầu. Thỉnh thoảng, chúng tôi sẽ muốn đào sâu hơn vào những chi tiết về
mô hình mà thông thường sẽ được che giấu khỏi người dùng bởi những lớp
trừu tượng bậc cao Gluon. Điều này đặc biệt hay xuất hiện trong các
hướng dẫn cơ bản, nơi chúng tôi muốn bạn hiểu về tất cả mọi thứ đang
diễn ra trong một tầng hoặc bộ tối ưu nào đó. Trong những trường hợp
này, chúng tôi sẽ thường trình bày hai phiên bản của một ví dụ: một
phiên bản trong đó chúng tôi hiện thực mọi thứ từ đầu, chỉ dựa vào giao
diện Numpy và việc tính đạo hàm tự động, và một phiên bản khác, thực tế
hơn, khi chúng tôi viết mã ngắn gọn sử dụng Gluon. Một khi chúng tôi đã
dạy bạn cách một số thành phần hoạt động cụ thể như thế nào, chúng tôi
có thể chỉ sử dụng phiên bản Gluon trong những hướng dẫn tiếp theo.








\subsection{Nội dung và Bố cục}
\label{\detokenize{chapter_preface/index_vn:noi-dung-va-bo-cuc}}


Cuốn sách này có thể được chia thành ba phần, với các phần được thể hiện
bởi những màu khác nhau trong \hyperref[\detokenize{chapter_preface/index_vn:fig-book-org}]{Fig.\@ \ref{\detokenize{chapter_preface/index_vn:fig-book-org}}}:



\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics{{book-org}.pdf}
\caption{Bố cục cuốn sách}\label{\detokenize{chapter_preface/index_vn:id5}}\label{\detokenize{chapter_preface/index_vn:fig-book-org}}\end{figure}


\begin{itemize}
\item {} 
Phần đầu cuốn sách trình bày các kiến thức cơ bản và những việc cần
chuẩn bị sơ bộ. \hyperref[\detokenize{chapter_introduction/index_vn:chap-introduction}]{Section \ref{\detokenize{chapter_introduction/index_vn:chap-introduction}}} giới thiệu về học sâu.
Sau đó, qua \sphinxcode{\sphinxupquote{chap\_preliminaries}}, chúng tôi nhanh chóng
trang bị cho bạn những kiến thức nền cần thiết để thực hành học sâu
như cách lưu trữ, thao tác dữ liệu và cách áp dụng những phép tính
dựa trên những khái niệm cơ bản trong đại số tuyến tính, giải tích và
xác suất. \sphinxcode{\sphinxupquote{chap\_linear}} và \sphinxcode{\sphinxupquote{chap\_perceptrons}}
giới thiệu những khái niệm và kỹ thuật cơ bản của học sâu, ví dụ như
hồi quy tuyến tính, mạng perceptron đa lớp và điều chuẩn.

\end{itemize}


\begin{itemize}
\item {} 
Năm chương tiếp theo tập trung vào những kỹ thuật học sâu hiện đại.
\sphinxcode{\sphinxupquote{chap\_computation}} miêu tả những thành phần thiết yếu của
các phép tính trong học sâu và tạo nền tảng để chúng tôi triển khai
những mô hình phức tạp hơn. Sau đó, chúng tôi sẽ giới thiệu mạng
nơ-ron tích chập (Convolutional neural networks/CNNs), một công cụ
mạnh mẽ đang là nền tảng của hầu hết các hệ thống thị giác máy tính
hiện đại. Tiếp đến, trong \sphinxcode{\sphinxupquote{chap\_rnn}} và
\sphinxcode{\sphinxupquote{chap\_modern\_rnn}}, chúng tôi giới thiệu mạng nơ-ron hồi quy
(recurrent neural networks/RNNs), một loại mô hình khai thác cấu trúc
tạm thời hoặc tuần tự trong dữ liệu và thường được sử dụng để xử lý
ngôn ngữ tự nhiên và dự đoán chuỗi thời gian. Trong
\sphinxcode{\sphinxupquote{chap\_attention}}, chúng tôi giới thiệu một lớp mô hình mới
sử dụng kỹ thuật cơ chế chú ý (attention mechanisms), một kỹ thuật
gần đây đã thay thế RNNs trong xử lý ngôn ngữ tự nhiên. Những phần
này sẽ giúp bạn nhanh chóng nắm được những công cụ cơ bản đứng sau
hầu hết các ứng dụng hiện đại của học sâu.

\end{itemize}


\begin{itemize}
\item {} 
Phần ba thảo luận quy mô mở rộng, hiệu quả và ứng dụng. Đầu tiên,
trong \sphinxcode{\sphinxupquote{chap\_optimization}}, chúng tôi bàn luận một số thuật
toán tối ưu phổ biến được sử dụng để huấn luyện các mô hình học sâu.
Chương tiếp theo, \sphinxcode{\sphinxupquote{chap\_performance}} khảo sát những yếu tố
chính ảnh hưởng đến chất lượng tính toán của mã nguồn học sâu. Trong
\sphinxcode{\sphinxupquote{chap\_cv}} và \sphinxcode{\sphinxupquote{chap\_nlp}}, chúng tôi minh họa lần
lượt những ứng dụng chính của học sâu trong thị giác máy tính và xử
lý ngôn ngữ tự nhiên.

\end{itemize}






\subsection{Mã nguồn}
\label{\detokenize{chapter_preface/index_vn:ma-nguon}}\label{\detokenize{chapter_preface/index_vn:sec-code}}


Hầu hết các phần của cuốn sách đều bao gồm mã nguồn hoạt động được, bởi
vì chúng tôi tin rằng trải nghiệm học bằng cách tương tác đóng vai trò
quan trọng trong học sâu. Hiện tại, một số kinh nghiệm nhất định chỉ có
thể được hình thành thông qua phương pháp thử và sai, thay đổi mã nguồn
từng chút một và quan sát kết quả. Lý tưởng nhất là sử dụng một lý
thuyết toán học đẹp đẽ nào đó có thể cho chúng ta biết chính xác cách
thay đổi mã nguồn để đạt được kết quả mong muốn. Thật đáng tiếc là hiện
tại những lý thuyết đẹp đẽ đó vẫn chưa được khám phá. Mặc dù chúng tôi
đã cố gắng hết sức, nhưng vẫn chưa có cách giải thích trọn vẹn cho nhiều
vấn đề kĩ thuật, bởi vì phần toán học để mô tả những mô hình đấy có thể
là rất khó và công cuộc tìm hiểu về những chủ đề này mới chỉ tăng cao
trong thời gian gần đây. Chúng tôi hi vọng rằng khi mà những lý thuyết
về học sâu phát triển, những phiên bản tiếp theo của quyển sách sẽ có
thể cung cấp những cái nhìn sâu sắc hơn mà phiên bản hiện tại chưa làm
được.



Hầu hết mã nguồn trong cuốn sách được dựa theo Apache MXNet. MXNet là
một framework mã nguồn mở dành cho học sâu và là lựa chọn yêu thích của
AWS (Amazon Web Services), và cả ở nhiều trường đại học và công ty. Tất
cả mã nguồn trong cuốn sách này đã được kiểm thử trên phiên bản mới nhất
của MXNet. Tuy nhiên, bởi vì học sâu phát triển rất nhanh, một vài đoạn
mã \sphinxstyleemphasis{trong phiên bản sách in} có thể không hoạt động chuẩn trên những
phiên bản MXNet sau này. Dù vậy, chúng tôi dự định sẽ giữ phiên bản trực
tuyến luôn được cập nhật. Trong trường hợp bạn gặp phải bất cứ vấn đề
nào, hãy tham khảo {\hyperref[\detokenize{chapter_install/index_vn:chap-installation}]{\sphinxcrossref{\DUrole{std,std-ref}{Cài đặt}}}} (\autopageref*{\detokenize{chapter_install/index_vn:chap-installation}}) để cập nhật mã nguồn và
môi trường thực thi.



Để tránh việc lặp lại không cần thiết, chúng tôi đóng gói những hàm,
lớp,… mà thường xuyên được chèn vào và tham khảo đến trong cuốn sách này
trong gói thư viện \sphinxcode{\sphinxupquote{d2l}}. Đối với bất kì đoạn mã nguồn nào như là một
hàm, một lớp, hoặc các khai báo thư viện cần được đóng gói, chúng tôi sẽ
đánh dấu bằng dòng
\sphinxcode{\sphinxupquote{\# Saved in the d2l package for later use (Lưu lại trong gói thư viện d2l để sử dụng sau)}}.
Thư viện \sphinxcode{\sphinxupquote{d2l}} khá nhẹ và chỉ phụ thuộc vào những gói thư viện và
mô-đun sau:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Saved in the d2l package for later use}
\PYG{k+kn}{import} \PYG{n+nn}{collections}
\PYG{k+kn}{from} \PYG{n+nn}{collections} \PYG{k+kn}{import} \PYG{n}{defaultdict}
\PYG{k+kn}{from} \PYG{n+nn}{IPython} \PYG{k+kn}{import} \PYG{n}{display}
\PYG{k+kn}{import} \PYG{n+nn}{math}
\PYG{k+kn}{from} \PYG{n+nn}{matplotlib} \PYG{k+kn}{import} \PYG{n}{pyplot} \PYG{k}{as} \PYG{n}{plt}
\PYG{k+kn}{from} \PYG{n+nn}{mxnet} \PYG{k+kn}{import} \PYG{n}{autograd}\PYG{p}{,} \PYG{n}{context}\PYG{p}{,} \PYG{n}{gluon}\PYG{p}{,} \PYG{n}{image}\PYG{p}{,} \PYG{n}{init}\PYG{p}{,} \PYG{n}{np}\PYG{p}{,} \PYG{n}{npx}
\PYG{k+kn}{from} \PYG{n+nn}{mxnet}\PYG{n+nn}{.}\PYG{n+nn}{gluon} \PYG{k+kn}{import} \PYG{n}{nn}\PYG{p}{,} \PYG{n}{rnn}
\PYG{k+kn}{import} \PYG{n+nn}{os}
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{import} \PYG{n+nn}{random}
\PYG{k+kn}{import} \PYG{n+nn}{re}
\PYG{k+kn}{import} \PYG{n+nn}{sys}
\PYG{k+kn}{import} \PYG{n+nn}{tarfile}
\PYG{k+kn}{import} \PYG{n+nn}{time}
\PYG{k+kn}{import} \PYG{n+nn}{zipfile}
\end{sphinxVerbatim}



Chúng tôi có một bản tổng quan chi tiết về những hàm và lớp này trong
\sphinxcode{\sphinxupquote{sec\_d2l}}.








\subsection{Đối tượng độc giả}
\label{\detokenize{chapter_preface/index_vn:doi-tuong-doc-gia}}


Cuốn sách này dành cho các bạn sinh viên (đại học hoặc sau đại học), các
kỹ sư và các nhà nghiên cứu, những người tìm kiếm một nền tảng vững chắc
về những kỹ thuật thực tế của học sâu. Bởi vì chúng tôi giải thích mọi
khái niệm từ đầu, bạn không bắt buộc phải có nền tảng về học sâu hay học
máy. Việc giải thích đầy đủ các phương pháp học sâu đòi hỏi một số kiến
thức về toán học và lập trình, nhưng chúng tôi sẽ chỉ giả định rằng bạn
nắm được một số kiến thức cơ bản về đại số tuyến tính, giải tích, xác
suất, và lập trình Python. Hơn nữa, trong phần Phụ lục, chúng tôi cung
cấp thêm về hầu hết các phần toán được đề cập trong cuốn sách này. Phần
lớn thời gian, chúng tôi sẽ ưu tiên dùng cách giải thích trực quan và
các ý tưởng hơn là giải thích chặt chẽ bằng toán. Có rất nhiều cuốn sách
tuyệt vời có thể thu hút bạn đọc quan tâm sâu hơn nữa. Chẳng hạn, cuốn
“Giải tích tuyến tính” (Linear Analysis) của Bela Bollobas
\DUrole{bibtex}{{[}Bollobas.1999{]}} bao gồm cả đại số tuyến tính và giải tích hàm ở
mức độ rất chi tiết. Cuốn “Tất cả về Thống kê” (All of Statistics)
\DUrole{bibtex}{{[}Wasserman.2013{]}} là hướng dẫn tuyệt vời để học thống kê. Và nếu
bạn chưa sử dụng Python, bạn có thể muốn xem cuốn \sphinxhref{http://learnpython.org/}{hướng dẫn
Python}%
\begin{footnote}[20]\sphinxAtStartFootnote
\sphinxnolinkurl{http://learnpython.org/}
%
\end{footnote}.




\subsection{Diễn đàn}
\label{\detokenize{chapter_preface/index_vn:dien-dan}}


Gắn liền với cuốn sách, chúng tôi đã tạo ra một diễn đàn trực tuyến tại
\sphinxhref{https://discuss.mxnet.io/}{discuss.mxnet.io}%
\begin{footnote}[21]\sphinxAtStartFootnote
\sphinxnolinkurl{https://discuss.mxnet.io/}
%
\end{footnote} (và tại \sphinxhref{https://forum.machinelearningcoban.com/c/d2l}{Diễn đàn dó
nhóm dịch tạo}%
\begin{footnote}[22]\sphinxAtStartFootnote
\sphinxnolinkurl{https://forum.machinelearningcoban.com/c/d2l}
%
\end{footnote}). Khi có
câu hỏi về bất kỳ phần nào của cuốn sách, bạn có thể tìm thấy trang thảo
luận liên quan bằng cách quét mã QR ở cuối mỗi chương để tham gia vào
các cuộc thảo luận. Các tác giả của cuốn sách này và rộng hơn là cộng
đồng phát triển MXNet cũng thường xuyên tham gia thảo luận trong diễn
đàn.




\section{Lời cảm ơn}
\label{\detokenize{chapter_preface/index_vn:loi-cam-on}}


Chúng tôi xin gửi lời cảm ơn chân thành tới hàng trăm người đã đóng góp
cho cả hai bản thảo tiếng Anh và tiếng Trung. Mọi người đã giúp cải
thiện nội dung và đưa ra những phản hồi rất có giá trị. Cụ thể, chúng
tôi cảm ơn tất cả những người đóng góp cho dự thảo tiếng Anh này giúp nó
tốt hơn cho tất cả mọi người. Tài khoản GitHub hoặc tên các bạn đóng góp
(không theo trình tự cụ thể nào): alxnorden, avinashingit, bowen0701,
brettkoonce, Chaitanya Prakash Bapat, cryptonaut, Davide Fiocco,
edgarroman, gkutiel, John Mitro, Liang Pu, Rahul Agarwal, Mohamed Ali
Jamaoui, Michael (Stu) Stewart, Mike Müller, NRauschmayr, Prakhar
Srivastav, sad-, sfermigier, Sheng Zha, sundeepteki, topecongiro, tpdi,
vermicelli, Vishaal Kapoor, vishwesh5, YaYaB, Yuhong Chen, Evgeniy
Smirnov, lgov, Simon Corston-Oliver, IgorDzreyev, Ha Nguyen, pmuens,
alukovenko, senorcinco, vfdev-5, dsweet, Mohammad Mahdi Rahimi, Abhishek
Gupta, uwsd, DomKM, Lisa Oakley, Bowen Li, Aarush Ahuja, prasanth5reddy,
brianhendee, mani2106, mtn, lkevinzc, caojilin, Lakshya, Fiete Lüer,
Surbhi Vijayvargeeya, Muhyun Kim, dennismalmgren, adursun, Anirudh
Dagar, liqingnz, Pedro Larroy, lgov, ati-ozgur, Jun Wu, Matthias Blume,
Lin Yuan, geogunow, Josh Gardner, Maximilian Böther, Rakib Islam,
Leonard Lausen, Abhinav Upadhyay, rongruosong, Steve Sedlmeyer, ruslo,
Rafael Schlatter, liusy182, Giannis Pappas, ruslo, ati-ozgur, qbaza,
dchoi77, Adam Gerson. Notably, Brent Werness (Amazon) và Rachel Hu
(Amazon) đồng tác giả chương \sphinxstyleemphasis{Toán học cho Học sâu} trong Phụ lục với
chúng tôi và là những người đóng góp chính cho chương đó.



Chúng tôi cảm ơn Amazon Web Services, đặc biệt là Swami Sivasubramanian,
Raju Gulabani, Charlie Bell, và Andrew Jassy vì sự hỗ trợ hào phóng của
họ trong việc viết cuốn sách này. Nếu không có thời gian, tài nguyên,
thảo luận cùng các đồng nghiệp, cũng như những khuyến khích liên tục,
cuốn sách này sẽ không thể thành hiện thực.




\section{Tóm tắt}
\label{\detokenize{chapter_preface/index_vn:tom-tat}}

\begin{itemize}
\item {} 
Học sâu đã cách mạng hóa nhận dạng mẫu, đưa ra công nghệ cốt lõi hiện
được sử dụng trong nhiều ứng dụng công nghệ, bao gồm thị giác máy, xử
lý ngôn ngữ tự nhiên và nhận dạng giọng nói tự động.

\item {} 
Để áp dụng thành công kĩ thuật học sâu, bạn phải hiểu được cách biến
đổi bài toán, toán học của việc mô hình hóa, các thuật toán để khớp
mô hình theo dữ liệu của bạn, và các kỹ thuật để thực hiện tất cả
những điều này.

\item {} 
Cuốn sách này là một nguồn tài liệu toàn diện, bao gồm các diễn giải,
hình minh hoạ, công thức toán và mã nguồn, tất cả trong một.

\item {} 
Để tìm câu trả lời cho các câu hỏi liên quan đến cuốn sách này, hãy
truy cập diễn đàn của chúng tôi tại \sphinxurl{https://discuss.mxnet.io/}. (Diễn
đàn của nhóm dịch tại \sphinxurl{https://forum.machinelearningcoban.com/c/d2l}).

\item {} 
Apache MXNet là một thư viện mạnh mẽ để lập trình các mô hình học sâu
và chạy chúng song song trên các GPU.

\item {} 
Gluon là một thư viện cấp cao giúp việc viết mã các mô hình học sâu
một cách dễ dàng bằng cách sử dụng Apache MXNet.

\item {} 
Conda là trình quản lý gói Python đảm bảo tất cả các phần mềm phụ
thuộc đều được đáp ứng đủ.

\item {} 
Tất cả các notebook đều có thể tải xuống từ GitHub và các cấu hình
conda cần thiết để chạy mã nguồn của cuốn sách này được viết trong
tệp \sphinxcode{\sphinxupquote{môi trường.yml}}.

\item {} 
Nếu bạn có kế hoạch chạy mã này trên GPU, đừng quên cài đặt các
driver cần thiết và cập nhật cấu hình của bạn.

\end{itemize}




\section{Bài tập}
\label{\detokenize{chapter_preface/index_vn:bai-tap}}

\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Đăng ký tài khoản diễn đàn của cuốn sách tại
\sphinxhref{https://discuss.mxnet.io/}{discussion.mxnet.io}%
\begin{footnote}[23]\sphinxAtStartFootnote
\sphinxnolinkurl{https://discuss.mxnet.io/}
%
\end{footnote}. (Nhóm dịch:
\sphinxurl{https://machinelearningcoban.com}).

\item {} 
Cài đặt Python trên máy tính.

\item {} 
Làm theo hướng dẫn ở các liên kết đến diễn đàn ở cuối phần này, ở các
liên kết diễn đàn đó bạn sẽ có thể nhận được giúp đỡ và thảo luận về
cuốn sách cũng như tìm ra câu trả lời cho câu hỏi của bạn bằng cách
thu hút các tác giả và cộng đồng lớn hơn.

\item {} 
Tạo một tài khoản trên diễn đàn và giới thiệu bản thân.

\end{enumerate}




\section{Thảo luận\sphinxfootnotemark[24]}
\label{\detokenize{chapter_preface/index_vn:thao-luan}}%
\begin{footnotetext}[24]\sphinxAtStartFootnote
\sphinxnolinkurl{https://discuss.mxnet.io/t/2311}
%
\end{footnotetext}\ignorespaces 


\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics{{qr_preface}.pdf}
\caption{\sphinxstyleemphasis{dịch chú thích ảnh phía trên}}\label{\detokenize{chapter_preface/index_vn:id6}}\end{figure}




\subsection{Những người thực hiện}
\label{\detokenize{chapter_preface/index_vn:nhung-nguoi-thuc-hien}}
Bản dịch trong trang này được thực hiện bởi:


\begin{itemize}
\item {} 
Vũ Hữu Tiệp

\end{itemize}


\begin{itemize}
\item {} 
\end{itemize}


\begin{itemize}
\item {} 
Sẩm Thế Hải

\item {} 
Lê Khắc Hồng Phúc

\item {} 
Nguyễn Cảnh Thướng

\end{itemize}


\begin{itemize}
\item {} 
Ngô Thế Anh Khoa

\end{itemize}


\begin{itemize}
\item {} 
\end{itemize}


\begin{itemize}
\item {} 
Trần Thị Hồng Hạnh

\end{itemize}






\chapter{Cài đặt}
\label{\detokenize{chapter_install/index_vn:cai-dat}}\label{\detokenize{chapter_install/index_vn:chap-installation}}\label{\detokenize{chapter_install/index_vn::doc}}


Để sẵn sàng cho những bài học thực hành, bạn cần một môi trường để chạy
Python, Jupyter notebook, các thư viện liên quan và mã nguồn cần thiết
cho những bài tập trong cuốn sách này.




\section{Cài đặt Miniconda}
\label{\detokenize{chapter_install/index_vn:cai-dat-miniconda}}


Cách đơn giản nhất để bắt đầu là cài đặt
\sphinxhref{https://conda.io/en/latest/miniconda.html}{Miniconda}%
\begin{footnote}[25]\sphinxAtStartFootnote
\sphinxnolinkurl{https://conda.io/en/latest/miniconda.html}
%
\end{footnote}. Phiên bản
Python 3.x được khuyên dùng. Bạn có thể bỏ qua những bước sau đây nếu đã
cài đặt conda. Tải về tập tin sh tương ứng của Miniconda từ trang web và
sau đó thực thi phần cài đặt từ command line sử dụng câu lệnh
\sphinxcode{\sphinxupquote{sh \textless{}FILENAME\textgreater{} -b}}. Với người dùng macOS:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} The file name is subject to changes}
sh Miniconda3\PYGZhy{}latest\PYGZhy{}MacOSX\PYGZhy{}x86\PYGZus{}64.sh \PYGZhy{}b
\end{sphinxVerbatim}



Với người dùng Linux:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} The file name is subject to changes}
sh Miniconda3\PYGZhy{}latest\PYGZhy{}Linux\PYGZhy{}x86\PYGZus{}64.sh \PYGZhy{}b
\end{sphinxVerbatim}



Tiếp theo, khởi tạo shell để chạy trực tiếp lệnh \sphinxcode{\sphinxupquote{conda}}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZti{}/miniconda3/bin/conda init
\end{sphinxVerbatim}



Bây giờ, hãy đóng và mở lại shell hiện tại. Bạn đã có thể tạo một môi
trường mới bằng lệnh sau:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
conda create \PYGZhy{}\PYGZhy{}name d2l \PYGZhy{}y
\end{sphinxVerbatim}




\section{Tải về notebook của D2L}
\label{\detokenize{chapter_install/index_vn:tai-ve-notebook-cua-d2l}}


Tiếp theo, ta cần tải về mã nguồn của cuốn sách này. Bạn có thể tải mã
nguồn từ \sphinxhref{https://d2l.ai/d2l-en-0.7.0.zip}{đường dẫn này}%
\begin{footnote}[26]\sphinxAtStartFootnote
\sphinxnolinkurl{https://d2l.ai/d2l-en-0.7.0.zip}
%
\end{footnote} và giải
nén. Một cách khác, nếu bạn đã có cài đặt sẵn \sphinxcode{\sphinxupquote{unzip}} (nếu chưa, hãy
chạy lệnh \sphinxcode{\sphinxupquote{sudo apt install unzip}}):

\begin{sphinxVerbatim}[commandchars=\\\{\}]
mkdir d2l\PYGZhy{}en \PYG{o}{\PYGZam{}\PYGZam{}} \PYG{n+nb}{cd} d2l\PYGZhy{}en
curl https://d2l.ai/d2l\PYGZhy{}en\PYGZhy{}0.7.0.zip \PYGZhy{}o d2l\PYGZhy{}en.zip
unzip d2l\PYGZhy{}en.zip \PYG{o}{\PYGZam{}\PYGZam{}} rm d2l\PYGZhy{}en.zip
\end{sphinxVerbatim}



Bây giờ, ta sẽ kích hoạt môi trường \sphinxcode{\sphinxupquote{d2l}} và cài đặt \sphinxcode{\sphinxupquote{pip}}. Hãy nhập
\sphinxcode{\sphinxupquote{y}} cho phần truy vấn theo sau lệnh này:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
conda activate d2l
conda install \PYG{n+nv}{python}\PYG{o}{=}\PYG{l+m}{3}.7 pip \PYGZhy{}y
\end{sphinxVerbatim}








\section{Cài đặt MXNet và gói thư viện \sphinxstyleliteralintitle{\sphinxupquote{d2l}}}
\label{\detokenize{chapter_install/index_vn:cai-dat-mxnet-va-goi-thu-vien-d2l}}


Trước khi cài đặt MXNet, hãy kiểm tra thiết bị của bạn xem có GPU (card
màn hình) đúng chuẩn hay không (không phải những GPU tích hợp hỗ trợ
hiển thị trên các máy tính xách tay thông thường). Nếu bạn đang cài đặt
trên một máy chủ GPU, hãy tiến hành theo {\hyperref[\detokenize{chapter_install/index_vn:sec-gpu}]{\sphinxcrossref{\DUrole{std,std-ref}{Hỗ trợ GPU}}}} (\autopageref*{\detokenize{chapter_install/index_vn:sec-gpu}}) để cài đặt
phiên bản MXNet có hỗ trợ GPU.



Ngược lại, bạn có thể cài đặt phiên bản chỉ sử dụng CPU. Phiên bản này
cũng thừa đủ để có thể tiến hành các chương đầu tiên nhưng bạn sẽ cần sử
dụng GPU để có thể chạy những mô hình lớn hơn.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} For Windows users}
pip install \PYG{n+nv}{mxnet}\PYG{o}{=}\PYG{o}{=}\PYG{l+m}{1}.6.0b20190926

\PYG{c+c1}{\PYGZsh{} For Linux and macOS users}
pip install \PYG{n+nv}{mxnet}\PYG{o}{=}\PYG{o}{=}\PYG{l+m}{1}.6.0b20191122
\end{sphinxVerbatim}



Ta cũng sẽ cài đặt gói thư viện \sphinxcode{\sphinxupquote{d2l}} mà bao gồm các hàm và lớp thường
xuyên được sử dụng trong cuốn sách này.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
pip install \PYG{n+nv}{d2l}\PYG{o}{=}\PYG{o}{=}\PYG{l+m}{0}.11.0
\end{sphinxVerbatim}



Một khi đã cài đặt xong, ta mở notebook Jupyter lên bằng cách chạy lệnh
sau:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
jupyter notebook
\end{sphinxVerbatim}



Bây giờ, bạn có thể truy cập vào địa chỉ \sphinxurl{http://localhost:8888} (thường
sẽ được tự động mở) trên trình duyệt Web. Sau đó ta đã có thể chạy mã
nguồn trong từng phần của cuốn sách này. Lưu ý là luôn luôn thực thi
lệnh \sphinxcode{\sphinxupquote{conda activate d2l}} để kích hoạt môi trường trước khi chạy mã
nguồn trong sách cũng như khi cập nhật MXNet hoặc gói thư viện \sphinxcode{\sphinxupquote{d2l}}.
Thực thi lệnh \sphinxcode{\sphinxupquote{conda deactivate}} để thoát khỏi môi trường.




\section{Nâng cấp lên Phiên bản Mới}
\label{\detokenize{chapter_install/index_vn:nang-cap-len-phien-ban-moi}}


Cả cuốn sách này và MXNet đều đang tiếp tục được cải thiện. Thỉnh
thoảng, hãy kiểm tra xem đã có phiên bản mới hay chưa.


\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Đường dẫn \sphinxurl{https://d2l.ai/d2l-en.zip} luôn luôn trỏ đến phiên bản mới
nhất.

\item {} 
Để cập nhật gói thư viện \sphinxcode{\sphinxupquote{d2l}} hãy sử dụng lệnh
\sphinxcode{\sphinxupquote{pip install d2l -{-}upgrade}}.

\item {} 
Đối với phiên bản CPU, MXNet có thể được cập nhật bằng lệnh
\sphinxcode{\sphinxupquote{pip install -U -{-}pre mxnet}}.

\end{enumerate}








\section{Hỗ trợ GPU}
\label{\detokenize{chapter_install/index_vn:ho-tro-gpu}}\label{\detokenize{chapter_install/index_vn:sec-gpu}}


Phiên bản MXNet mặc định được cài đặt không hỗ trợ GPU để đảm bảo có thể
chạy trên bất kỳ máy tính nào (bao gồm phần lớn các máy tính xách tay).
Một phần của cuốn sách này yêu cầu hoặc khuyến khích chạy trên GPU. Nếu
máy tính của bạn có card đồ hoạ của NVIDIA và đã cài đặt
\sphinxhref{https://developer.nvidia.com/cuda-downloads}{CUDA}%
\begin{footnote}[27]\sphinxAtStartFootnote
\sphinxnolinkurl{https://developer.nvidia.com/cuda-downloads}
%
\end{footnote}, thì bạn nên cài
đặt bản MXNet có hỗ trợ GPU. Trong trường hợp bạn đã cài đặt phiên bản
dành riêng cho CPU, bạn có thể cần xoá nó trước bằng cách chạy lệnh:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
pip uninstall mxnet
\end{sphinxVerbatim}



Sau đó, ta cần tìm phiên bản CUDA mà bạn đã cài đặt. Bạn có thể kiểm tra
thông qua lệnh \sphinxcode{\sphinxupquote{nvcc -{-}version}} hoặc
\sphinxcode{\sphinxupquote{cat /usr/local/cuda/version.txt}}. Giả sử, bạn đã cài đặt CUDA 10.1,
bạn có thể cài đặt MXNet với lệnh sau:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Dành cho người dùng Windows}
pip install mxnet\PYGZhy{}cu101\PYG{o}{=}\PYG{o}{=}\PYG{l+m}{1}.6.0b20190926

\PYG{c+c1}{\PYGZsh{} Dành cho người dùng Linux và macOS}
pip install mxnet\PYGZhy{}cu101\PYG{o}{=}\PYG{o}{=}\PYG{l+m}{1}.6.0b20191122
\end{sphinxVerbatim}



Tương tự phiên bản CPU, MXNet hỗ trợ GPU có thể được nâng cấp bằng lệnh
\sphinxcode{\sphinxupquote{pip install -U -{-}pre mxnet-cu101}}. Bạn có thể thay đổi các chữ số
cuối theo phiên bản CUDA của bạn, ví dụ, \sphinxcode{\sphinxupquote{cu100}} cho CUDA phiên bản
10.0 và \sphinxcode{\sphinxupquote{cu90}} cho CUDA phiên bản 9.0. Bạn có thể tìm thấy tất cả các
phiên bản MXNet có sẵn thông qua lệnh \sphinxcode{\sphinxupquote{pip search mxnet}}.




\section{Bài tập}
\label{\detokenize{chapter_install/index_vn:bai-tap}}

\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Tải xuống mã nguồn dành cho cuốn sách và cài đặt môi trường chạy.

\end{enumerate}




\section{Thảo luận\sphinxfootnotemark[28]}
\label{\detokenize{chapter_install/index_vn:thao-luan}}%
\begin{footnotetext}[28]\sphinxAtStartFootnote
\sphinxnolinkurl{https://discuss.mxnet.io/t/2315}
%
\end{footnotetext}\ignorespaces 


\begin{center}\sphinxincludegraphics{{qr_install}.pdf}\end{center}




\subsection{Những người thực hiện}
\label{\detokenize{chapter_install/index_vn:nhung-nguoi-thuc-hien}}
Bản dịch trong trang này được thực hiện bởi:


\begin{itemize}
\item {} 
Phạm Hồng Vinh

\item {} 
Sâm Thế Hải

\item {} 
Nguyễn Cảnh Thướng

\item {} 
Lê Khắc Hồng Phúc

\item {} 
Đoàn Võ Duy Thanh

\end{itemize}






\chapter{Giới thiệu}
\label{\detokenize{chapter_introduction/index_vn:gioi-thieu}}\label{\detokenize{chapter_introduction/index_vn:chap-introduction}}\label{\detokenize{chapter_introduction/index_vn::doc}}


Mãi tới tận gần đây, gần như tất cả mọi chương trình máy tính mà chúng
ta tương tác hàng ngày đều được tạo ra bởi lập trình viên phần mềm từ
những định đề cơ bản. Giả sử chúng ta muốn viết một ứng dụng quản lý hệ
thống thương mại điện tử. Sau khi túm tụm lại xung quanh chiếc bảng
trắng để suy nghĩ về vấn đề một cách cặn kẽ, chúng ta có thể phác thảo
một giải pháp vận hành được, phần nào sẽ nhìn giống như sau: (i) người
dùng tương tác với ứng dụng thông qua một giao diện chạy trên trình
duyệt web hoặc ứng dụng trên điện thoại; (ii) ứng dụng tương tác với một
hệ thống cơ sở dữ liệu thương mại để theo dõi trạng thái của từng người
dùng và duy trì hồ sơ lịch sử các giao dịch; và (iii) (cũng là cốt lõi
của ứng dụng) các logic nghiệp vụ (hay cũng có thể nói \sphinxstyleemphasis{bộ não}) mô tả
cách thức xử lí cụ thể của ứng dụng trong từng tình huống có thể xảy ra.



Để xây dựng \sphinxstyleemphasis{bộ não} của ứng dụng này, ta phải xem xét tất cả mọi trường
hợp mà chúng ta cho rằng sẽ gặp phải , qua đó đặt ra những quy tắc thích
hợp. Ví dụ, mỗi lần người dùng nhấn để thêm một món đồ vào giỏ hàng, ta
thêm một trường vào bảng giỏ hàng trong cơ sở dữ liệu, liên kết ID của
người dùng với ID của món hàng được yêu cầu. Mặc dù hầu như rất ít lập
trình viên có thể làm đúng hết trong lần đầu tiên, (sẽ cần vài lần chạy
kiểm tra để xử lý hết được những trường hợp hiểm hóc), hầu như phần lớn
ta có thể lập trình được từ những định đề cơ bản và tự tin chạy ứng dụng
\sphinxstyleemphasis{trước khi được dùng bởi một khách hàng thực sự nào}. Khả năng phát
triển những sản phầm và hệ thống tự động từ những định đề cơ bản, thường
là trong những điều kiện mới lạ, là một kì công trong suy luận và nhận
thức của con người. Và khi mà bạn có thể tạo ra một giải pháp mà có thể
hoạt động được trong mọi tình huống, \sphinxstyleemphasis{bạn không nên sử dụng học máy}.



May mắn thay cho cộng đồng đang tăng trưởng của các nhà khoa học về học
máy, nhiều tác vụ mà chúng ta muốn tự động hoá không dễ dàng bị khuất
phục bởi sự tài tình của con người. Thử tưởng tượng bạn đang quây quần
bên tấm bảng trắng với những bộ não thông minh nhất mà bạn biết, nhưng
lần này bạn đang đương đầu với một trong những vấn đề dưới đây:


\begin{itemize}
\item {} 
Viết một chương trình dự báo thời tiết ngày mai, cho biết trước thông
tin địa lý, hình ảnh vệ tinh, và một chuỗi dữ liệu thời tiết trong
quá khứ.

\item {} 
Viết một chương trình lấy đầu vào là một câu hỏi, được diễn đạt không
theo khuôn mẫu nào, và trả lời nó một cách chính xác.

\item {} 
Viết một chương trình hiển thị ra cho người dùng những sản phẩm mà họ
có khả năng cao sẽ thích, nhưng lại ít có khả năng gặp được khi duyệt
qua môt cách tự nhiên.

\end{itemize}



Trong mỗi trường hợp trên, cho dù có là lập trình viên thượng thừa cũng
không thể lập trình lên được từ con số không. Có nhiều lý do khác nhau.
Đôi khi chương trình mà chúng ta cần lại đi theo một khuôn mẫu thay đổi
theo thời gian, và chương trình của chúng ta cần phải thích ứng. Trong
trường hợp khác, mối quan hệ (giả dụ như giữa các điểm ảnh và các hạng
mục trừu tượng) có thể là quá phức tạp, yêu cầu hàng ngàn hàng triệu các
phép tính vượt ra khỏi khả năng thấu hiểu của nhận thức chúng ta (mặc dù
mắt của chúng ta có thể xử lý tác vụ này một cách dễ dàng). Học máy
(Machine Learning - ML) là lĩnh vực nghiên cứu những kĩ thuật tiên tiến
mà có thể \sphinxstyleemphasis{học} từ \sphinxstyleemphasis{kinh nghiệm}. Khi thuật toán ML tích luỹ thêm nhiều
kinh nghiệm, thường là dưới dạng dữ liệu quan sát hoặc tương tác với môi
trường, chất lượng của nó sẽ tăng lên. Tương phản với hệ thống thương
mại điện tử tất định của chúng ta, khi mà nó luôn tuân theo cùng logic
nghiệp vụ đã có, mặc cho đã tích luỹ thêm bao nhiêu là kinh nghiệm, tận
cho tới khi lập trình viên tự quyết định rằng đã tới lúc cập nhật phần
mềm này. Trong cuốn sách này, chúng tôi sẽ dạy cho bạn về những điều căn
bản nhất trong học máy, và tập trung đặc biệt vào học sâu, một tập hợp
hùng mạnh những kĩ thuật đang thúc đẩy sự đổi mới ở nhiều lĩnh vực khác
nhau như thị giác máy tính, xử lý ngôn ngữ tự nhiên, chăm sóc y tế và
nghiên cứu cấu trúc gen.








\section{Một ví dụ tạo động lực}
\label{\detokenize{chapter_introduction/index_vn:mot-vi-du-tao-dong-luc}}


Trước khi có thể bắt đầu viết, những tác giả của cuốn sách này, giống
nhiều người đi làm khác, cần phải uống cà phê. Chúng tôi leo lên xe và
bắt đầu lái. Sử dụng một chiếc iPhone, Alex nói “Hey Siri” để đánh thức
hệ thống nhận dạng giọng nói của điện thoại. Sau đó Mu ra lệnh “chỉ
đường đến quán cà phê Blue Bottle”. Chiếc điện thoại nhanh chóng hiển
thị bản ghi thoại của câu lệnh đó. Nó cũng nhận ra rằng chúng tôi đang
yêu cầu chỉ dẫn đường đi và khởi động ứng dụng Bản đồ để hoàn thành yêu
cầu của chúng tôi. Khi đã khởi động xong, ứng dụng Bản đồ xác định một
vài tuyến đường đến đó. Kế bên mỗi tuyến đường, điện thoại hiển thị con
số thời gian di chuyển dự tính. Trong khi chúng tôi bịa ra câu chuyện
này để tiện cho việc giảng dạy, điều này cho thấy rằng chỉ trong khoảng
vài giây, những tương tác hàng ngày của chúng ta với một chiếc điện
thoại thông minh có thể liên quan đến nhiều mô hình học máy.



Tưởng tượng rằng ta mới viết một chương trình để phản hồi một \sphinxstyleemphasis{hiệu lệnh
đánh thức} như là “Alexa”, “Okay, Google” hoặc “Siri”. Hãy thử viết nó
chỉ một mình bạn không có gì ngoài một chiếc máy tính và ứng dụng soạn
thảo mã nguồn, như được minh hoạ trong \hyperref[\detokenize{chapter_introduction/index_vn:fig-wake-word}]{Fig.\@ \ref{\detokenize{chapter_introduction/index_vn:fig-wake-word}}}. Bạn sẽ
viết một chương trình như vậy từ những định đề cơ bản như thế nào? Thử
nghĩ về nó… vấn đề này khó quá. Mỗi giây, chiếc micro sẽ thu thập cỡ tầm
44,000 mẫu. Mỗi mẫu là một phép đo biên độ của sóng âm. Quy tắc nào có
thể ánh xạ một cách tin cậy từ một đoạn âm thanh thô đến các dự đoán
\sphinxcode{\sphinxupquote{\{có, không\}}} để xác định đoạn âm thanh đó có chứa hiệu lệnh đánh thức
hay không? Nếu bạn không biết xử lý điều này như thế nào, đừng lo lắng.
Chúng tôi cũng không biết làm cách nào để viết một chương trình như vậy
từ đầu. Đó là lý do vì sao chúng tôi sử dụng học máy.



\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics{{wake-word}.pdf}
\caption{Xác định một hiệu lệnh đánh thức}\label{\detokenize{chapter_introduction/index_vn:id9}}\label{\detokenize{chapter_introduction/index_vn:fig-wake-word}}\end{figure}



Và sau đây là thủ thuật. Thậm chí ngay cả khi chúng ta không thể nói cho
một cái máy tính biết cách để ánh xạ từ đầu vào đến đầu ra như thế nào,
thường chúng ta vẫn có khả năng làm việc đó bằng bộ não của mình. Hay
nói cách khác, thậm chí nếu chúng ta không biết \sphinxstyleemphasis{cách lập trình một cái
máy tính} để nhận dạng từ “Alexa”, chính chúng ta lại \sphinxstyleemphasis{có khả năng} để
nhận thức được từ “Alexa”. Với khả năng này, chúng ta có thể thu thập
một \sphinxstyleemphasis{tập dữ liệu} lớn các mẫu âm thanh kèm nhãn mà \sphinxstyleemphasis{có chứa} hoặc \sphinxstyleemphasis{không
có chứa} hiệu lệnh đánh thức. Trong cách tiếp cận học máy, chúng ta
không thiết kế một hệ thống \sphinxstyleemphasis{rõ ràng} để nhận dạng hiệu lệnh đánh thức.
Thay vào đó, chúng ta định nghĩa ra một chương trình linh hoạt có những
hành vi được xác định bởi những \sphinxstyleemphasis{tham số}. Sau đó chúng ta sử dụng tập
dữ liệu để xác định tập hợp các tham số tốt nhất có thể, mà sẽ cải thiện
được hiệu suất của chương trình thoả mãn một số yêu cầu về hiệu suất
trong nhiệm vụ được giao.



Bạn có thể xem những tham số như là các núm quay mà ta có thể điều
chỉnh, để thay đổi hành vi của chương trình. Khi đã cố định các tham số,
chúng ta gọi chương trình này là một \sphinxstyleemphasis{mô hình}. Tập hợp của tất cả các
chương trình khác nhau (ánh xạ đầu vào-đầu ra) mà chúng ta có thể tạo ra
chỉ bằng cách thay đổi các tham số được gọi là một \sphinxstyleemphasis{nhóm} các mô hình.
Và \sphinxstyleemphasis{siêu chương trình} mà sử dụng tập dữ liệu của chúng ta để chọn ra
các tham số được gọi là \sphinxstyleemphasis{thuật toán học}.







Trước khi tiếp tục và bắt đầu với các thuật toán học, chúng ta phải xác
định vấn đề rõ ràng, hiểu chính xác bản chất của đầu vào và đầu ra, và
lựa chọn một loại mô hình thích hợp. Trong trường hợp này, mô hình của
chúng ta nhận \sphinxstyleemphasis{đầu vào} là một đoạn âm thanh, và \sphinxstyleemphasis{đầu ra} là một giá trị
giữa \sphinxcode{\sphinxupquote{\{đúng, sai\}}} Nếu tất cả diễn ra như kế hoạch, mô hình thường dự
đoán chính xác liệu đoạn âm thanh có chứa hiệu lệnh kích hoạt hay không.



Nếu chúng ta lựa chọn đúng loại mô hình, sẽ tồn tại một cách thiết lập
các núm quay mà mô hình sẽ đưa ra \sphinxcode{\sphinxupquote{đúng}} mỗi khi nghe thấy từ “Alexa”.
Bởi vì việc lựa chọn hiệu lệnh đánh thức nào là tuỳ ý, ta có thể sẽ muốn
có một loại mô hình đủ mạnh để với một thiết lập khác của các núm quay,
nó sẽ đưa ra \sphinxcode{\sphinxupquote{đúng}} mỗi khi nghe từ “Apricot (quả mơ)”. Bằng trực giác
ta có thể nhận thấy rằng việc \sphinxstyleemphasis{nhận dạng “Alexa”} và \sphinxstyleemphasis{nhận dạng
“Apricot”} là tương tự nhau, có thể sử dụng chung một loại mô hình. Tuy
nhiên, trong trường hợp có sự khác biệt về bản chất ở đầu vào và đầu ra,
chẳng hạn như việc ánh xạ từ hình ảnh sang chú thích, hoặc từ câu tiếng
Anh sang câu tiếng Trung thì ta có thể sẽ phải sử dụng các loại mô hình
hoàn toàn khác nhau.



Dễ dàng nhận thấy, nếu như chúng ta chỉ thiết lập một cách ngẫu nhiên
các núm quay, mô hình sẽ hầu như không có khả năng nhận dạng “Alexa”,
“Apricot”, hay bất cứ từ tiếng Anh nào. Trong học sâu, \sphinxstyleemphasis{học} là quá
trình khám phá ra thiết lập đúng của các núm quay để mô hình có thể hành
xử như chúng ta mong muốn.



Quá trình huấn luyện thường giống như mô tả trong hình
\hyperref[\detokenize{chapter_introduction/index_vn:fig-ml-loop}]{Fig.\@ \ref{\detokenize{chapter_introduction/index_vn:fig-ml-loop}}} :


\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Khởi tạo mô hình một cách ngẫu nhiên chưa thể thực hiện tác vụ có ích
nào.

\item {} 
Thu thập một số dữ liệu đã được gán nhán (ví dụ., đoạn âm thanh kèm
nhãn {}`\sphinxcode{\sphinxupquote{\{đúng, sai\}}} tương ứng).

\item {} 
Thay đổi các núm quay để mô hình dự đoán chính xác hơn trên những mẫu
đó.

\item {} 
Lặp lại cho đến khi có một mô hình tuyệt vời.

\end{enumerate}



\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics{{ml-loop}.pdf}
\caption{Một quá trình huấn luyện điển hình}\label{\detokenize{chapter_introduction/index_vn:id10}}\label{\detokenize{chapter_introduction/index_vn:fig-ml-loop}}\end{figure}







Tóm lại, thay vì tạo ra một chương trình nhận dạng từ đánh thức, ta tạo
ra một chương trình có thể \sphinxstyleemphasis{học} cách nhận dạng các từ đánh thức, \sphinxstyleemphasis{khi
được cho xem một tập lớn những ví dụ đã được gán nhãn}. Ta có thể gọi
việc xác định hành vi của một chương trình bằng cách cho nó xem một tập
dữ liệu là \sphinxstyleemphasis{lập trình với dữ liệu}. Chúng ta có thể “lập trình” một bộ
phát hiện mèo bằng cách cung cấp cho hệ thống học máy rất nhiều mẫu ảnh
chó và mèo, ví dụ như trong hình dưới đây:




\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxincludegraphics{{cat1}.png}
&\sphinxstyletheadfamily 
\sphinxincludegraphics{{cat2}.jpg}
&\sphinxstyletheadfamily 
\sphinxincludegraphics{{dog1}.jpg}
&\sphinxstyletheadfamily 
\sphinxincludegraphics{{dog2}.jpg}
\\
\hline
mèo
&
mèo
&
chó
&
chó
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}



Bằng cách này bộ phát hiện sẽ dần học cách trả về một số dương lớn nếu
đó là một con mèo, hoặc một số âm lớn nếu đó là một con chó, hoặc một
giá trị gần với không nếu nó không chắc chắn. Đấy mới chỉ là ví dụ nhỏ
về những gì mà học máy có thể làm được.



Học sâu chỉ là một trong nhiều phương pháp phổ biến để giải quyết những
bài toán học máy. Tới giờ chúng ta mới chỉ nói tổng quát về học máy chứ
chưa nói về học sâu. Để thấy được tại sao học sâu lại quan trọng, ta nên
dừng lại một chút để làm rõ một vài điểm thiết yếu.



Thứ nhất, những vấn đề mà chúng ta đã thảo luận —học từ tín hiệu âm
thanh thô, từ những giá trị điểm ảnh của tấm ảnh, hoặc dịch những câu có
độ dài bất kỳ sang một ngôn ngữ khác— là những vấn đề học sâu có thể xử
lý tốt còn học máy thì không. Mô hình sâu thực sự \sphinxstyleemphasis{sâu} theo nghĩa nó có
thể học nhiều \sphinxstyleemphasis{tầng} tính toán. Những mô hình đa tầng (có thứ bậc) này
có khả năng xử lý dữ liệu tri giác mức thấp theo cái cách mà những công
cụ trước đây không thể. Trước đây, một phần quan trọng trong việc áp
dụng học máy vào các bài toán này là tìm ra những kỹ thuật thủ công để
biến đổi dữ liệu sang một hình thức nào đó mà những mô hình \sphinxstyleemphasis{nông} có
thể cáng đáng. Thứ hai, bằng cách thay thế các kỹ thuật “tiền xử lý theo
từng phân ngành”, học sâu đã loại bỏ ranh giới giữa thị giác máy tính,
nhận dạng tiếng nói, xử lý ngôn ngữ tự nhiên, tin học y tế và các lĩnh
vực khác. Học sâu cung cấp một tập hợp các công cụ xử lý những loại bài
toán khác nhau.








\section{Các thành phần chính: Dữ liệu, Mô hình và Thuật toán}
\label{\detokenize{chapter_introduction/index_vn:cac-thanh-phan-chinh-du-lieu-mo-hinh-va-thuat-toan}}


Trong ví dụ về \sphinxstyleemphasis{từ đánh thức}, chúng tôi đã mô tả một bộ dữ liệu bao gồm
các đoạn âm thanh và các nhãn nhị phân, giúp các bạn hiểu một cách chung
chung về cách chúng ta có thể \sphinxstyleemphasis{huấn luyện} một mô hình để phân loại các
đoạn âm thanh. Với loại bài toán này, chúng tôi cố gắng dự đoán một
\sphinxstyleemphasis{nhãn} chưa biết với \sphinxstyleemphasis{đầu vào} cho trước, dựa trên tập dữ liệu cho trước
bao gồm các mẫu đã được gán nhãn. Đây là ví dụ về bài toán \sphinxstyleemphasis{học có giám
sát}, đây chỉ là một trong số rất nhiều \sphinxstyleemphasis{dạng} bài toán học máy khác
nhau mà chúng ta sẽ học tới trong các chương sau. Đầu tiên, chúng tôi
muốn làm rõ hơn về một số thành phần cốt lõi sẽ theo chúng ta xuyên suốt
tất cả các bài toán học máy:


\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxstyleemphasis{Dữ liệu} mà chúng ta có thể học.

\item {} 
Một \sphinxstyleemphasis{mô hình} về cách biến đổi dữ liệu.

\item {} 
Một hàm \sphinxstyleemphasis{mất mát} định lượng \sphinxstyleemphasis{độ lỗi} của mô hình.

\item {} 
Một \sphinxstyleemphasis{thuật toán} điều chỉnh các tham số của mô hình để giảm thiểu mất
mát.

\end{enumerate}




\subsection{Dữ liệu}
\label{\detokenize{chapter_introduction/index_vn:du-lieu}}


Có thể nói rằng bạn không thể làm khoa học dữ liệu mà không có dữ liệu.
Chúng ta sẽ tốn rất nhiều giấy mực để cân nhắc chính xác những gì cấu
thành nên dữ liệu, nhưng bây giờ chúng ta sẽ rẽ sang khía cạnh thực tế
và tập trung vào các thuộc tính quan trọng cần quan tâm. Thông thường,
chúng ta quan tâm đến một bộ \sphinxstyleemphasis{mẫu} (còn được gọi là \sphinxstyleemphasis{điểm dữ liệu}, \sphinxstyleemphasis{ví
dụ} hoặc \sphinxstyleemphasis{trường hợp}). Để làm việc với dữ liệu một cách hữu ích, chúng
ta thường cần phải có một cách biễu diễn phù hợp dưới dạng số. Mỗi \sphinxstyleemphasis{ví
dụ} thường bao gồm một bộ thuộc tính số gọi là \sphinxstyleemphasis{đặc trưng}. Trong các
bài toán học có giám sát ở trên, một đặc trưng đặc biệt được chọn như
\sphinxstyleemphasis{mục tiêu} dự đoán, (đôi khi được gọi là \sphinxstyleemphasis{nhãn} hoặc \sphinxstyleemphasis{biến phụ thuộc}).
Các đặc trưng nhất định mà mô hình dựa vào để đưa ra dự đoán có thể được
gọi đơn giản là các \sphinxstyleemphasis{đặc trưng}, (hoặc thường là \sphinxstyleemphasis{đầu vào}, \sphinxstyleemphasis{hiệp biến}
hoặc \sphinxstyleemphasis{biến độc lập}).



Nếu chúng ta đang làm việc với dữ liệu hình ảnh, mỗi bức ảnh riêng lẻ có
thể tạo thành một \sphinxstyleemphasis{mẫu} được biểu diễn bởi một danh sách các giá trị số
theo thứ tự tương ứng với độ sáng của từng pixel. Một bức ảnh màu có
kích thước \(200\times 200\) sẽ bao gồm
\(200\times200\times3=120000\) giá trị số, tương ứng với độ sáng của
các kênh màu đỏ, xanh lá cây và xanh dương cho từng vị trí trong không
gian. Trong một tác vụ truyền thống hơn, chúng ta có thể cố gắng dự đoán
xem một bệnh nhân liệu có cơ hội sống sót hay không, dựa trên bộ đặc
trưng tiêu chuẩn cho trước như tuổi, các triệu chứng quan trọng, thông
số chẩn đoán, .v.v.







Khi mỗi mẫu được biểu diễn bởi cùng một số lượng các giá trị, ta nói
rằng dữ liệu bao gồm các vector có \sphinxstyleemphasis{độ dài cố định} và ta mô tả độ dài
(không đổi) của vector là \sphinxstyleemphasis{chiều} của dữ liệu. Bạn có thể hình dung,
chiều dài cố định có thể là một thuộc tính thuận tiện. Nếu ta mong muốn
huấn luyện một mô hình để nhận biết ung thư qua hình ảnh từ kính hiển
vi, độ dài cố định của đầu vào sẽ giúp ta loại bỏ một vấn đề cần quan
tâm.



Tuy nhiên, không phải tất cả dữ liệu có thể dễ dàng được biểu diễn dưới
dạng vector có độ dài cố định. Đôi khi ta có thể mong đợi hình ảnh từ
kính hiển vi đến từ thiết bị tiêu chuẩn, nhưng ta không thể mong đợi
hình ảnh được khai thác từ Internet sẽ hiển thị với cùng độ phân giải
hoặc hình dạng được. Đối với hình ảnh, ta có thể tính đến việc cắt xén
nhằm đưa chúng về kích thước tiêu chuẩn, nhưng chiến lược này chỉ đưa ta
đến đấy mà thôi. Và ta có nguy cơ sẽ mất đi thông tin trong các phần bị
cắt bỏ. Hơn nữa, dữ liệu văn bản không thích hợp với cách biểu diễn dưới
dạng vector có độ dài cố định. Suy xét một chút về những đánh giá của
khách hàng để lại trên các trang Thương mại điện tử như Amazon, IMDB
hoặc TripAdvisor. Ta có thể thấy được số bình luận ngắn gọn như: “nó bốc
mùi!”, một số khác thì bình luận lan man hàng trang. Một lợi thế lớn của
học sâu so với các phương pháp truyền thống đó là các mô hình học sâu
hiện đại có thể xử lý dữ liệu có \sphinxstyleemphasis{độ dài biến đổi} một cách uyển chuyển
hơn.



Nhìn chung, chúng ta có càng nhiều dữ liệu thì công việc sẽ suôn sẻ hơn.
Khi ta có nhiều dữ liệu hơn, ta có thể huấn luyện ra những mô hình mạnh
mẽ hơn và ít phụ thuộc hơn vào các giả định được hình thành từ trước.
Việc chuyển từ dữ liệu nhỏ sang dữ liệu lớn là một đóng góp chính cho sự
thành công của học sâu hiện đại. Để cho rõ hơn, nhiều mô hình thú vị
nhất trong học sâu có thể không hoạt động nếu như không có bộ dữ liệu
lớn. Một số người vẫn áp dụng học sâu với số dữ liệu ít ỏi mà mình có
được, nhưng trong trường hợp này nó không tốt hơn các cách tiếp cận
truyền thống.



Cuối cùng, có nhiều dữ liệu và xử lý dữ liệu một cách khéo léo thôi thì
chưa đủ. Ta cần những dữ liệu \sphinxstyleemphasis{đúng}. Nếu dữ liệu mang đầy lỗi, hoặc nếu
các đặc trưng được chọn lại không dự đoán được số lượng mục tiêu cần
quan tâm, việc học sẽ thất bại. Tình huống trên có thể được khái quát
bởi thuật ngữ: \sphinxstyleemphasis{đưa rác vào thì nhận rác ra} (\sphinxstyleemphasis{garbage in, garbage
out}). Hơn nữa, chất lượng dự đoán kém không phải hậu quả tiềm tàng duy
nhất. Trong các ứng dụng học máy có tính nhạy cảm như: dự đoán hành vi
phạm pháp, sàng lọc hồ sơ cá nhân và mô hình rủi ro được sử dụng để cho
vay, chúng ta phải đặc biệt cảnh giác với hậu quả của dữ liệu rác. Một
dạng lỗi thường thấy xảy ra trong các bộ dữ liệu mà ở đó một số nhóm
người không tồn tại trong dữ liệu huấn luyện. Hãy hình dung khi áp dụng
một hệ thống nhận diện ung thư da trong thực tế mà trước đây nó chưa
từng thấy da màu đen. Thất bại cũng có thể xảy ra khi dữ liệu không đại
diện đầy đủ và chính xác cho một số nhóm người, nhưng lại đánh giá nhóm
người này dựa vào định kiến của xã hội. Một ví dụ, nếu như các quyết
định tuyển dụng trong quá khứ được sử dụng để huấn luyện một mô hình dự
đoán sẽ được sử dụng nhằm sàng lọc sơ yếu lý lịch, thì các mô hình học
máy có thể vô tình học được từ những bất công trong quá khứ. Lưu ý rằng
tất cả vấn đề trên có thể xảy ra mà không hề có tác động xấu nào của nhà
khoa học dữ liệu hoặc thậm chí họ còn không ý thức được về các vấn đề
đó.








\subsection{Mô hình}
\label{\detokenize{chapter_introduction/index_vn:mo-hinh}}


Phần lớn học máy đều liên quan đến \sphinxstyleemphasis{biến đổi} dữ liệu theo một cách nào
đó. Đó có thể là một hệ thống nhận ảnh đầu vào và dự đoán bức ảnh có
chứa khuôn mặt cười hay không. Hoặc đó cũng có thể là một hệ thống nhận
vào dữ liệu đo đạc từ cảm biến và dự đoán xem những số liệu đó là \sphinxstyleemphasis{bình
thường} hay \sphinxstyleemphasis{bất thường}. Ở đây chúng ta gọi \sphinxstyleemphasis{mô hình} là một hệ thống
tính toán lấy đầu vào là một dạng dữ liệu và trả về kết quả dự đoán có
thể ở một dạng dữ liệu khác.




\subsection{Hàm mục tiêu}
\label{\detokenize{chapter_introduction/index_vn:ham-muc-tieu}}


Trước đây, chúng tôi giới thiệu học máy theo kiểu “học từ kinh nghiệm”.
\sphinxstyleemphasis{Học} ở đây tức là có \sphinxstyleemphasis{tiến bộ} ở một tác vụ nào đó theo thời gian.
Nhưng ai sẽ chỉ ra như thế nào là tiến bộ? Thử tưởng tượng bạn đang đề
xuất cập nhật mô hình nhưng một số người có thể không đồng ý liệu việc
đó giúp cải thiện mô hình hay không.



Để có thể phát triển một mô hình toán học chính quy cho học máy, chúng
ta cần những phép đo chính quy xem mô hình đang tốt (hoặc tệ) như thế
nào. Trong học máy, hoặc nói rộng ra là lĩnh vực tối ưu hoá, ta gọi
chúng là các hàm mục tiêu (\sphinxstyleemphasis{objective function}). Theo quy ước, ta
thường định nghĩa các hàm tối ưu sao cho giá trị càng thấp thì mô hình
càng tốt. Đây chỉ là một quy ước ngầm. Bạn cũng có thể lấy một hàm
\(f\) sao cho giá trị càng cao thì càng tốt, sau đó lấy một hàm
\(f' = -f\) tương đương với giá trị càng thấp thì mô hình càng tốt.
Chính vì ta mong muốn hàm có giá trị thấp, nó còn được gọi dưới tên \sphinxstyleemphasis{hàm
mất mát} (\sphinxstyleemphasis{loss function}) và \sphinxstyleemphasis{hàm chi phí} (\sphinxstyleemphasis{cost function})







Khi cố gắng dự đoán một giá trị số thực, hàm mục tiêu phổ biến nhất là
hàm bình phương lỗi \((y-\hat{y})^2\). Với bài toán phân loại, hàm
mục tiêu phổ biến nhất là tối thiểu hoá tỉ lệ lỗi, nghĩa là tỉ lệ mẫu mà
mô hình dự đoán lệch với nhãn thực tế. Một vài hàm mục tiêu (ví dụ bình
phương lỗi) khá dễ để tối ưu. Các hàm khác (như tỉ lệ lỗi) lại khó tối
ưu một cách trực tiếp hơn bởi các hàm này không khả vi hoặc phức tạp.
Trong những trường hợp này, người ta thường tối thiểu hoá một \sphinxstyleemphasis{hàm mục
tiêu thay thế} (\sphinxstyleemphasis{surrogate function}).



Thông thường, hàm mất mát được định nghĩa theo các tham số mô hình và
phụ thuộc vào tập dữ liệu. Những giá trị tham số mô hình tốt nhất được
học bằng cách tối tiểu hoá hàm mất mát trên một \sphinxstyleemphasis{tập huấn luyện} bao gồm
các \sphinxstyleemphasis{mẫu} được thu thập cho việc huấn luyện. Tuy nhiên, mô hình hoạt
động tốt trên tập huấn luyện không có nghĩa là nó sẽ hoạt động tốt trên
dữ liệu kiểm tra (mà mô hình chưa nhìn thấy). Bởi vậy, chúng ta thường
chia dữ liệu sẵn có thành hai phần: dữ liệu huấn luyện (để khớp các tham
số mô hình) và dữ liệu kiểm tra (được giữ lại cho việc đánh giá). Sau đó
ta quan sát hai đại lượng:


\begin{itemize}
\item {} 
\sphinxstylestrong{Lỗi huấn luyện:} Lỗi trên dữ liệu được dùng để huấn luyện mô hình.
Bạn có thể coi đây như điểm của sinh viên trên bài thi thử được dùng
để chuẩn bị cho bài thi thật. Ngay cả khi kết quả thi thử khả quan,
nó cũng không đảm bảo bài thi thật đạt kết quả tốt.{}`{}`{}`

\item {} 
\sphinxstylestrong{Lỗi kiểm tra:} Đây là lỗi trên tập kiểm tra (không dùng để huấn
luyện mô hình). Đại lượng này có thể tệ hơn đáng kể so với lỗi huấn
luyện. Khi một mô hình hoạt đồng tốt trên tập huấn luyện nhưng không
tổng quát hóa tốt trên dữ liệu chưa gặp, ta nói rằng mô hình bị \sphinxstyleemphasis{quá
khớp} (overfit). Theo ngôn ngữ thông thường, đây là hiện tượng “học
lệch tủ” khi kết quả bài thi thật rất kém mặc dù có kết quả cao trong
bài thi thử.

\end{itemize}








\subsection{Các thuật toán tối ưu}
\label{\detokenize{chapter_introduction/index_vn:cac-thuat-toan-toi-uu}}


Một khi đã có dữ liệu, mô hình và một hàm mục tiêu rõ ràng, ta cần một
thuật toán có khả năng tìm kiếm các tham số khả dĩ tốt nhất để tối thiểu
hoá hàm mất mát. Các thuật toán tối ưu phổ biến nhất cho mạng nơ-ron đi
theo một hướng tiếp cận được gọi là hạ gradient. Một cách ngắn gọn, tại
mỗi bước, ta kiểm tra xem hàm mất mát thay đổi như thế nào nếu mỗi tham
số thay đổi chỉ một lượng nhỏ. Sau đó các tham số này được cập nhật theo
hướng làm giảm hàm mất mát.




\section{Các dạng Học Máy}
\label{\detokenize{chapter_introduction/index_vn:cac-dang-hoc-may}}


Trong các mục tiếp theo, chúng ta thảo luận một vài \sphinxstyleemphasis{dạng} bài toán học
máy một cách chi tiết hơn. Chúng ta bắt đầu với một danh sách \sphinxstyleemphasis{các mục
tiêu}, tức một danh sách các tác vụ chúng ta muốn học máy thực hiện. Chú
ý rằng các mục tiêu được gắn với một tập các kỹ thuật giúp trả lời câu
hỏi “làm sao” để đạt được chúng, bao gồm các kiểu dữ liệu, mô hình, kỹ
thuật huấn luyện, v.v. Danh sách dưới đây là một tập con các bài toán mà
Học Máy có thể xử lý nhằm tạo động lực cho độc giả, đồng thời cung cấp
một ngôn ngữ chung khi ta nói về những bài toán khác xuyên suốt cuốn
sách.








\subsection{Học có giám sát}
\label{\detokenize{chapter_introduction/index_vn:hoc-co-giam-sat}}


Học có giám sát giải quyết tác vụ dự đoán \sphinxstyleemphasis{mục tiêu} khi cho trước \sphinxstyleemphasis{đầu
vào}. Các mục tiêu, mà chúng ta thường gọi là \sphinxstyleemphasis{nhãn}, thường được ký
hiệu bằng \(y\). Dữ liệu đầu vào, cũng thường được gọi là \sphinxstyleemphasis{đặc
trưng} hoặc hiệp biến, thường được ký hiệu là \(\mathbf{x}\). Mỗi
cặp (đầu vào, mục tiêu) được gọi là một \sphinxstyleemphasis{mẫu}. Thi thoảng, khi văn cảnh
rõ ràng hơn, chúng ta có thể sử dụng thuật ngữ \sphinxstyleemphasis{các ví dụ} để chỉ một
tập các đầu vào, ngay cả khi mục tiêu tương ứng là chưa biết. Ta ký hiệu
bất cứ một mẫu cụ thể nào với một chỉ số dưới, thường là \(i\), ví
dụ (\(\mathbf{x}_i, y_i\)). Một tập dữ liệu là một tập của \(n\)
mẫu \(\{\mathbf{x}_i, y_i\}_{i=1}^n\). Mục đích của chúng ta là tạo
một mô hình \(f_\theta\) ánh xạ bất kỳ đầu vào \(\mathbf{x}_i\)
tới một dự đoán \(f_{\theta}(\mathbf{x}_i)\).



Một ví dụ cụ thể hơn, nếu chúng ta đang làm việc trong lĩnh vực chăm sóc
sức khoẻ, chúng ta có thể mong muốn dự đoán liệu rằng một bệnh nhân có
bị đau tim không. Việc \sphinxstyleemphasis{bị đau tim} hay \sphinxstyleemphasis{không bị đau tim} sẽ là nhãn
\(y\). Dữ liệu đầu vào \(\mathbf{x}\) có thể là các dấu hiệu
quan trọng như nhịp tim, huyết áp tâm trương và tâm thu, v.v.



Sự giám sát xuất hiện ở đây bởi để chọn các tham số \(\theta\),
chúng ta (các giám sát viên) cung cấp cho mô hình một tập dữ liệu chứa
các \sphinxstyleemphasis{mẫu được gán nhãn} (\(\mathbf{x}_i, y_i\)), ở đó mỗi mẫu
\(\mathbf{x}_i\) tương ứng một nhãn cho trước.



Theo thuật ngữ xác suất, ta thường quan tâm tới việc đánh giá xác suất
có điều kiện \(P(y|\){\color{red}\bfseries{}:raw-latex:{}`\textbackslash{}mathbf\{x\}{}`})\$. Mặc dù chỉ là một
trong số nhiều mô hình trong học máy, học có giám sát là nhân tố chính
đem đến sự thành công cho các ứng dụng của học máy trong công nghiệp.
Một phần, đó là bởi rất nhiều tác vụ có thể được mô tả dưới dạng ước
lượng xác suất của một đại lượng chưa biết cho trước một tập dữ liệu cụ
thể:


\begin{itemize}
\item {} 
Dự đoán có bị ung thư hay không cho trước một bức ảnh CT.

\item {} 
Dự đoán bản dịch chính xác trong tiếng Pháp cho trước một câu trong
tiếng Anh.

\item {} 
Dự đoán giá của cổ phiếu trong tháng tới dựa trên dữ liệu báo cáo tài
chính của tháng này.

\end{itemize}







Ngay cả với mô tả đơn giản là “dự đoán mục tiêu từ đầu vào”, học có giám
sát đã có nhiều hình thái đa dạng và đòi hỏi đưa ra nhiều quyết định mô
hình hoá khác nhau, phụ thuộc vào kiểu, kích thước, số lượng của cặp đầu
vào và đầu ra cũng như các yếu tố khác. Ví dụ, ta sử dụng các mô hình
khác nhau để xử lý các chuỗi (như chuỗi ký tự hay dữ liệu chuỗi thời
gian) và để xử lý các biểu diễn vector với chiều dài cố định. Chúng ta
sẽ đào sâu vào rất nhiều bài toán dạng này thông qua 9 phần đầu của cuốn
sách.



Một cách dễ hiểu, quá trình học tương tự với: Lấy một tập mẫu lớn ở đó
các hiệp biến đã biết trước. Từ đó chọn ra một tập con ngẫu nhiên, thu
thập các nhãn gốc cho chúng. Đôi khi những nhãn này có thể đã có sẵn
trong dữ liệu (ví dụ liệu bệnh nhân đã qua đời trong năm tiếp theo?),
khi khác chúng ta cần thuê người gán nhãn cho dữ liệu (ví dụ gán một bức
ảnh vào một hạng mục nào đó).



Những đầu vào và nhãn tương ứng này cùng tạo nên tập huấn luyện. Chúng
ta đưa tập dữ liệu huấn luyện vào một thuật toán học có giám sát \textendash{} một
hàm số mà đầu vào là tập dữ liệu và đầu ra là một hàm số khác thể hiện
\sphinxstyleemphasis{mô hình đã học được}. Cuối cùng, ta có thể đưa dữ liệu chưa nhìn thấy
vào mô hình đã học được, sử dụng đầu ra của nó như là giá trị dự đoán
của các nhãn tương ứng. Toàn bộ quá trình được mô tả trong
\hyperref[\detokenize{chapter_introduction/index_vn:fig-supervised-learning}]{Fig.\@ \ref{\detokenize{chapter_introduction/index_vn:fig-supervised-learning}}}.







\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics{{supervised-learning}.pdf}
\caption{Học có giám sát.}\label{\detokenize{chapter_introduction/index_vn:id11}}\label{\detokenize{chapter_introduction/index_vn:fig-supervised-learning}}\end{figure}




\subsubsection{Hồi quy}
\label{\detokenize{chapter_introduction/index_vn:hoi-quy}}


Có lẽ tác vụ học có giám sát đơn giản nhất là \sphinxstyleemphasis{hồi quy}. Xét ví dụ một
tập dữ liệu thu thập được từ cơ sở dữ liệu buôn bán nhà. Chúng ta có thể
xây dựng một bảng dữ liệu, ở đó mỗi hàng tương ứng với một nhà và mỗi
cột tương ứng với một thuộc tính liên quan nào đó, chẳng hạn như diện
tích nhà, số lượng phòng ngủ, số lượng phòng tắm và thời gian (theo
phút) để đi bộ tới trung tâm thành phố. Trong tập dữ liệu này, mỗi \sphinxstyleemphasis{mẫu}
là một căn nhà cụ thể và \sphinxstyleemphasis{vector đặc trưng} tương ứng là một hàng trong
bảng.



Nếu bạn sống ở New York hoặc San Francisco và bạn không phải là CEO của
Amazon, Google, Microsoft hay Facebook, thì vector đặc trưng (diện tích,
số phòng ngủ, số phòng tắm, khoảng cách đi bộ) của căn nhà của bạn có
thể có dạng \([100, 0, 0.5, 60]\). Tuy nhiên, nếu bạn sống ở
Pittsburgh, vector đó có thể là \([3000, 4, 3, 10]\). Vector đặc
trưng là thiết yếu trong hầu hết các thuật toán học máy cổ điển. Chúng
ta sẽ tiếp tục ký hiệu vector đặc trưng tương ứng với bất ký mẫu
\(i\) nào bởi \(\mathbf{x}_i\) và có thể đặt \(X\) là toàn
bộ bảng chứa tất cả các vector đặc trưng.



Để xác định một bài toán là \sphinxstyleemphasis{hồi quy} hay không, ta dựa vào đầu ra của
nó. Chẳng hạn, bạn đang khảo sát thị trường cho một căn nhà mới. Bạn có
thể ước lượng giá thị trường của một căn nhà khi biết trước những đặc
trưng phía trên. Giá trị mục tiêu, hay giá bán của căn nhà, là một \sphinxstyleemphasis{số
thực}. Nếu bạn còn nhớ định nghĩa toán học của số thực, bạn có thể băn
khoăn. Nhà đất có lẽ không bao giờ bán với giá lẻ tới từng cent chứ đừng
nói đến nhỏ hơn cent (các số vô tỉ). Trong trường hợp này, khi mục tiêu
thực sự là các số rời rạc, nhưng việc làm tròn có thể chấp nhận được,
chúng ta sẽ lạm dụng cách dùng từ một chút để tiếp tục mô tả đầu ra và
mục tiêu như các số thực.







Ký hiệu mục tiêu là \(y_i\) (tương ứng với mẫu \(\mathbf{x_i}\))
và tập tất cả các mục tiêu là \(\mathbf{y}\) (tương ứng với tất cả
các mẫu \(X\)). Khi các mục tiêu lấy các giá trị bất kỳ trong một
khoảng, chúng ta gọi đây là bài toán hồi quy. Mục đích của chúng ta là
tạo ra một mô hình mà các giá trị dự đoán của nó xấp xỉ với các giá trị
mục tiêu thực sự. Chúng ta ký hiệu mục tiêu dự đoán của một mẫu là
\(\hat{y}_i\). Đừng quá lo lắng nếu có quá nhiều ký hiệu. Chúng ta
sẽ tìm hiểu kỹ từng kỳ hiệu trong các chương tiếp theo.



Rất nhiều bài toán thực tế có thể được mô tả thông qua các bài toán hồi
quy. Dự đoán điểm số một người dùng gán cho một bộ phim có thể được coi
là một bài toán hồi quy và nếu bạn thiết kế một thuật toán tốt để đạt
được điều này năm 2009, bạn có thể đã giành \sphinxhref{https://en.wikipedia.org/wiki/Netflix\_Prize}{giải thưởng Netflix một
triệu Đô-la}%
\begin{footnote}[29]\sphinxAtStartFootnote
\sphinxnolinkurl{https://en.wikipedia.org/wiki/Netflix\_Prize}
%
\end{footnote}. Dự đoán
thời gian nằm viện của một bệnh nhân cũng là một bài toán hồi quy. Một
quy tắc dễ nhớ là các bài toán mà ta phải trả lời cho câu hỏi \sphinxstyleemphasis{bao
nhiêu} (\sphinxstyleemphasis{bao lâu}, \sphinxstyleemphasis{bao xa}, v.v.) có thể được coi là các bài toán hồi
quy.


\begin{itemize}
\item {} 
“Ca phẫu thuật này sẽ mất bao lâu?”: \sphinxstyleemphasis{hồi quy}

\item {} 
“Có bao nhiêu chú chó trong bức ảnh?”: \sphinxstyleemphasis{hồi quy}

\end{itemize}



Tuy nhiên, nếu bạn có thể biến bài toán của bạn thành “Có đúng là \_?”
thì khả năng cao đó là bài toán phân loại, một dạng khác của bài toán
học có giám sát mà chúng ta thảo luận trong phần tiếp. Ngay cả khi bạn
chưa từng làm việc với học máy, bạn có thể đã làm việc với các bài toán
hồi quy một cách không chính thức. Ví dụ, hãy tưởng tượng bạn cần sửa
chữa đường ống cống và người thợ đã dành \(x_1=3\) giờ để thông cống
rồi gửi hoá đơn \(y_1 = \$350\). Bây giờ bạn của bạn thuê cùng người
thợ trong \(x_2 = 2\) tiếng và cô ấy nhận được hoá đơn là
\(y_2 = \$250\). Nếu một người sau đó hỏi bạn dự tính giá phải trả
để thông cống, bạn có thể có một vài giả sử có lý, chẳng hạn nhiều thời
gian sẽ tốn nhiều tiền hơn. Bạn cũng có thể giả sử rằng có một mức phí
cơ bản và sau đó người thợ tính tiền theo giờ. Nếu giả sử này là đúng,
thì cho trước hai điểm dữ liệu, bạn đã có thể tính được cách mà người
thợ xây dựng bảng giá: \$100 một giờ cộng với \$50 cho việc tới nhà bạn.
Nếu bạn theo được logic tới đây thì bạn đã có thể hiểu ý tưởng sơ lược
đằng sau hồi quy tuyến tính (và bạn vô tình đã thiết kế một mô hình
tuyến tính với thành phần điều chỉnh).







Trong trường hợp này, chúng ta có thể tìm được các tham số chính xác cho
mô hình ước tính chi phí của người thợ sửa ống cống. Đôi khi việc này là
không khả thi, ví dụ một biến thể nào đó gây ra bởi các yếu tố ngoài hai
đặc trưng kể trên. Trong những trường hợp này, ta sẽ cố học các mô hình
sao cho nó tối thiểu hoá khoảng cách giữa các giá trị dự đoán và các giá
trị thực sự. Trong hầu hết các chương, chúng ta sẽ tập trong vào một
trong hai hàm mất mát phổ biến nhất: hàm \sphinxhref{http://mxnet.incubator.apache.org/api/python/gluon/loss.html\#mxnet.gluon.loss.L1Loss}{mất mát
L1}%
\begin{footnote}[30]\sphinxAtStartFootnote
\sphinxnolinkurl{http://mxnet.incubator.apache.org/api/python/gluon/loss.html\#mxnet.gluon.loss.L1Loss}
%
\end{footnote},
ở đó
\begin{equation}\label{equation:chapter_introduction/index_vn:chapter_introduction/index_vn:0}
\begin{split}l(y, y') = \sum_i |y_i-y_i'|\end{split}
\end{equation}


và hàm thứ hai là mất mát trung bình bình phương nhỏ nhất, hoặc \sphinxhref{http://mxnet.incubator.apache.org/api/python/gluon/loss.html\#mxnet.gluon.loss.L2Loss}{mất mát
L2}%
\begin{footnote}[31]\sphinxAtStartFootnote
\sphinxnolinkurl{http://mxnet.incubator.apache.org/api/python/gluon/loss.html\#mxnet.gluon.loss.L2Loss}
%
\end{footnote},
ở đó
\begin{equation}\label{equation:chapter_introduction/index_vn:chapter_introduction/index_vn:1}
\begin{split}l(y, y') = \sum_i (y_i - y_i')^2.\end{split}
\end{equation}


Như chúng ta sẽ thấy về sau, mất mát \(L_2\) tương ứng với giả sử
rằng dữ liệu của chúng ta có nhiễu Gauss, trong khi mất mát \(L_1\)
tương ứng với giả sử nhiễu đến từ một phân phối Laplace.








\subsubsection{Phân loại}
\label{\detokenize{chapter_introduction/index_vn:phan-loai}}


Trong khi các mô hình hồi quy hiệu quả cho việc trả lời các câu hỏi \sphinxstyleemphasis{có
bao nhiêu?}, rất nhiều bài toán không phù hợp với nhóm mô hình này. Ví
dụ, một ngân hàng muốn thêm chức năng quét ngân phiếu trong ứng dụng di
động của họ. Việc này sẽ bao gồm việc khách hàng chụp một bức ảnh của
ngân phiếu với camera của điện thoại và mô hình học máy sẽ cần phải tự
động hiểu nội dung chữ trong bức ảnh. Hiểu được cả chữ viết tay sẽ giúp
ứng dụng hoạt động còn ổn định hơn nữa. Kiểu hệ thống này được gọi là
nhận dạng ký tự quang học (\sphinxstyleemphasis{optical charactor recognition} \textendash{} OCR), và
kiểu bài toán mà nó giải quyết được gọi là \sphinxstyleemphasis{phân loại}. Nó được thiết kế
bởi một tập các thuật toán khác với thuật toán dùng trong hồi quy (mặc
dù có nhiều kỹ thuật chung).



Trong phân loại, ta muốn mô hình nhìn vào một vector đặc trưng, ví dụ
như các giá trị điểm ảnh trong một bức ảnh, và sau đó dự đoán mẫu đó rơi
vào hạng mục nào (được gọi là \sphinxstyleemphasis{lớp}) trong số một tập (rời rạc) các lựa
chọn. Với chữ số viết tay, ta có thể có 10 lớp tương ứng với các chữ số
từ 0 tới 9. Dạng đơn giản nhất của phân loại là khi chỉ có hai lớp, khi
đó ta gọi bài toán này là phân loại nhị phân. Ví dụ, tập dữ liệu
\(X\) có thể chứa các bức ảnh động vật và các \sphinxstyleemphasis{nhãn} \(Y\) có
thể là các lớp \(\mathrm{\{cat, dog\}}\). Với hồi quy, ta tìm một
\sphinxstyleemphasis{bộ hồi quy} để đưa ra một giá trị thực \(\hat{y}\). Trong khi đó
với phân loại, ta tìm một \sphinxstyleemphasis{bộ phân loại} để dự đoán lớp \(\hat{y}\).



Khi cuốn sách đi sâu hơn vào các vấn đề kỹ thuật, chúng ta sẽ bàn về các
lý do tại sao lại khó hơn để tối ưu hoá một mô hình mà đầu ra là các giá
trị hạng mục rời rạc, ví dụ \sphinxstyleemphasis{mèo} hoặc \sphinxstyleemphasis{chó}. Trong những trường hợp
này, thường sẽ dễ hơn khi thay vào đó, ta biểu diễn mô hình dưới ngôn
ngữ xác suất. Cho trước một mẫu \(\mathbf{x}\), mô hình cần gán một
giá trị xác suất \(\hat{y}_k\) cho mỗi nhãn \(k\). Vì là các giá
trị xác suất, chúng phải là các số dương có tổng bằng \(1\). Bởi
vậy, ta chỉ cần \(K-1\) số để gán xác suất cho \(K\) hạng mục.
Việc này dễ nhận thấy đối với phân loại nhị phân. Nếu một đồng xu không
đều có xác suất ra mặt ngửa là \(0.6\) (\(60\%\)), thì xác suất
ra mặt xấp là \(0.4\) (\(40\%\)). Trở lại với ví dụ phân loại
động vật, một bộ phân loại có thể nhìn một bức ảnh và đưa ra xác suất để
bức ảnh đó là mèo \(P(y=\text{mèo} \mid x) = 0.9\). Chúng ta có thể
diễn giải giá trị này tương ứng với việc bộ phân loại \(90\%\) tin
rằng bức ảnh đó chứa một con mèo. Giá trị xác suất của một lớp được dự
đoán mang ý nghĩa về sự không chắc chắn. Đó không phải là ký hiệu duy
nhất của sự không chắc chắn, chúng ta sẽ thảo luận các ký hiệu khác
trong các chương nâng cao.



Khi có nhiều hơn hai lớp, ta gọi bài toán này là \sphinxstyleemphasis{phân loại đa lớp}. Bài
toán phân loại chữ viết tay \sphinxcode{\sphinxupquote{{[}0, 1, 2, 3 ... 9, a, b, c, ...{]}}} là một
trong số các ví dụ điển hình. Trong khi các hàm mất mát thường được sử
dụng trong các bài toán hồi quy là hàm mất mát L1 hoặc L2, hàm mất mát
phổ biến cho bài toán phân loại được gọi là entropy chéo
(\sphinxstyleemphasis{cross-entropy}), hàm tương ứng trong MXNet Gluon có thể xem \sphinxhref{https://mxnet.incubator.apache.org/api/python/gluon/loss.html\#mxnet.gluon.loss.SoftmaxCrossEntropyLoss}{tại
đây}%
\begin{footnote}[32]\sphinxAtStartFootnote
\sphinxnolinkurl{https://mxnet.incubator.apache.org/api/python/gluon/loss.html\#mxnet.gluon.loss.SoftmaxCrossEntropyLoss}
%
\end{footnote}







Lưu ý rằng lớp có khả năng xảy ra nhất theo dự đoán của mô hình không
nhất thiết là lớp mà ta quyết định sử dụng. Giả sử bạn tìm được một cây
nấm rất đẹp trong sân nhà như hình \hyperref[\detokenize{chapter_introduction/index_vn:fig-death-cap}]{Fig.\@ \ref{\detokenize{chapter_introduction/index_vn:fig-death-cap}}}.



\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=200\sphinxpxdimen]{{death_cap}.jpg}
\caption{\sphinxstyleemphasis{dịch chú thích ảnh phía trên}}\label{\detokenize{chapter_introduction/index_vn:id12}}\label{\detokenize{chapter_introduction/index_vn:fig-death-cap}}\end{figure}



Bây giờ giả sử ta đã xây dựng một bộ phân loại và huấn luyện nó để dự
đoán liệu một cây nấm có độc hay không dựa trên ảnh chụp. Giả sử bộ phân
loại phát hiện chất độc đưa ra
\(P(y=\mathrm{nấm độc}|\mathrm{bức ảnh}) = 0.2\). Nói cách khác, bộ
phân loại này chắc chắn rằng \(80\%\) cây này \sphinxstyleemphasis{không phải} nấm độc.
Dù vậy, đừng dại mà ăn nhé. Vì việc có bữa tối ngon lành không đáng gì
so với rủi ro \(20\%\) sẽ chết vì nấm độc. Nói cách khác, hậu quả
của \sphinxstyleemphasis{rủi ro không chắc chắn} nghiêm trọng hơn nhiều so với lợi ích thu
được. Ta có thể nhìn việc này một cách hợp thức hơn. Về cơ bản, ta cần
tính toán rủi ro kỳ vọng mà mình sẽ gánh chịu, ví dụ, ta nhân xác suất
xảy ra kết quả đó với lợi ích (hoặc hậu quả) đi liền tương ứng:
\begin{equation}\label{equation:chapter_introduction/index_vn:chapter_introduction/index_vn:2}
\begin{split}L(\mathrm{hành động}| x) = E_{y \sim p(y| x)}[\mathrm{mất mát}(\mathrm{hành động},y)].\end{split}
\end{equation}


Do đó, mất mát \(L\) do ăn phải nấm là
\(L(a=\mathrm{ăn}| x) = 0.2 * \infty + 0.8 * 0 = \infty\), mặc dù
phí tổn do bỏ nấm đi là
\(L(a=\mathrm{bỏ đi}| x) = 0.2 * 0 + 0.8 * 1 = 0.8\).



Sự thận trọng của chúng ta là chính đáng: như bất kỳ nhà nghiên cứu nấm
nào cũng sẽ nói, cây nấm ở trên thực sự \sphinxstyleemphasis{là} nấm độc. Việc phân loại có
thể còn phức tạp hơn nhiều so với phân loại nhị phân, đa lớp, hoặc thậm
chí đa nhãn. Ví dụ, có vài biến thể của phân loại để xử lý vấn đề phân
cấp bậc (\sphinxstyleemphasis{hierarchy}). Việc phân cấp giả định rằng tồn tại các mối quan
hệ giữa các lớp với nhau. Vậy nên không phải tất cả các lỗi đều như
nhau—nếu bắt buộc có lỗi, ta sẽ mong rằng các mẫu bị phân loại nhầm
thành một lớp liên quan thay vì một lớp khác xa nào đó. Thông thường,
việc này được gọi là \sphinxstyleemphasis{phân loại cấp bậc (hierarchical classification)}.
Một trong những ví dụ đầu tiên về việc xây dựng hệ thống phân cấp là từ
\sphinxhref{https://en.wikipedia.org/wiki/Carl\_Linnaeus}{Linnaeus}%
\begin{footnote}[33]\sphinxAtStartFootnote
\sphinxnolinkurl{https://en.wikipedia.org/wiki/Carl\_Linnaeus}
%
\end{footnote}, người đã sắp
xếp các loại động vật theo hệ thống phân cấp.



Trong trường hợp phân loại động vật, cũng không tệ lắm nếu phân loại
nhầm hai giống chó xù poodle và schnauzer, nhưng sẽ rất nghiêm trọng nếu
ta nhầm lẫn chó poodle với một con khủng long. Hệ phân cấp nào là phù
hợp phụ thuộc vào việc ta dự định dùng mô hình như thế nào. Ví dụ, rắn
đuôi chuông và rắn sọc không độc có thể nằm gần nhau trong cây phả hệ,
nhưng phân loại nhầm hai loài này có thể dẫn tới hậu quả chết người.








\subsubsection{Gán thẻ}
\label{\detokenize{chapter_introduction/index_vn:gan-the}}


Một vài bài toán phân loại không phù hợp với các mô hình phân loại nhị
phân hoặc đa lớp. Ví dụ, chúng ta có thể huấn luyện một bộ phân loại nhị
phân thông thường để phân loại mèo và chó. Với khả năng hiện tại của thị
giác máy tính, việc này có thể được thực hiện dễ dàng bằng các công cụ
sẵn có. Tuy nhiên, bất kể mô hình của bạn chính xác đến đâu, nó có thể
gặp rắc rối khi thấy bức ảnh Những Nhạc Sĩ thành Bremen.



\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=300\sphinxpxdimen]{{stackedanimals}.jpg}
\caption{Mèo, gà trống, chó và lừa}\label{\detokenize{chapter_introduction/index_vn:id13}}\label{\detokenize{chapter_introduction/index_vn:subsec-recommender-systems}}\end{figure}



Bạn có thể thấy trong ảnh có một con mèo, một con gà trống, một con chó,
một con lừa và một con chim, cùng với một vài cây ở hậu cảnh. Tuỳ vào
mục đích cuối cùng của mô hình, sẽ không hợp lý nếu coi đây là một bài
toán phân loại nhị phân. Thay vào đó, ta có thể cho mô hình lựa chọn nói
rằng bức ảnh có một con mèo \sphinxstyleemphasis{và} một con chó \sphinxstyleemphasis{và} một con cừu \sphinxstyleemphasis{và} một
con gà trống \sphinxstyleemphasis{và} một con chim.



Bài toán học để dự đoán các lớp \sphinxstyleemphasis{không xung khắc} được gọi là phân loại
đa nhãn. Các bài toán tự động gán thẻ là các ví dụ điển hình của phân
loại đa nhãn. Nghĩ về các thẻ mà một người có thể gán cho một blog công
nghệ, ví dụ “học máy”, “công nghệ”, “ngôn ngữ lập trình”, “linux”, “điện
toán đám mây” hay “AWS”. Một bài báo thông thường có thể có từ 5-10 thẻ
bởi các khái niệm này có liên quan với nhau. Các bài về “điện toán đám
mây” khả năng cao đề cập “AWS” và các bài về “học máy” cũng có thể dính
dáng tới “ngôn ngữ lập trình”.





Ta cũng phải xử lý các vấn đề này trong nghiên cứu y sinh, ở đó việc gán
thẻ cho các bài báo một cách chính xác là quan trọng bởi nó cho phép các
nhà nghiên cứu tổng hợp đầy đủ các tài liệu liên quan. Tại Thư Viện Y
Khoa Quốc Gia, một số chuyên gia gán nhãn duyệt qua tất cả các bài báo
được lưu trên PubMed để gán chúng với các thuật ngữ y khoa (\sphinxstyleemphasis{MeSH}) liên
quan \textendash{} một bộ sư tập với khoảng 28 nghìn thẻ. Đây là một quá trình tốn
thời gian và những người gán nhãn thường có một năm trễ kể từ khi lưu
trữ tới khi gán thẻ. Học máy có thể được sử dụng ở đây để cung cấp các
thẻ tạm thời cho tới khi được kiểm chứng lại một cách thủ công. Thực
vậy, BioASQ đã {[}tổ chức một cuộc thi{]}((\sphinxurl{http://bioasq.org/}) dành riêng
cho việc này.








\subsubsection{Tìm kiếm và xếp hạng}
\label{\detokenize{chapter_introduction/index_vn:tim-kiem-va-xep-hang}}


Đôi khi ta không chỉ muốn gán một lớp hoặc một giá trị vào một mẫu.
Trong lĩnh vực thu thập thông tin (\sphinxstyleemphasis{intromation retrieval}), ta muốn gán
thứ hạng cho một tập các mẫu. Lấy ví dụ trong tìm kiếm web, mục tiêu
không dừng lại ở việc xác định liệu một trang nào đó có liên quan tới
một từ khoá mà xa hơn, trang nào trong số vô vàn kết quả trả về có \sphinxstyleemphasis{liên
quan nhất} tới một người dùng. Ta thực sự quan tâm đến thứ tự của các
kết quả tìm kiếm liên quan và thuật toán cần đưa ra một tập con có thứ
tự của các thành phần trong một tập lớn hơn. Nói cách khác, nếu được hỏi
đưa ra năm chữ cái từ bảng chữ cái, hai kết quả \sphinxcode{\sphinxupquote{A B C D E}} và
\sphinxcode{\sphinxupquote{C A B E D}} là khác nhau. Ngay cả khi hai tập kết quả là như nhau, thứ
tự các phần tử trong mỗi tập mới là điều quan trọng.



Một giải pháp khả dĩ cho bài toán này là trước tiên gán cho mỗi phần tử
trong tập hợp một điểm liên quan tương ứng và sau đó trả về những phẩn
tử có điểm cao nhất.
\sphinxhref{https://en.wikipedia.org/wiki/PageRank}{PageRank}%
\begin{footnote}[34]\sphinxAtStartFootnote
\sphinxnolinkurl{https://en.wikipedia.org/wiki/PageRank}
%
\end{footnote}, vũ khí bí mật
đằng sau cỗ máy tìm kiếm của Google là một trong những ví dụ đầu tiên
của hệ thống tính điểm kiểu này. Tuy nhiên, điều bất thường là nó không
phụ thuộc vào từ khoá tìm kiếm. Chúng phụ thuộc vào một bộ lọc đơn giản
để xác định tập hợp các trang liên quan rồi sau đó mới dùng PageRank để
sắp xếp các kết quả có chứa cụm tìm kiếm. Có cả những hội thảo khoa học
chuyên nghiên cứu về lĩnh vực này.








\subsubsection{Hệ thống gợi ý}
\label{\detokenize{chapter_introduction/index_vn:he-thong-goi-y}}


Hệ thống gợi ý là một bài toán khác liên quan đến tìm kiếm và xếp hạng.
Chúng có chung mục đích là hiển thị một tập các kết quả liên quan tới
người dùng. Sự khác biệt chính là nó nhấn mạnh tới \sphinxstyleemphasis{cá nhân hoá} cho
từng người dùng cụ thể với trường hợp hệ thống gợi ý. Ví dụ, với gợi ý
phim ảnh, trang kết quả cho một fan của phim khoa học viễn tưởng và cho
một người sành sỏi hài Peter Sellers có thể khác nhau một cách đáng kể.
Tương tự với các bài toán gợi ý khác như các hệ thống gợi ý sản phẩm bán
lẻ, âm nhạc hoặc tin tức.



Trong một vài trường hợp, khách hàng cung cấp phản hồi trực tiếp
(\sphinxstyleemphasis{explicit feedback}) họ thích một sản phẩm cụ thể như thế nào (ví dụ
các đánh giá sản phẩm và phản hồi trên Amazon, IMDB, GoodReads, v.v.).
Trong vài trường hợp khác, họ cung cấp phản hồi gián tiếp (\sphinxstyleemphasis{implicit
feedback}), ví dụ như bỏ qua các bài hát trong một danh sách nhạc. Việc
này có thể chỉ ra sự không thoả mãn hoặc có thể chỉ đơn giản là bài hát
không phù hợp trong bối cảnh. Trong cách trình bày đơn giản nhất, những
hệ thống này được huấn luyện để ước lượng một điểm \(y_{ij}\) nào
đó, ví dụ như một ước lượng đánh giá hoặc xác suất mua hàng, của một
người dùng \(u_i\) tới một sản phẩm \(p_j\).



Cho trước mô hình, với một người dùng bất kỳ, ta có thể thu thập một tập
các sản phẩm với điểm lớn nhất \(y_{ij}\) để gợi ý cho khách hàng.
Các hệ thống đang vận hành còn cao cấp hơn nữa. Chúng sử dụng hành vi
của người dùng và các thuộc tính sản phẩm để tính điểm.
\hyperref[\detokenize{chapter_introduction/index_vn:fig-deeplearning-amazon}]{Fig.\@ \ref{\detokenize{chapter_introduction/index_vn:fig-deeplearning-amazon}}} là một ví dụ về các cuốn sách học
sâu được gợi ý bởi Amazon dựa trên các thuật toán cá nhân hoá sử dụng
các thông tin của tác giả.



\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics{{deeplearning_amazon}.png}
\caption{Các sách học sâu được gợi ý bởi Amazon.}\label{\detokenize{chapter_introduction/index_vn:id14}}\label{\detokenize{chapter_introduction/index_vn:fig-deeplearning-amazon}}\end{figure}



Mặc dù có giá trị kinh tế lớn, các hệ thống gợi ý được xây dựng đơn
thuần theo các mô hình dự đoán cũng có những hạn chế nghiêm trọng. Ban
đầu, ta chỉ quan sát các \sphinxstyleemphasis{phản hồi được kiểm duyệt}. Người dùng thường
có xu hướng đánh giá các bộ phim họ thực sự thích hoặc ghét: bạn có thể
để ý rằng các bộ phim nhận được rất nhiều đánh giá 5 và 1 sao nhưng rất
ít các bộ phim với 3 sao. Hơn nữa, thói quen mua hàng hiện tại thường là
kết quả của thuật toán gợi ý đang được dùng, nhưng các thuật toán gợi ý
không luôn để ý đến chi tiết này. Bởi vậy có khả năng xảy ra các vòng
phản hồi (\sphinxstyleemphasis{feedback loop}) luẩn quẩn mà ở đó một hệ thống gợi ý đẩy lên
một sản phẩm và sau đó nó cho rằng sản phẩm này tốt hơn (do được mua
nhiều hơn) rồi cuối cùng sản phẩm đó lại được hệ thống gợi ý thường
xuyên hơn nữa. Rất nhiều trong số các bài toán về cách xử lý với kiểm
duyệt, động cơ {[}của việc đánh giá{]} và vòng phản hồi là các câu hỏi quan
trọng cho nghiên cứu.








\subsubsection{Học chuỗi}
\label{\detokenize{chapter_introduction/index_vn:hoc-chuoi}}




Cho tới hiện tại, chúng ta đã gặp các bài toán mà ở đó mô hình nhận đầu
vào với kích thước cố định và đưa ra kết quả cũng với kích thước cố
định. Trước đây chúng ta xem xét dự đoán giá nhà từ một tập các đặc
trưng cố định: diện tích, số phòng ngủ, số phòng tắm và thời gian đi bộ
tới trung tâm thành phố. Ta cũng đã thảo luận cách ánh xạ từ một bức ảnh
(với kích thước cố định) tới các dự đoán xác suất nó thuộc vào một số
lượng lớp cố định, hoặc lấy một mã người dùng và mã sản phẩm để dự đoán
số sao xếp loại. Trong những trường hợp này, một khi chúng ta đưa cho mô
hình một đầu vào có độ dài cố định để dự đoán một đầu ra, mô hình ngay
lập tức “quên” dữ liệu nó vừa thấy.



Việc này không ảnh hưởng nhiều nếu đầu vào của mô hình thật sự có cùng
kích thước và nếu các đầu vào liên tiếp thật sự không liên quan đến
nhau. Tuy nhiên chúng ta sẽ xử lý các video như thế nào khi chúng có thể
có số lượng khung hình khác nhau? Sự thật là dự đoán của chúng ta về
việc gì đang xảy ra ở mỗi khung hình sẽ chính xác hơn nếu quan sát thêm
các khung hình kề nó. Hiện tượng tương tự xảy ra trong ngôn ngữ. Một bài
toán học sâu phổ biến là dịch máy (\sphinxstyleemphasis{machine translation}): tác vụ lấy
đầu vào là các câu trong một ngôn ngữ nguồn và trả về bản dịch của chúng
ở một ngôn ngữ khác.



Các bài toán này cũng xảy ra trong y khoa. Với một mô hình theo dõi bệnh
nhân trong chế độ đặc biệt, ta có thể mong muốn nó đưa ra cảnh báo nếu
nguy cơ tử vong trong 24 giờ tới vượt một ngưỡng nào đó. Dĩ nhiên, ta
chắc chắn không muốn mô mình này bỏ qua mọi lịch sử bệnh lý và chỉ đưa
ra dự đoán dựa trên các thông số gần nhất.



Những bài toán này nằm trong những ứng dụng thú vị nhất của học máy và
chúng là các ví dụ của \sphinxstyleemphasis{học chuỗi}. Chúng đòi hỏi một mô hình có khả
năng lấy một chuỗi các đầu vào hoặc đưa ra một chuỗi các đầu ra (hoặc cả
hai!). Những bài toán này này đôi khi được gọi là \sphinxcode{\sphinxupquote{seq2seq}}. Dịch ngôn
ngữ là một bài toán \sphinxcode{\sphinxupquote{seq2seq}}. Phiên thoại từ một bài nói thành chữ
cũng là một bài toán \sphinxcode{\sphinxupquote{seq2seq}}. Mặc dù không thể xét hết mọi dạng của
biến đổi chuỗi, một vài trường hợp đặc biệt rất đáng được lưu tâm:







\sphinxstylestrong{Gán thẻ và Phân tích cú pháp}. Đây là bài toán gán chú thích cho
chuỗi các từ. Nói cách khác, số lượng đầu vào và đầu ra là như nhau. Ví
dụ, ta có thể muốn biết vị trí của động từ và chủ ngữ. Hoặc ta cũng có
thể muốn biết từ nào là danh từ riêng. Mục đích nói chung là phân tích
và chú thích các từ dựa trên các giả sử về cấu trúc và ngữ pháp. Việc
này nghe có vẻ phức tạp hơn vẻ bề ngoài của nó. Dưới đây là một ví dụ
đơn giản về việc chú thích một câu với thẻ để chỉ ra từ nào là các danh
từ riêng (Ent).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Tom has dinner in Washington with Sally.
Ent  \PYGZhy{}    \PYGZhy{}    \PYGZhy{}     Ent      \PYGZhy{}    Ent
\end{sphinxVerbatim}



\sphinxstylestrong{Tự động nhận dạng giọng nói}. Với nhận dạng giọng nói, chuỗi đầu vào
\(\mathbf{x}\) là một bản thu âm của một người
(\hyperref[\detokenize{chapter_introduction/index_vn:fig-speech}]{Fig.\@ \ref{\detokenize{chapter_introduction/index_vn:fig-speech}}}) và đầu ra \(y\) là một đoạn chữ ghi lại
những gì người đó nói. Thử thách ở đây là việc có rất nhiều các khung âm
thanh (âm thanh thường được lấy mẫu ở 8kHz or 16kHz) hơn so với chữ,
nghĩa là không có một phép ánh xạ 1:1 nào giữa âm thanh và chữ viết, bởi
vì một tiếng nói có thể tương ứng với hàng ngàn mẫu âm thanh. Có các bài
toán \sphinxcode{\sphinxupquote{seq2seq}} mà đầu ra ngắn hơn rất nhiều so với đầu vào.

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=700\sphinxpxdimen]{{speech}.png}
\caption{\sphinxcode{\sphinxupquote{-D-e-e-p- L-ea-r-ni-ng-}}}\label{\detokenize{chapter_introduction/index_vn:id15}}\label{\detokenize{chapter_introduction/index_vn:fig-speech}}\end{figure}



\sphinxstylestrong{Chữ ra Tiếng nói} (\sphinxstyleemphasis{Text-to-Speech} hay TTS) là bài toán ngược của
nhận dạng tiếng nói. Nói cách khác, đầu vào \(\mathbf{x}\) là chữ và
đầu ra \(y\) là tệp tin âm thanh. Trong trường hợp này, đầu ra \sphinxstyleemphasis{dài
hơn nhiều} so với đầu vào. Việc nhận dạng các tệp tin âm thanh chất
lượng kém không khó với \sphinxstyleemphasis{con người} nhưng lại không hề đơn giản với máy
tính.



\sphinxstylestrong{Dịch máy}. Không giống trường hợp nhận dạng tiếng nói, ở đó các đầu
vào và đầu ra tương ứng xuất hiện theo cùng thứ tự (sau khi căn chỉnh),
việc đảo trật tự trong dịch máy có thể rất quan trọng. Nói cách khác,
trong khi chúng ta vẫn chuyển đổi từ chuỗi này sang chuỗi khác, ta không
thể giả định số lượng đầu vào và đầu ra cũng như thứ tự của các cặp là
như nhau. Xét ví dụ dưới đây về việc động từ được đặt ở cuối câu trong
tiếng Đức.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Tiếng Đức:           Haben Sie sich schon dieses grossartige Lehrwerk angeschaut?
Tiếng Anh:          Did you already check out this excellent tutorial?
Căn chỉnh sai:  Did you yourself already this excellent tutorial looked\PYGZhy{}at?
\end{sphinxVerbatim}



Rất nhiều bài toán liên quan xuất hiện trong các tác vụ học khác. Ví dụ,
xác định thứ tự ở đó một người dùng đọc một trang mạng là một bài toán
phân tích sắp xếp bố cục hai chiều. Các bài toán hội thoại gặp phải mọi
loại phức tạp khác, như việc xác định câu nói tiếp theo đòi hỏi kiến
thức thực tế cũng như trạng thái trước đó của cuộc hội thoại trong một
khoảng thời gian dài. Vấn đề này đang được nhiều nhà nghiên cứu quan
tâm.








\subsection{\sphinxstyleemphasis{dịch tiêu đề phía trên}}
\label{\detokenize{chapter_introduction/index_vn:dich-tieu-de-phia-tren}}


\sphinxstyleemphasis{dịch đoạn phía trên}



\sphinxstyleemphasis{dịch đoạn phía trên}



\sphinxstyleemphasis{dịch đoạn phía trên}








\subsection{Tương tác với Môi trường}
\label{\detokenize{chapter_introduction/index_vn:tuong-tac-voi-moi-truong}}


Cho tới giờ, chúng ta chưa thảo luận về việc dữ liệu tới từ đâu hoặc
chuyện gì thực sự sẽ \sphinxstyleemphasis{xảy ra} khi một mô hình học máy trả về kết quả dự
đoán. Điều này là do học có giám sát và học không giám sát chưa giải
quyết các vấn đề một cách thấu đáo. Trong cả hai cách học, chúng ta yêu
cầu mô hình học từ một lượng dữ liệu lớn đã được cung cấp từ đầu mà
không cho nó tương tác trở lại với môi trường trong suốt quá trình học.
Bởi vì toàn bộ việc học diễn ra khi thuật toán đã được ngắt kết nối khỏi
môi trường, đôi khi ta gọi đó là \sphinxstyleemphasis{học ngoại tuyến} (\sphinxstyleemphasis{offline learning}).
Quá trình này cho học có giám sát được mô tả trong
\hyperref[\detokenize{chapter_introduction/index_vn:fig-data-collection}]{Fig.\@ \ref{\detokenize{chapter_introduction/index_vn:fig-data-collection}}}.



\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics{{data-collection}.pdf}
\caption{Thu thập dữ liệu từ môi trường cho học có giám sát}\label{\detokenize{chapter_introduction/index_vn:id16}}\label{\detokenize{chapter_introduction/index_vn:fig-data-collection}}\end{figure}



Sự đơn giản của học ngoại tuyến có nét đẹp của nó. Ưu điểm là ta chỉ cần
quan tâm đến vấn đề nhận dạng mẫu mà không bị phân tâm bởi những vấn đề
khác. Nhưng nhược điểm là sự hạn chế trong việc thiết lập các bài toán.
Nếu bạn đã đọc loạt truyện ngắn Robots của Asimov hoặc là người có tham
vọng, bạn có thể đang tưởng tượng ra trí tuệ nhân tạo không những biết
đưa ra dự đoán mà còn có thể tương tác với thế giới. Chúng ta muốn nghĩ
tới những \sphinxstyleemphasis{tác nhân} (\sphinxstyleemphasis{agent}) thông minh chứ không chỉ những mô hình dự
đoán. Tức là ta phải nhắm tới việc chọn \sphinxstyleemphasis{hành động} chứ không chỉ đưa ra
những \sphinxstyleemphasis{dự đoán}. Hơn thế nữa, không giống dự đoán, hành động còn tác
động đến môi trường. Nếu muốn huấn luyện một tác nhân thông mình, chúng
ta phải tính đến cách những hành động của nó có thể tác động đến những
gì nó nhận lại trong tương lai.



Xem xét việc tương tác với môi trường mở ra một loạt những câu hỏi về mô
hình hoá mới. Liệu môi trường có:


\begin{itemize}
\item {} 
Nhớ những gì ta đã làm trước đó?

\item {} 
Muốn giúp đỡ chúng tôi, chẳng hạn: một người dùng đọc văn bản vào một
bộ nhận dạng giọng nói?

\item {} 
Muốn đánh bại chúng tôi, chẳng hạn: một thiết lập đối kháng giống như
bộ lọc thư rác (chống lại những kẻ viết thư rác) hay là chơi game
(với đối thủ)?

\item {} 
Không quan tâm (có rất nhiều trường hợp thế này)?

\item {} 
Có xu hướng thay đổi (dữ liệu trong tương lai có giống với trong quá
khứ không, hay là khuôn mẫu có thay đổi theo thời gian một cách tự
nhiên hoặc do phản ứng với những công cụ tự động)?

\end{itemize}



Câu hỏi cuối cùng nêu lên vấn đề về \sphinxstyleemphasis{dịch chuyển phân phối}
(\sphinxstyleemphasis{distribution shift}) khi dữ liệu huấn luyện và dữ liệu kiểm tra khác
nhau. Vấn đề này giống như khi chúng ta làm bài kiểm tra cho bởi giảng
viên nhưng lại làm bài tập về nhà do trợ giảng chuẩn bị. Chúng ta sẽ
thảo luận sơ qua về học tăng cường (\sphinxstyleemphasis{reinforcement learning}) và học đối
kháng (\sphinxstyleemphasis{adversarial training}), đây là hai thiết lập đặc biệt có xét tới
tương tác với môi trường.








\subsection{Học tăng cường}
\label{\detokenize{chapter_introduction/index_vn:hoc-tang-cuong}}


Nếu bạn muốn dùng học máy để phát triển một tác nhân tương tác với môi
trường và đưa ra hành động, khả năng cao là bạn sẽ cần tập trung vào
\sphinxstyleemphasis{học tăng cường} (\sphinxstyleemphasis{reinforcemnent learning \textendash{} RL}). Học tăng cường còn
các ứng dụng trong ngành công nghệ robot, hệ thống đối thoại và cả việc
phát triển AI cho trò chơi điện tử. \sphinxstyleemphasis{Học sâu tăng cường} (\sphinxstyleemphasis{Deep
reinforcement learning DRL}) áp dụng kĩ thuật học sâu để giải quyết
những vấn để của học tăng cường, đã trở nên phổ biến trong thời gian gần
đây. Hai ví dụ tiêu biểu nhất là thành tựu bứt phá của \sphinxhref{https://www.wired.com/2015/02/google-ai-plays-atari-like-pros/}{mạng-Q sâu đánh
bại con người trong các trò chơi điện tử Atari chỉ sử dụng đầu vào hình
ảnh}%
\begin{footnote}[35]\sphinxAtStartFootnote
\sphinxnolinkurl{https://www.wired.com/2015/02/google-ai-plays-atari-like-pros/}
%
\end{footnote},
và \sphinxhref{https://www.wired.com/2017/05/googles-alphago-trounces-humans-also-gives-boost/}{chương trình AlphaGo chiếm ngôi vô địch thế giới trong môn Cờ
Vây}%
\begin{footnote}[36]\sphinxAtStartFootnote
\sphinxnolinkurl{https://www.wired.com/2017/05/googles-alphago-trounces-humans-also-gives-boost/}
%
\end{footnote}



Học tăng cường mô tả bài toán theo cách rất tổng quát, trong đó tác nhân
tương tác với môi trường qua một chuỗi các \sphinxstyleemphasis{bước thời gian}
(\sphinxstyleemphasis{timesteps}). Tại mỗi bước thời gian \(t\), tác nhân sẽ nhận được
một quan sát \(o_t\) từ môi trường và phải chọn một hành động
\(a_t\) để tương tác với môi trường thông qua một cơ chế nào đó (đôi
khi còn được gọi là bộ dẫn động). Sau cùng, tác nhân sẽ nhận được một
điểm thưởng \(r_t\) từ môi trường. Sau đó, tác nhân lại nhận một
quan sát khác và chọn ra một hành động, cứ tiếp tục như thế. Hành vi của
tác nhân học tăng cường được kiểm soát bởi một \sphinxstyleemphasis{chính sách} (\sphinxstyleemphasis{policy}).
Nói ngắn gọn, một \sphinxstyleemphasis{chính sách} là một hàm ánh xạ từ những quan sát (từ
môi trường) tới các hành động. Mục tiêu của học tăng cường là tạo ra một
chính sách tốt.



\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics{{rl-environment}.pdf}
\caption{Sự tương tác giữa học tăng cường và môi trường}\label{\detokenize{chapter_introduction/index_vn:id17}}\label{\detokenize{chapter_introduction/index_vn:dich-tieu-de-phia-tren-2}}\label{\detokenize{chapter_introduction/index_vn:dich-tieu-de-phia-tren-1}}\end{figure}



Thật khó để nói quá lên sự tổng quát của mô hình học tăng cường. Ví dụ,
ta có thể chuyển bất cứ bài toán học có giám sát nào thành một bài toán
học tăng cường. Chẳng hạn với một bài toán phân loại, ta có thể tạo một
tác nhân học tăng cường với một \sphinxstyleemphasis{hành động} tương ứng với mỗi lớp. Sau
đó ta có thể tạo một môi trường mà trả về điểm thưởng đúng bằng với giá
trị của hàm mất mát từ bài toán học có giám sát ban đầu.







Vì thế, học tăng cường có thể giải quyết nhiều vấn đề mà học có giám sát
không thể. Ví dụ, trong học có giám sát, chúng ta luôn đòi hỏi dữ liệu
huấn luyện đi kèm nhãn tương ứng. Tuy nhiên với học tăng cường, ta không
giả định rằng môi trường sẽ chỉ ra hành động nào là tối ưu tại mỗi quan
sát (điểm dữ liệu). Nhìn chung, mô hình sẽ nhận được một điểm thưởng nào
đó. Tuy nhiên, môi trường có thể sẽ không chỉ ra những hành động nào đã
dẫn tới điểm thưởng đó.



Lấy cờ tướng làm ví dụ. Phần thưởng thật sự sẽ đến vào cuối trò chơi.
Khi thắng, ta sẽ được 1 điểm, hoặc khi thua, ta sẽ nhận về -1 điểm. Vì
vậy, việc học tăng cường phải giải quyết “bài toán phân bố công trạng”:
xác định hành động nào sẽ được thưởng hay bị phạt dựa theo kết quả.
Tương tự như khi một nhân viên được thăng chức vào ngày 11/10. Việc
thăng chức này phản ánh những việc làm có cân nhắc kĩ càng trong suốt
một năm qua. Để thăng chức sau này đòi hỏi quá trình tìm hiểu đâu là
những hành động dẫn đến sự thăng chức này.



Học tăng cường còn phải đương đầu với vấn đề về những quan sát không
hoàn chỉnh. Có nghĩa là quan sát hiện tại có thể không cho bạn biết mọi
thứ về tình trạng lúc này. Lấy ví dụ khi robot hút bụi trong nhà bị kẹt
tại một trong nhiều nhà kho giống y như nhau. Việc can thiệp vào vị trí
chính xác (trạng thái) của robot có thể cần đến những quan sát từ trước
khi nó đi vào phòng.



Cuối cùng, tại một thời điểm bất kỳ, các thuật toán học tăng cường có
thể biết một chính sách tốt, tuy nhiên có thể có những chính sách khác
tốt hơn mà tác nhân chưa bao giờ thử tới. Các thuật toán học tăng cường
phải luôn lựa chọn giữa việc liệu rằng tiếp tục \sphinxstyleemphasis{khai thác} chính sách
tốt nhất hiện thời hay \sphinxstyleemphasis{khám phá} thêm những giải pháp khác, tức bỏ qua
những điểm thưởng ngắn hạn để thu về thêm kiến thức.








\subsubsection{\sphinxstyleemphasis{dịch tiêu đề phía trên}}
\label{\detokenize{chapter_introduction/index_vn:id3}}


\sphinxstyleemphasis{dịch đoạn phía trên}



\sphinxstyleemphasis{dịch đoạn phía trên}








\section{\sphinxstyleemphasis{dịch tiêu đề phía trên}}
\label{\detokenize{chapter_introduction/index_vn:id4}}


\sphinxstyleemphasis{dịch đoạn phía trên}



\sphinxstyleemphasis{dịch đoạn phía trên}



\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=500\sphinxpxdimen]{{koebel}.jpg}
\caption{\sphinxstyleemphasis{dịch chú thích ảnh phía trên}}\label{\detokenize{chapter_introduction/index_vn:id18}}\label{\detokenize{chapter_introduction/index_vn:fig-koebel}}\end{figure}



\sphinxstyleemphasis{dịch đoạn phía trên}



\sphinxstyleemphasis{dịch đoạn phía trên}







\sphinxstyleemphasis{dịch đoạn phía trên}



\sphinxstyleemphasis{dịch đoạn phía trên}



\sphinxstyleemphasis{dịch đoạn phía trên}



\sphinxstyleemphasis{dịch đoạn phía trên}



\sphinxstyleemphasis{dịch đoạn phía trên}



\sphinxstyleemphasis{dịch đoạn phía trên}








\section{Con đường tới Học Sâu}
\label{\detokenize{chapter_introduction/index_vn:con-duong-toi-hoc-sau}}


Rất nhiều trong số này thay đổi với sự sẵn có của lượng lớn dữ liệu nhờ
vào Mạng Lưới Toàn Cầu (World Wide Web), sự phát triển của các công ty
với hàng triệu người dùng trực tuyến, sự phổ biến của các cảm biến rẻ
với chất lượng cao, bộ lưu trữ dữ liệu rẻ (luật Kryder), và tính toán
chi phí thấp (theo luật Moore), đặc biệt là các GPU - mà ban đầu được
thiết kế cho trò chơi máy tính. Bỗng nhiên các thuật toán và mô hình
tưởng chừng như không khả thi về mặt tính toán trở nên có thể. Điều này
được minh họa trong \sphinxcode{\sphinxupquote{tab\_intro\_decade}}.



:Liên hệ giữa tập dữ liệu với bộ nhớ máy tính và năng lực tính toán




\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxthecaptionisattop
\sphinxcaption{label:\sphinxstyleliteralintitle{\sphinxupquote{tab\_intro\_decade}}}\label{\detokenize{chapter_introduction/index_vn:id19}}
\sphinxaftertopcaption
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|}
\hline
\sphinxstyletheadfamily 
Thập kỷ
&\sphinxstyletheadfamily 
Tập dữ liệu
&\sphinxstyletheadfamily 
Bộ nhớ
&\sphinxstyletheadfamily 
Số lượng phép
tính dấu phẩy
động trên giây
\\
\hline
1970
&
100 (Iris)
&
1 KB
&
100 KF (Intel
8080)
\\
\hline
1980
&
1 K (Giá nhà ở
Boston)
&
100 KB
&
1 MF (Intel
80186)
\\
\hline
1990
&
10 K (Nhận dạng
ký tự quang
học)
&
10 MB
&
10 MF (Intel
80486)
\\
\hline
2000
&
10 M (các trang
web)
&
100 MB
&
1 GF (Intel
Core)
\\
\hline
2010
&
10 G (quảng
cáo)
&
1 GB
&
1 TF (Nvidia
C2050)
\\
\hline
2020
&
1 T (mạng xã
hội)
&
100 GB
&
1 PF (Nvidia
DGX-2)
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}



Sự thật là RAM đã không theo kịp với tốc độ phát triển của dữ liệu. Đồng
thời, sự tiến bộ trong năng lực tính toán đã vượt lên sự sẵn có của dữ
liệu. Điều này nghĩa là các mô hình thống kê cần phải trở nên hiệu quả
hơn về bộ nhớ (thường đạt được bằng cách thêm các thành phần phi tuyến)
đồng thời có thể tập trung thời gian cho việc tối ưu các tham số bởi khả
năng tính toán đã tăng. Kéo theo đó, tiêu điểm trong học máy và thống kê
đã dịch chuyển từ các mô hình tuyến tính (tổng quát) và các phương pháp
hạt nhân (\sphinxstyleemphasis{kernel methods}) sang các mạng nơ-ron sâu. Đây cũng là một
trong những lý do những kỹ thuật cổ điển trong học sâu như perceptron đa
tầng \DUrole{bibtex}{{[}McCulloch.Pitts.1943{]}}, mạng nơ-ron tích chập,
\DUrole{bibtex}{{[}LeCun.Bottou.Bengio.ea.1998{]}}, bộ nhớ ngắn hạn dài (\sphinxstyleemphasis{Long
Short-Term Memory} \textendash{} LSTM) \DUrole{bibtex}{{[}Hochreiter.Schmidhuber.1997{]}}, và học
Q \DUrole{bibtex}{{[}Watkins.Dayan.1992{]}}, đã được “tái khám phá” trong thập kỷ
trước, sau khi tương đối chìm trong một khoảng thời gian dài.



Những tiến bộ gần đây trong các mô hình thống kê, các ứng dụng và các
thuật toán đôi khi được liên hệ với Sự bùng nổ kỷ Cambry: thời điểm phát
triển nhanh chóng trong sự tiến hoá của các loài. Thật sự, các kĩ thuật
tiên tiến nhất hiện nay không chỉ đơn thuần là các kĩ thuật cũ được áp
dụng với các nguồn tài nguyên hiện tại. Danh sách dưới đây còn chưa thấm
vào đâu với số lượng những ý tưởng đã và đang giúp các nhà nghiên cứu
đang được những thành tựu khổng lồ trong thập kỉ vừa qua.







\sphinxstyleemphasis{dịch đoạn phía trên}



\sphinxstyleemphasis{dịch đoạn phía trên}



\sphinxstyleemphasis{dịch đoạn phía trên}



\sphinxstyleemphasis{dịch đoạn phía trên}



\sphinxstyleemphasis{dịch đoạn phía trên}



\sphinxstyleemphasis{dịch đoạn phía trên}



\sphinxstyleemphasis{dịch đoạn phía trên}



\sphinxstyleemphasis{dịch đoạn phía trên}








\section{Các câu chuyện thành công}
\label{\detokenize{chapter_introduction/index_vn:cac-cau-chuyen-thanh-cong}}


Trí Tuệ Nhân Tạo có một lịch sử lâu dài trong việc mang đến những kết
quả khó có thể đạt được bằng các phương pháp khác. Ví dụ, thư tín được
sắp xếp sử dụng công nghệ nhận dạng ký tự quang. Những hệ thống này được
triển khai từ những năm 90 (đây là nguồn của các bộ dữ liệu chữ viết tay
nổi tiếng MNIST và USPS). Các hệ thống tương tự áp dụng vào đọc ngân
phiếu nộp tiền vào ngân hàng và tính điểm tín dụng cho ứng viên. Các
giao dịch tài chính được kiểm tra lừa đảo một cách tự động. Đây là nền
tảng phát triển cho rất nhiều hệ thống thanh toán điện tử như Paypal,
Stripe, AliPay, WeChat, Apple, Visa và MasterCard. Các chương trình máy
tính cho cờ tướng đã phát triển hàng thập kỷ. Học máy đứng sau các hệ
thống tìm kiếm, gợi ý, cá nhân hóa và xếp hạng trên mạng Internet. Nói
cách khác, trí tuệ nhân tạo và học máy xuất hiện mọi nơi tuy đôi khi ta
không để ý thấy.



Chỉ tới gần đây AI mới được chú ý đến, chủ yếu là bởi nó cung cấp giải
pháp cho các bài toán mà trước đây được coi là không khả thi.







\sphinxstyleemphasis{dịch đoạn phía trên}



\sphinxstyleemphasis{dịch đoạn phía trên}



\sphinxstyleemphasis{dịch đoạn phía trên}



\sphinxstyleemphasis{dịch đoạn phía trên}



\sphinxstyleemphasis{dịch đoạn phía trên}







Danh sách trên đây chỉ lướt qua những ứng dụng mà học máy có ảnh hưởng
lớn. Ngoài ra, robot, hậu cần, sinh học điện toán, vật lý hạt và thiên
văn học cũng tạo ra những thành quả ấn tượng gần đây phần nào nhờ vào
học máy. Bởi vậy, Học Máy đang trở thành một công cụ phổ biến cho các kỹ
sư và nhà khoa học.



Gần đây, câu hỏi về ngày tận thế do AI, hay điểm kì dị (\sphinxstyleemphasis{singularity})
của AI đã được nhắc tới trong các bài viết phi kỹ thuật về AI. Đã có
những nỗi lo sợ về việc các hệ thống học máy bằng cách nào đó sẽ trở nên
có cảm xúc và ra quyết định độc lập với những lập trình viên (và chủ
nhân) về những điều ảnh hưởng trực tiếp tới sự sống của nhân loại. Trong
phạm vi nào đó, AI đã ảnh hưởng tới sự sống của con người một cách trực
tiếp, chẳng hạn như điểm tín dụng được tính tự động, tự động điều hướng
xe hơi, hay các quyết định về việc liệu có chấp nhận bảo lãnh hay không
sử dụng đầu vào là dữ liệu thống kê. Phù phiếm hơn, ta có thể yêu cầu
Alexa bật máy pha cà phê.



May mắn thay, chúng ta còn xa một hệ thống AI có cảm xúc sẵn sàng điều
khiển chủ nhân của nó (hay đốt cháy cà phê của họ). Thứ nhất, các hệ
thống AI được thiết kế, huấn luyện và triển khai trong một môi trường cụ
thể hướng mục đích. Trong khi hành vi của chúng có thể tạo ra ảo giác về
trí tuệ phổ quát, đó là một tổ hợp của các quy tắc và các mô hình thống
kê. Thứ hai, hiện tại các công cụ cho \sphinxstyleemphasis{trí tuệ nhân tạo phổ quát} đơn
giản là không tồn tại. Chúng không thể tự cải thiện, hoài nghi bản thân,
không thể thay đổi, mở rộng và tự cải thiện cấu trúc trong khi cố gắng
giải quyết các tác vụ thông thường.



Một mối lo cấp bách hơn đó là AI có thể được sử dụng trong cuộc sống
thường nhật như thế nào. Nhiều khả năng rất nhiều tác vụ đơn giản đang
được thực hiện bởi tài xế xe tải và trợ lý cửa hàng có thể và sẽ bị tự
động hóa. Các robot nông trại sẽ không những có khả năng làm giảm chi
phí của nông nghiệp hữu cơ mà còn tự động hóa quá trình thu hoạch. Thời
điểm này của cuộc cách mạng công nghiệp có thể có những hậu quả lan rộng
khắp toàn xã hội (tài xế xe tải và trợ lý cửa hàng là một vài trong số
những ngành phổ biến nhất ở nhiều địa phương). Hơn nữa, các mô hình
thống kê, khi được áp dụng không cẩn thận, có thể dẫn đến các quyết định
phân biệt chủng tộc, giới tính hoặc tuổi tác và gây nên những nỗi lo có
cơ sở về tính công bằng nếu chúng được tự động hóa để đưa ra các quyết
định có nhiều hệ lụy. Việc sử dụng các thuật toán này một cách cẩn thận
là rất quan trọng. Với những gì ta biết ngày nay, việc này dấy lên một
nỗi lo lớn hơn tiềm năng hủy diệt loài người của các hệ siêu trí tuệ.








\section{Tóm tắt}
\label{\detokenize{chapter_introduction/index_vn:tom-tat}}

\begin{itemize}
\item {} 
Học máy nghiên cứu cách các hệ thống máy tính tận dụng \sphinxstyleemphasis{kinh nghiệm}
(thường là dữ liệu) để cải thiện cất lượng trong những tác vụ cụ thể.
Nó bao gồm các ý tưởng từ thống kê, khai phá dữ liệu, trí tuệ nhân
tạo và tối ưu hóa. Nó thường được sử dụng như một công cụ để triển
khai các giải pháp trí tuệ nhân tạo.

\item {} 
Là một nhóm về học máy, học biểu diễn (\sphinxstyleemphasis{representational learning})
tập trung vào cách tự động tìm kiếm một cách thích hợp để biểu diễn
dữ liệu. Điều này thường đạt được bằng cách học một quá trình biến
đổi gồm nhiều bước.

\item {} 
Rất nhiều tiến triển gần đây trong học sâu được kích hoạt bởi một
lượng lớn dữ liệu thu được từ các cảm biến giá rẻ và các ứng dụng quy
mô Internet, và bởi tiến triển đáng kể trong điện toán, chủ yếu bằng
GPU.

\item {} 
Tối ưu hóa cả hệ thống là một thành phần quan trọng trong việc đạt
được chất lượng tốt. Sự sẵn có của các framework học sâu hiệu quả
giúp việc thiết kế và triển khai việc này dễ hơn một cách đáng kể.

\end{itemize}




\section{Bài tập}
\label{\detokenize{chapter_introduction/index_vn:bai-tap}}

\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Phần nào của mã nguồn mà bạn đang viết có thể “được học”, nghĩa là có
thể được cải thiện bằng cách học và tự động xác định lựa chọn thiết
kế? Mã nguồn của bạn có sử dụng các quy tắc thiết kế dựa trên trực
giác không ?

\item {} 
Những bài toán nào bạn từng gặp có nhiều cách giải quyết, nhưng không
có cách cụ thể nào tự động hóa chúng? Những bài toán này có thể là
ứng viên cho việc áp dụng học sâu.

\item {} 
Từ góc nhìn sự phát triển của trí tuệ nhân tạo như một cuộc cách mạng
công nghiệp mới, mối quan hệ giữa thuật toán và dữ liệu là gì? Nó có
tương tự như máy hơi nước và than đá không (đâu là sự khác nhau căn
bản)?

\item {} 
Bạn có thể áp dụng hướng tiếp cận học đầu-cuối ở đâu? Vật lý? Kỹ
thuật? Kinh tế lượng?

\end{enumerate}




\section{Thảo luận\sphinxfootnotemark[37]}
\label{\detokenize{chapter_introduction/index_vn:thao-luan}}%
\begin{footnotetext}[37]\sphinxAtStartFootnote
\sphinxnolinkurl{https://discuss.mxnet.io/t/2310}
%
\end{footnotetext}\ignorespaces 


\begin{center}\sphinxincludegraphics{{qr_introduction}.pdf}\end{center}




\subsection{Những người thực hiện}
\label{\detokenize{chapter_introduction/index_vn:nhung-nguoi-thuc-hien}}
Bản dịch trong trang này được thực hiện bởi:


\begin{itemize}
\item {} 
Lê Khắc Hồng Phúc

\item {} 
Vũ Hữu Tiệp

\item {} 
Sâm Thế Hải

\item {} 
Hoàng Trọng Tuấn

\end{itemize}


\begin{itemize}
\item {} 
\end{itemize}


\begin{itemize}
\item {} 
\end{itemize}


\begin{itemize}
\item {} 
\end{itemize}


\begin{itemize}
\item {} 
Trần Thị Hồng Hạnh

\end{itemize}


\begin{itemize}
\item {} 
Đoàn Võ Duy Thanh

\item {} 
Phạm Chí Thành

\end{itemize}


\begin{itemize}
\item {} 
\end{itemize}


\begin{itemize}
\item {} 
\end{itemize}


\begin{itemize}
\item {} 
\end{itemize}


\begin{itemize}
\item {} 
\end{itemize}


\begin{itemize}
\item {} 
\end{itemize}


\begin{itemize}
\item {} 
\end{itemize}


\begin{itemize}
\item {} 
\end{itemize}


\begin{itemize}
\item {} 
\end{itemize}


\begin{itemize}
\item {} 
\end{itemize}


\begin{itemize}
\item {} 
Mai Sơn Hải

\item {} 
\end{itemize}


\begin{itemize}
\item {} 
Vũ Đình Quyền

\item {} 
Nguyễn Cảnh Thướng

\end{itemize}


\begin{itemize}
\item {} 


\item {} 
\end{itemize}


\begin{itemize}
\item {} 
\end{itemize}


\begin{itemize}
\item {} 
\end{itemize}


\begin{itemize}
\item {} 
Lê Đàm Hồng Lộc

\end{itemize}


\begin{itemize}
\item {} 
\end{itemize}


\begin{itemize}
\item {} 
\end{itemize}


\begin{itemize}
\item {} 
\end{itemize}


\begin{itemize}
\item {} 
\end{itemize}


\begin{itemize}
\item {} 
\end{itemize}


\begin{itemize}
\item {} 
\end{itemize}


\begin{itemize}
\item {} 
\end{itemize}


\begin{itemize}
\item {} 
\end{itemize}


\begin{itemize}
\item {} 
\end{itemize}




\chapter{Phụ lục: Toán học cho Học Sâu}
\label{\detokenize{chapter_appendix_math/index_vn:phu-luc-toan-hoc-cho-hoc-sau}}\label{\detokenize{chapter_appendix_math/index_vn:chap-appendix-math}}\label{\detokenize{chapter_appendix_math/index_vn::doc}}


\sphinxstylestrong{Brent Werness} (\sphinxstyleemphasis{Amazon}), \sphinxstylestrong{Rachel Hu} (\sphinxstyleemphasis{Amazon}), và các tác giả
của cuốn sách này



Một trong những điểm tuyệt vời nhất của học sâu hiện đại là nó có thể
được hiểu và sử dụng mà không cần hiểu cặn kẽ nền tảng toán học đằng
sau. Đây là một dấu hiệu thể hiện lĩnh vực này đã trưởng này. Giống như
hầu hết các nhà phát triển phần mềm không cần bận tâm đến lý thuyết hàm
số khả tính, những người làm việc với học sâu cũng không cần bận tâm đến
nền tảng lý thuyết của học hợp lý cực đại (maximum likelihood).



Tuy nhiên, chúng ta chưa thực sự gần với mức đó.



Trên thực tế, bạn sẽ thi thoảng cần hiểu sự lựa chọn kiến trúc ảnh hưởng
tới dòng gradient như thế nào, hoặc những giả thiết ngầm khi huấn luyện
với một hàm mất mát cụ thể. Bạn có thể cần biết entropy đong đếm thứ gì
trên thế giới, và nó có thể giúp bạn hiểu chính xác số lượng bit trên
một ký tự có ý nghĩa như thế nào trong mô hình của bạn. Tất cả những
điều này đòi hỏi những hiểu biết toán học sâu hơn.



Phần phụ lục này nhằm cung cấp cho bạn nền tảng toán học cần thiết để
hiểu lý thuyết cốt lõi của học sâu hiện đại, nhưng đây không phải là
toàn bộ kiến thức cần thiết. Chúng ta sẽ bắt đầu xem xét đại số tuyến
tính sâu hơn. Chúng tôi phát triển ý nghĩa hình học của các đại lượng và
toán tử đại số tuyến tính, việc này cho phép chúng ta minh hoạ hiệu ứng
của nhiều phép biến đổi dữ liệu. Một thành phần chủ chốt là sự phát
triển của các kiến thức nền tảng liên quan tới phân tích trị riêng.



Tiếp theo, chúng ta phát triển lý thuyết giải tích vi phân để có thể
hiểu cặn kẽ tại sao gradient là hướng hạ dốc nhất, và tại sao lan truyền
ngược có công thức như vậy. Giải tích tích phân được thảo luận tiếp sau
đó ở mức cần thiết để hỗ trợ chủ đề tiếp theo \textendash{} lý thuyết xác suất.



Các vấn đề gặp phải trên thực tế thường không chắc chắn, và bởi vậy
chúng ta cần một ngôn ngữ để nói về những điều không chắc chắn. Chúng ta
sẽ ôn tập lại lý thuyết biến ngẫu nhiên và những phân phối thường gặp
nhất để có thể thảo luận các mô hình dưới góc nhìn xác suất. Việc này
cung cấp nền tảng cho bộ phân loại Naive Bayes, một phương pháp phân
loại dựa trên xác suất.



Liên quan mật thiết tới lý thuyết xác suất là lý thuyết thống kê. Trong
khi thống kê là một mảng quá lớn để ôn tập trong một mục ngắn, chúng tôi
sẽ giới thiệu các khái niệm cơ bản mà mọi người làm học máy cần biết, cụ
thể: đánh giá và so sánh các bộ ước lượng, thực hiện kiểm chứng thống
kê, và xây dựng khoảng tin cậy.



Cuối cùng, chúng ta sẽ thảo luận chủ đề lý thuyết thông tin qua nghiên
cứu toán học về lưu trữ và truyền tải thông tin. Phần này cung cấp ngôn
ngữ cơ bản ở đó chúng ta thảo luận một cách định lượng lượng thông tin
một mô hình hàm chứa.



Kết hợp lại, những kiến thức này định hình những khái niệm toán học cốt
lõi cần thiết để bắt đầu đi tới con đường hiểu sâu về học sâu.






\section{Các phép toán Hình Học và Đại Số Tuyến Tính}
\label{\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:cac-phep-toan-hinh-hoc-va-dai-so-tuyen-tinh}}\label{\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:sec-geometry-linear-algebric-ops}}\label{\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn::doc}}


Trong \sphinxcode{\sphinxupquote{sec\_linear-algebra}}, chúng ta đã đề cập tới những kiến
thức cơ bản trong đại số tuyến tính và cách nó được dùng để thể hiện các
phép biến đổi dữ liệu cơ bản. Đại số tuyến tính là một trong những trụ
cột toán học chính hỗ trợ học sâu và rộng hơn là học máy. Trong khi
\sphinxcode{\sphinxupquote{sec\_linear-algebra}} chứa đựng đầy đủ kiến thức cần thiết cho
các mô hình học sâu hiện đại, vẫn còn rất nhiều điều cần thảo luận trong
lĩnh vực này. Trong mục này, chúng ta sẽ đi sâu hơn, nhấn mạnh một số
diễn giải hình học của các phép toán đại số tuyến tính, và giới thiệu
một vài khái niệm cơ bản, bao gồm trị riêng và vector riêng.




\subsection{Ý nghĩa hình học của Vector}
\label{\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:y-nghia-hinh-hoc-cua-vector}}


Trước hết, chúng ta cần thảo luận hai diễn giải hình học phổ biến của
vector: điểm hoặc hướng trong không gian. Về cơ bản, một vector là một
danh sách các số giống như danh sách trong Python dưới đây:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{v} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{7}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}
\end{sphinxVerbatim}



Các nhà toán học thường viết chúng dưới dạng một vector \sphinxstyleemphasis{cột} hoặc
\sphinxstyleemphasis{hàng}, tức:
\begin{equation}\label{equation:chapter_appendix_math/geometry-linear-algebric-ops_vn:chapter_appendix_math/geometry-linear-algebric-ops_vn:0}
\begin{split}\mathbf{x} = \begin{bmatrix}1\\7\\0\\1\end{bmatrix},\end{split}
\end{equation}


hoặc
\begin{equation}\label{equation:chapter_appendix_math/geometry-linear-algebric-ops_vn:chapter_appendix_math/geometry-linear-algebric-ops_vn:1}
\begin{split}\mathbf{x}^\top = \begin{bmatrix}1 & 7 & 0 & 1\end{bmatrix}.\end{split}
\end{equation}


Những biểu diễn này thường có những cách diễn giải khác nhau. Các điểm
dữ liệu được biểu diễn bằng các vector cột và các trọng số dùng trong
các tổng có trọng số được biểu diễn bằng các vector hàng. Tuy nhiên,
việc linh động sử dụng các cách biểu diễn này mang lại nhiều lợi ích. Ma
trận là những cấu trúc dữ liệu hữu ích: chúng cho phép chúng ta tổ chức
dữ liệu với nhiều biến thể khác nhau. Ví dụ, các hàng của ma trận có thể
tương ứng với các nhà (điểm dữ liệu) khác nhau, trong khi các cột có thể
tương ứng với các thuộc tính khác nhau. Việc này nghe quen thuộc nếu bạn
từng sử dụng các phần mềm dạng bảng (spreadsheet) hoặc đã từng đọc
\sphinxcode{\sphinxupquote{sec\_pandas}}. Bởi vậy, mặc dù chiều mặc định của một vector là
một vector cột, trong một ma trận biểu diễn một tập dữ liệu dạng bảng,
sẽ thuận tiện hơn khi coi mỗi điểm dữ liệu là một vector hàng trong ma
trận đó. Và như chúng ta sẽ thấy trong các chương sau, cách biểu diễn
này phù hợp với cách triển khai các mô hình học sâu. Lấy ví dụ, dọc theo
trục ngoài cùng của một \sphinxcode{\sphinxupquote{ndarray}}, ta có thể truy cập hoặc đếm số
minibatch chứa điểm dữ liệu, hoặc chỉ đơn giản là các điểm dữ liệu nếu
minibatch không tồn tại.







Cách thứ nhất để giải thích một vector là coi nó như một điểm trong
không gian. Trong không gian hai hoặc ba chiều, chúng ta có thể biểu
diễn các điểm này bằng việc sử dụng các thành phần của vector để định
nghĩa vị trí của điểm trong không gian so với một điểm tham chiều được
gọi là \sphinxstyleemphasis{gốc tọa độ}. Xem \hyperref[\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:fig-grid}]{Fig.\@ \ref{\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:fig-grid}}}.



\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics{{GridPoints}.pdf}
\caption{Mô tả việc biểu diễn vector như các điểm trong mặt phẳng. Thành phần
thứ nhất của vector là tọa độ \(x\), thành phần thứ hai là tọa độ
\(y\). Tương tự với số chiều cao hơn, mặc dù khó hình dung hơn}\label{\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:id3}}\label{\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:fig-grid}}\end{figure}



Góc nhìn hình học này cho phép chúng ta xem xét bài toán ở một mức trừu
tượng hơn. Không giống như khi đối mặt với các bài toán khó hình dung
như phân loại ảnh chó mèo, chúng ta có thể bắt đầu xem xét các bài toán
này một cách trừu tượng hơn như là một tập hợp của các điểm trong không
gian. Việc phân loại ảnh chó mèo có thể coi như việc tìm ra cách phân
biệt hai nhóm điểm riêng biệt trong không gian.





Cách thứ hai để giải thích một vector là coi nó như một hướng trong
không gian. Chúng ta không những có thể coi vector
\(\mathbf{v} = [2,3]^\top\) là một điểm nằm bên phải \(2\) đơn
vị và bên trên \(3\) đơn vị so với gốc toạ độ, chúng ta cũng có thể
coi nó thể hiện một hướng \textendash{} hướng \(2\) bước về bên phải và
\(3\) bước lên trên. Theo cách này, ta coi tất cả các vector trong
hình \hyperref[\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:fig-arrow}]{Fig.\@ \ref{\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:fig-arrow}}} là như nhau.



\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics{{ParVec}.pdf}
\caption{Bất kỳ vector nào cũng có thể biểu diễn bằng một mũi tên trong mặt
phẳng. Trong trường hợp này, mọi vector trong hình đều biểu diễn
vector \((2,3)\).}\label{\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:id4}}\label{\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:fig-arrow}}\end{figure}



Một trong những lợi ý của việc chuyển cách hiểu này là phép cộng vector
có thể được hiểu theo nghĩa hình học. Cụ thể, chúng ta đi theo một hướng
được cho bởi một vector, sau đó đi theo một hướng cho bởi một vector
khác, như được cho trong \hyperref[\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:fig-add-vec}]{Fig.\@ \ref{\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:fig-add-vec}}}.



\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics{{VecAdd}.pdf}
\caption{Phép cộng vector có thể biểu diễn bằng cách đầu tiên đi theo một
vector, sau đó đi theo vector kia.}\label{\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:id5}}\label{\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:fig-add-vec}}\end{figure}







Hiệu của hai vector có một cách diễn giải tương tự. Bằng cách biểu diễn
\(\mathbf{u} = \mathbf{v} + (\mathbf{u}-\mathbf{v})\), ta thấy rằng
vector \(\mathbf{u}-\mathbf{v}\) là hướng mang điểm
\(\mathbf{u}\) tới điểm \(\mathbf{v}\).




\subsection{Tích vô hướng và Góc}
\label{\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:tich-vo-huong-va-goc}}


Như đã thấy trong \sphinxcode{\sphinxupquote{sec\_linear-algebra}}, tích vô hướng của hai
vector cột \(\mathbf{u}\) và \(\mathbf{v}\) có thể được tính
bởi:
\begin{equation}\label{equation:chapter_appendix_math/geometry-linear-algebric-ops_vn:eq_dot_def}
\begin{split}\mathbf{u}^\top\mathbf{v} = \sum_i u_i\cdot v_i.\end{split}
\end{equation}


Vì biểu thức \eqref{equation:chapter_appendix_math/geometry-linear-algebric-ops_vn:eq_dot_def} đối xứng, chúng ta có thể viết:
\begin{equation}\label{equation:chapter_appendix_math/geometry-linear-algebric-ops_vn:chapter_appendix_math/geometry-linear-algebric-ops_vn:2}
\begin{split}\mathbf{u}\cdot\mathbf{v} = \mathbf{u}^\top\mathbf{v} = \mathbf{v}^\top\mathbf{u},\end{split}
\end{equation}


để nhấn mạnh rằng phép đổi chỗ hai vector sẽ cho kết quả như nhau.



Tích vô hướng \eqref{equation:chapter_appendix_math/geometry-linear-algebric-ops_vn:eq_dot_def} cũng có diễn giải hình học: nó liên
quan mật thiết tới góc giữa hai vector. Xem góc hiển thị trong
\hyperref[\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:fig-angle}]{Fig.\@ \ref{\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:fig-angle}}}.



\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics{{VecAngle}.pdf}
\caption{Có một định nghĩa về góc (\(\theta\)) giữa hai vector bất kỳ
trong không gian. Ta sẽ thấy rằng góc này có liên hệ chặt chẽ tới
tích vô hướng.}\label{\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:id6}}\label{\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:fig-angle}}\end{figure}



Xét hai vector:
\begin{equation}\label{equation:chapter_appendix_math/geometry-linear-algebric-ops_vn:chapter_appendix_math/geometry-linear-algebric-ops_vn:3}
\begin{split}\mathbf{v} = (r,0) \; \text{and} \; \mathbf{w} = (s\cos(\theta), s \sin(\theta)).\end{split}
\end{equation}


Vector \(\mathbf{v}\) có độ dài \(r\) và song song với trục
\(x\), vector \(\mathbf{w}\) có độ dài \(s\) và tạo một góc
\(\theta\) với trục \(x\). Nếu tính tích vô hướng của hai vector
này, ta sẽ thấy rằng




\begin{equation}\label{equation:chapter_appendix_math/geometry-linear-algebric-ops_vn:chapter_appendix_math/geometry-linear-algebric-ops_vn:4}
\begin{split}\mathbf{v}\cdot\mathbf{w} = rs\cos(\theta) = \|\mathbf{v}\|\|\mathbf{w}\|\cos(\theta).\end{split}
\end{equation}


Với một vài biến đổi đơn giản, chúng ta có thể sắp xếp lại các thành
phần để được
\begin{equation}\label{equation:chapter_appendix_math/geometry-linear-algebric-ops_vn:chapter_appendix_math/geometry-linear-algebric-ops_vn:5}
\begin{split}\theta = \arccos\left(\frac{\mathbf{v}\cdot\mathbf{w}}{\|\mathbf{v}\|\|\mathbf{w}\|}\right).\end{split}
\end{equation}


Một cách ngắn gọn, với hai vector cụ thể này, tích vô hướng kết hợp với
chuẩn thể hiện góc giữa hai vector. Việc này cũng đúng trong trường hợp
tổng quát. Ta sẽ không viết biểu diễn ở đây, tuy nhiên, nếu viết
\(\|\mathbf{v} - \mathbf{w}\|^2\) bằng hai cách: cách thứ nhất với
tích vô hướng, và cách thứ hai sử dụng công thức tính cos, ta có thể
thấy được quan hệ giữa chúng. Thật vậy, với hai vector
\(\mathbf{v}\) và \(\mathbf{w}\) bất kỳ, góc giữa chúng là
\begin{equation}\label{equation:chapter_appendix_math/geometry-linear-algebric-ops_vn:eq_angle_forumla}
\begin{split}\theta = \arccos\left(\frac{\mathbf{v}\cdot\mathbf{w}}{\|\mathbf{v}\|\|\mathbf{w}\|}\right).\end{split}
\end{equation}


Kết quả này tổng quát cho không gian nhiều chiều vì nó không sử dụng
điều gì đặc biệt trong không gian hai chiều.



Xét ví dụ đơn giản tính góc giữa cặp vector:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZpc{}}\PYG{n}{matplotlib} \PYG{n}{inline}
\PYG{k+kn}{import} \PYG{n+nn}{d2l}
\PYG{k+kn}{from} \PYG{n+nn}{IPython} \PYG{k+kn}{import} \PYG{n}{display}
\PYG{k+kn}{from} \PYG{n+nn}{mxnet} \PYG{k+kn}{import} \PYG{n}{gluon}\PYG{p}{,} \PYG{n}{np}\PYG{p}{,} \PYG{n}{npx}
\PYG{n}{npx}\PYG{o}{.}\PYG{n}{set\PYGZus{}np}\PYG{p}{(}\PYG{p}{)}

\PYG{k}{def} \PYG{n+nf}{angle}\PYG{p}{(}\PYG{n}{v}\PYG{p}{,} \PYG{n}{w}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arccos}\PYG{p}{(}\PYG{n}{v}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{w}\PYG{p}{)} \PYG{o}{/} \PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{linalg}\PYG{o}{.}\PYG{n}{norm}\PYG{p}{(}\PYG{n}{v}\PYG{p}{)} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linalg}\PYG{o}{.}\PYG{n}{norm}\PYG{p}{(}\PYG{n}{w}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{angle}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}



Chúng ta sẽ không sử dụng đoạn mã này bây giờ, nhưng sẽ hữu ích để biết
rằng nếu góc giữa hai vector là \(\pi/2\) (hay \(90^{\circ}\))
thì hai vector đó được gọi là \sphinxstyleemphasis{trực giao}. Xem xét kỹ biểu thức trên, ta
thấy rằng việc này xảy ra khi \(\theta = \pi/2\), tức
\(\cos(\theta) = 0\). Điều này chứng tỏ tích vô hướng phải bằng
không, và hai vector là trực giao nếu và chỉ nếu
\(\mathbf{v}\cdot\mathbf{w} = 0\). Đẳng thức này sẽ hữu ích khi xem
xét các đối tượng dưới con mắt hình học.



Ta sẽ tự hỏi tại sao tính góc lại hữu ích? Câu trả lời nằm ở tính bất
biến ta mong đợi từ dữ liệu. Xét một bức ảnh, và một bức ảnh thứ hai
giống hệt nhưng với các điểm ảnh với độ sáng chỉ bằng \(10\%\) ảnh
ban đầu. Giá trị của từng điểm ảnh trong ảnh thứ hai nhìn chung khác xa
so với ảnh ban đầu. Bởi vậy, nếu tính khoảng cách giữa ảnh ban đầu và
ảnh tối hơn, khoảng cách có thể rất lớn. Tuy nhiên, trong hầu hết các
ứng dụng ML, \sphinxstyleemphasis{nội dung} của hai bức ảnh là như nhau \textendash{} nó vẫn là một bức
ảnh của một con mèo đối với một bộ phân loại chó mèo. Tuy nhiên, nếu xem
xét góc giữa hai ảnh, không khó để thấy rằng với bất kỳ vector
\(\mathbf{v}\), góc giữa \(\mathbf{v}\) và
\(0.1\cdot\mathbf{v}\) bằng không. Việc này tương ứng với việc nhân
vector với một số (dương) giữ nguyên hướng và chỉ thay đổi độ dài của
vector đó. Khi xét tới góc, hai bức ảnh được coi là như nhau.



Ví dụ tương tự có thể tìm thấy bất cứ đâu. Trong văn bản, chúng ta có
thể muốn chủ đề được thảo luận không thay đổi nếu chúng ta viết văn bản
dài gấp hai nhưng nói về cùng một thứ. Trong một số cách mã hóa (như đếm
số lượng xuất hiện của một từ trong từ điển), việc này tương đương với
nhân đôi vector mã hóa của văn bản, bởi vậy chúng ta lại có thể sử dụng
góc.








\subsubsection{Độ tương tự cosin}
\label{\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:do-tuong-tu-cosin}}


Trong văn cảnh học máy với góc được dùng để chỉ khoảng cách giữa hai
vector, người làm ML sử dụng thuật ngữ \sphinxstyleemphasis{độ tương tự cosin} để chỉ đại
lượng
\begin{equation}\label{equation:chapter_appendix_math/geometry-linear-algebric-ops_vn:chapter_appendix_math/geometry-linear-algebric-ops_vn:6}
\begin{split}\cos(\theta) = \frac{\mathbf{v}\cdot\mathbf{w}}{\|\mathbf{v}\|\|\mathbf{w}\|}.\end{split}
\end{equation}


Hàm cosin lấy giá trị lớn nhất bằng \(1\) khi hai vector chỉ cùng
một hướng, giá trị nhỏ nhất bằng \(-1\) khi chúng cùng phương khác
hướng, và \(0\) khi hai vector trực giao. Chú ý rằng nếu các thành
phần của hai vector nhiều chiều được lấy mẫu ngẫu nhiên với kỳ vọng
\(0\), cosin giữa chúng sẽ luôn gần với \(0\).




\subsection{Siêu phẳng}
\label{\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:sieu-phang}}


Ngoài việc làm việc với vector, một khái niệm quan trọng khác bạn phải
nắm vững khi đi sâu vào đại số tuyến tính là \sphinxstyleemphasis{siêu phẳng}, một khái niệm
tổng quát của đường thẳng (trong không gian hai chiều) hoặc một mặt
phẳng (trong không gian ba chiều). Trong một không gian vector \(d\)
chiều, một siêu phẳng có \(d-1\) chiều và chia không gian thành hai
nửa không gian.



Xét ví dụ sau. Giả sử ta có một vector cột
\(\mathbf{w}=[2,1]^\top\). Ta muốn biết “tập hợp những điểm
\(\mathbf{v}\) sao cho \(\mathbf{w}\cdot\mathbf{v} = 1\)?” Sử
dụng mối quan hệ giữa tích vô hướng và góc ở \eqref{equation:chapter_appendix_math/geometry-linear-algebric-ops_vn:eq_angle_forumla}
ở trên, ta có thể thấy điều này tương đương với
\begin{equation}\label{equation:chapter_appendix_math/geometry-linear-algebric-ops_vn:chapter_appendix_math/geometry-linear-algebric-ops_vn:7}
\begin{split}\|\mathbf{v}\|\|\mathbf{w}\|\cos(\theta) = 1 \; \iff \; \|\mathbf{v}\|\cos(\theta) = \frac{1}{\|\mathbf{w}\|} = \frac{1}{\sqrt{5}}.\end{split}
\end{equation}


\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics{{ProjVec}.pdf}
\caption{Nhắc lại trong lượng giác, chúng ta coi
\(\|\mathbf{v}\|\cos(\theta)\) là độ dài hình chiếu của vector
\(\mathbf{v}\) lên hướng của vector \(\mathbf{w}\)}\label{\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:id7}}\label{\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:fig-vector-project}}\end{figure}



Nếu xem xét ý nghĩa hình học của biểu diễn này, chúng ta thấy rằng việc
này tương đương với việc độ dài hình chiếu của \(\mathbf{v}\) lên
hướng của \(\mathbf{w}\) chính là \(1/\|\mathbf{w}\|\) như được
biểu diễn trong \hyperref[\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:fig-vector-project}]{Fig.\@ \ref{\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:fig-vector-project}}}. Tập hợp các điểm thỏa
mãn điều kiện này là một đường thẳng vuông góc với vector
\(\mathbf{w}\). Ta có thể tìm được phương trình của đường thẳng này
là \(2x + y = 1\) hoặc \(y = 1 - 2x\).







Tiếp theo, nếu tự hỏi về tập hợp các điểm thỏa mãn
\(\mathbf{w}\cdot\mathbf{v} > 1\) hoặc
\(\mathbf{w}\cdot\mathbf{v} < 1\), ta có thể thấy rằng đây là những
trường hợp mà hình chiếu của chúng lên \(\mathbf{w}\) lần lượt dài
hơn hoặc ngắn hơn \(1/\|\mathbf{w}\|\). Vì thế, hai bất phương trình
này định nghĩa hai phía của đường thẳng. Bằng cách này, ta có thể cắt
mặt phẳng thành hai nửa: một nửa chứa tất cả điểm có tích vô hướng nhỏ
hơn một mức ngưỡng và nửa còn lại chứa những điểm có tích vô hướng lớn
hơn mức ngưỡng đó. Xem hình \hyperref[\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:fig-space-division}]{Fig.\@ \ref{\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:fig-space-division}}}.



\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics{{SpaceDivision}.pdf}
\caption{Nếu nhìn từ dạng bất phương trình, ta thấy rằng siêu phẳng (chỉ là
một đường thẳng trong trường hợp này) chia không gian ra thành hai
nửa.}\label{\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:id8}}\label{\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:fig-space-division}}\end{figure}



Tương tự với không gian đa chiều. Nếu lấy
\(\mathbf{w} = [1,2,3]^\top\) và đi tìm các điểm trong không gian ba
chiều với \(\mathbf{w}\cdot\mathbf{v} = 1\), ta có một mặt phẳng
vuông góc với vectơ cho trước \(\mathbf{w}\). Hai bất phương trình
một lần nữa định nghĩa hai phía của mặt bẳng như trong hình
\hyperref[\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:fig-higher-division}]{Fig.\@ \ref{\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:fig-higher-division}}}.



\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics{{SpaceDivision3D}.pdf}
\caption{Siêu phẳng trong bất kì không gian nào chia không gian đó ra thành
hai nửa}\label{\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:id9}}\label{\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:fig-higher-division}}\end{figure}



Mặc dù không thể minh hoạ trong không gian nhiều chiều hơn, ta vẫn có
thể tổng quát điều này cho không gian mười, một trăm hay một tỷ chiều.
Việc này thường xuyên xảy ra khi nghĩ về các mô hình học máy. Chẳng hạn,
ta có thể hiểu các mô hình phân loại tuyến tính trong
\sphinxcode{\sphinxupquote{sec\_softmax}} cũng giống như những phương pháp đi tìm siêu
phẳng để phân chia các lớp mục tiêu khác nhau. Ở trường hợp này, những
siêu phẳng như trên thường được gọi là \sphinxstyleemphasis{mặt phẳng quyết định}. Phần lớn
các mô hình phân loại tìm được qua học sâu đều kết thúc bởi một tầng
tuyến tính và theo sau là một tầng softmax, bởi vậy ta có thể diễn giải
ý nghĩa của mạng nơ-ron sâu giống như việc tìm một embedding phi tuyến
sao cho các lớp mục tiêu có thể được phân chia bởi các siêu phẳng một
cách gọn gàng.



Xét ví dụ sau. Chú ý rằng, ta có thể tạo một mô hình đủ tốt để phân loại
những bức ảnh nhỏ xíu có áo thun và quần từ tập dữ liệu Fashion MNIST
(Xem \sphinxcode{\sphinxupquote{sec\_fashion\_mnist}}) bằng cách lấy vector giữa điểm trung
bình của mỗi lớp để định nghĩa một mặt phẳng quyết định và chọn thủ công
một ngưỡng. Trước tiên, chúng ta tải dữ liệu và tính hai ảnh trung bình:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Load in the dataset}
\PYG{n}{train} \PYG{o}{=} \PYG{n}{gluon}\PYG{o}{.}\PYG{n}{data}\PYG{o}{.}\PYG{n}{vision}\PYG{o}{.}\PYG{n}{FashionMNIST}\PYG{p}{(}\PYG{n}{train}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{test} \PYG{o}{=} \PYG{n}{gluon}\PYG{o}{.}\PYG{n}{data}\PYG{o}{.}\PYG{n}{vision}\PYG{o}{.}\PYG{n}{FashionMNIST}\PYG{p}{(}\PYG{n}{train}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}

\PYG{n}{X\PYGZus{}train\PYGZus{}0} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{stack}\PYG{p}{(}\PYG{p}{[}\PYG{n}{x}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]} \PYG{k}{for} \PYG{n}{x} \PYG{o+ow}{in} \PYG{n}{train} \PYG{k}{if} \PYG{n}{x}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{o}{==} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{n+nb}{float}\PYG{p}{)}
\PYG{n}{X\PYGZus{}train\PYGZus{}1} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{stack}\PYG{p}{(}\PYG{p}{[}\PYG{n}{x}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]} \PYG{k}{for} \PYG{n}{x} \PYG{o+ow}{in} \PYG{n}{train} \PYG{k}{if} \PYG{n}{x}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{o}{==} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{n+nb}{float}\PYG{p}{)}
\PYG{n}{X\PYGZus{}test} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{stack}\PYG{p}{(}
    \PYG{p}{[}\PYG{n}{x}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]} \PYG{k}{for} \PYG{n}{x} \PYG{o+ow}{in} \PYG{n}{test} \PYG{k}{if} \PYG{n}{x}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{o}{==} \PYG{l+m+mi}{0} \PYG{o+ow}{or} \PYG{n}{x}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{o}{==} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{n+nb}{float}\PYG{p}{)}
\PYG{n}{y\PYGZus{}test} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{stack}\PYG{p}{(}
    \PYG{p}{[}\PYG{n}{x}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{k}{for} \PYG{n}{x} \PYG{o+ow}{in} \PYG{n}{test} \PYG{k}{if} \PYG{n}{x}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{o}{==} \PYG{l+m+mi}{0} \PYG{o+ow}{or} \PYG{n}{x}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{o}{==} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{n+nb}{float}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Compute averages}
\PYG{n}{ave\PYGZus{}0} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{X\PYGZus{}train\PYGZus{}0}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{ave\PYGZus{}1} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{X\PYGZus{}train\PYGZus{}1}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
\end{sphinxVerbatim}



Để có cái nhìn rõ hơn, ta có thể biểu diễn các ảnh trung bình này. Trong
trường hợp này, chúng ta thấy rằng ảnh trung bình của áo thun cũng ở
dạng một phiên bản mờ của một chiếc áo thun.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Plot average t\PYGZhy{}shirt}
\PYG{n}{d2l}\PYG{o}{.}\PYG{n}{set\PYGZus{}figsize}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{d2l}\PYG{o}{.}\PYG{n}{plt}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{ave\PYGZus{}0}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{28}\PYG{p}{,} \PYG{l+m+mi}{28}\PYG{p}{)}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Greys}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{d2l}\PYG{o}{.}\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}



Trong trường hợp thứ hai, chúng ta cũng thấy ảnh trung bình có dạng một
ảnh phiên bản mờ một chiếc quần dài.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Plot average trousers}
\PYG{n}{d2l}\PYG{o}{.}\PYG{n}{plt}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{ave\PYGZus{}1}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{28}\PYG{p}{,} \PYG{l+m+mi}{28}\PYG{p}{)}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Greys}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{d2l}\PYG{o}{.}\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}



Trong lời giải học máy hoàn chỉnh, ta sẽ học được mức ngưỡng từ tập dữ
liệu. Trong trường hợp này, tôi chỉ đơn giản chọn thủ công một ngưỡng mà
cho kết quả khá tốt trên tập huấn luyện.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Print test set accuracy with eyeballed threshold}
\PYG{n}{w} \PYG{o}{=} \PYG{p}{(}\PYG{n}{ave\PYGZus{}1} \PYG{o}{\PYGZhy{}} \PYG{n}{ave\PYGZus{}0}\PYG{p}{)}\PYG{o}{.}\PYG{n}{T}
\PYG{n}{predictions} \PYG{o}{=} \PYG{n}{X\PYGZus{}test}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{2000}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{w}\PYG{o}{.}\PYG{n}{flatten}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)} \PYG{o}{\PYGZgt{}} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1500000}

\PYG{c+c1}{\PYGZsh{} Accuracy}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{predictions}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{o}{.}\PYG{n}{dtype}\PYG{p}{)} \PYG{o}{==} \PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{float64}\PYG{p}{)}
\end{sphinxVerbatim}








\subsection{Ý nghĩa hình học của các Phép biến đổi Tuyến tính}
\label{\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:y-nghia-hinh-hoc-cua-cac-phep-bien-doi-tuyen-tinh}}


Thông qua \sphinxcode{\sphinxupquote{sec\_linear-algebra}} và các thảo luận phía trên, ta
có một cái nhìn trọn vẹn về ý nghĩa hình học của vector, độ dài, và góc.
Tuy nhiên, có một khái niệm quan trọng chúng ta đã bỏ qua, đó là ý nghĩa
hình học của các phép biến đổi tuyến tính thể hiện bởi các ma trận. Hiểu
một cách đầy đủ cách ma trận được dùng để biến đổi dữ liệu giữa hai
không gian nhiều chiều khác nhau cần một lượng thực hành đáng kể và nằm
ngoài phạm vi của phần phụ lục này. Tuy nhiên, chúng ta có thể xây dựng
ý niệm trong không gian hai chiều.



Giả sử ta có một ma trận:
\begin{equation}\label{equation:chapter_appendix_math/geometry-linear-algebric-ops_vn:chapter_appendix_math/geometry-linear-algebric-ops_vn:8}
\begin{split}\mathbf{A} = \begin{bmatrix}
a & b \\ c & d
\end{bmatrix}.\end{split}
\end{equation}


Nếu muốn áp dụng ma trận này vào một vector bất kỳ
\(\mathbf{v} = [x, y]^\top\), ta thực hiện phép nhân và thấy rằng
\begin{equation}\label{equation:chapter_appendix_math/geometry-linear-algebric-ops_vn:chapter_appendix_math/geometry-linear-algebric-ops_vn:9}
\begin{split}\begin{aligned}
\mathbf{A}\mathbf{v} & = \begin{bmatrix}a & b \\ c & d\end{bmatrix}\begin{bmatrix}x \\ y\end{bmatrix} \\
& = \begin{bmatrix}ax+by\\ cx+dy\end{bmatrix} \\
& = x\begin{bmatrix}a \\ c\end{bmatrix} + y\begin{bmatrix}b \\d\end{bmatrix} \\
& = x\left\{\mathbf{A}\begin{bmatrix}1\\0\end{bmatrix}\right\} + y\left\{\mathbf{A}\begin{bmatrix}0\\1\end{bmatrix}\right\}.
\end{aligned}\end{split}
\end{equation}


Thoạt nhìn đây là một phép tính khá kì lạ, nó biến một thứ vốn rõ ràng
thành một thứ khó hiểu. Tuy nhiên, điều này cho ta thấy cách một ma trận
biến đổi \sphinxstyleemphasis{bất kỳ} vector nào thông qua cách nó biến đổi \sphinxstyleemphasis{hai vector cụ
thể}: \([1,0]^\top\) và \([0,1]^\top\). Quan sát một chút, chúng
ta thực tế đã thu gọn một bài toán vô hạn (tính toán cho bất kỳ vector
nào) thành một bài toán hữu hạn (tính toán cho chỉ hai vector). Hai
vector này còn có tên gọi khác là vector cơ sở - có nghĩa là vector bất
kỳ nào trong không gian đều có thể biểu diễn dưới dạng tổng có trọng số
của những vector này.







Cùng xét ví dụ với một ma trận cụ thể
\begin{equation}\label{equation:chapter_appendix_math/geometry-linear-algebric-ops_vn:chapter_appendix_math/geometry-linear-algebric-ops_vn:10}
\begin{split}\mathbf{A} = \begin{bmatrix}
1 & 2 \\
-1 & 3
\end{bmatrix}.\end{split}
\end{equation}


Xét vector \(\mathbf{v} = [2, -1]^\top\), ta thấy rằng vector này
chính bằng \(2\cdot[1,0]^\top + -1\cdot[0,1]^\top\), và bởi vậy ta
biết ma trận \(A\) sẽ biến đổi nó thành
\(2(\mathbf{A}[1,0]^\top) + -1(\mathbf{A}[0,1])^\top = 2[1, -1]^\top - [2,3]^\top = [0, -5]^\top\).
Bằng cách xem lưới của tất cả các điểm có tọa độ nguyên, ta có thể thấy
rằng phép nhân ma trận có thể làm xiên, xoay và co giãn lưới đó, nhưng
cấu trúc của lưới phải giữ nguyên như trong
\hyperref[\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:fig-grid-transform}]{Fig.\@ \ref{\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:fig-grid-transform}}}.





\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics{{GridTransform}.pdf}
\caption{Ma trận \(\mathbf{A}\) biến đổi các vector cơ sở cho trước. Hãy
chú ý việc toàn bộ lưới cũng bị biến đổi theo như thế nào.}\label{\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:id10}}\label{\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:fig-grid-transform}}\end{figure}



Đây là điểm quan trọng nhất để hình dung các phép biến đổi tuyến tính
thông qua ma trận. Ma trận không thể làm biến dạng một vài phần của
không gian khác với các phần khác. Chúng chỉ có thể lấy các tọa độ ban
đầu và làm xiên, xoay và co giãn chúng.



Một vài phép biển đổi có thể rất kỳ dị. Chẳng hạn ma trận
\begin{equation}\label{equation:chapter_appendix_math/geometry-linear-algebric-ops_vn:chapter_appendix_math/geometry-linear-algebric-ops_vn:11}
\begin{split}\mathbf{B} = \begin{bmatrix}
2 & -1 \\ 4 & -2
\end{bmatrix},\end{split}
\end{equation}


nén toàn bộ mặt phẳng hai chiều thành một đường thẳng. Xác định và làm
việc với các phép biến đổi này là chủ đề của phần sau, nhưng nhìn trên
khía cạnh hình học, ta có thể thấy rằng điều này cơ bản khác so với các
phép biến đổi ở trên. Ví dụ, kết quả từ ma trận \(\mathbf{A}\) có
thể bị “bẻ cong lại” thành dạng ban đầu. Kết quả từ ma trận
\(\mathbf{B}\) thì không thể vì sẽ không thể biết vector
\([1,2]^\top\) đến từ đâu \textendash{} từ \([1,1]^\top\) hay
\([0, -1]^\top\)?



Trong khi hình vẽ này áp dụng cho ma trận \(2\times2\), kết quả
tương tự cũng có thể được mở rộng cho ma trận bậc cao hơn. Nếu chúng ta
lấy các vector cơ sở như \([1,0, \ldots,0]\) và xem ma trận đó biến
đổi các vector này như thế nào, ta có thể phần nào hình dung được phép
nhân ma trận đã làm biến dạng toàn bộ không gian đa chiều như thế nào.








\subsection{Phụ thuộc Tuyến tính}
\label{\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:phu-thuoc-tuyen-tinh}}


Quay lại với ma trận
\begin{equation}\label{equation:chapter_appendix_math/geometry-linear-algebric-ops_vn:chapter_appendix_math/geometry-linear-algebric-ops_vn:12}
\begin{split}\mathbf{B} = \begin{bmatrix}
2 & -1 \\ 4 & -2
\end{bmatrix}.\end{split}
\end{equation}


Ma trận này nén toàn bộ mặt phẳng xuống thành một đường thằng
\(y = 2x\). Câu hỏi đặt ra là: có cách nào phát hiện ra điều này nếu
chỉ nhìn vào ma trận? Câu trả lời là có thể. Đặt
\(\mathbf{b}_1 = [2,4]^\top\) và
\(\mathbf{b}_2 = [-1, -2]^\top\) là hai cột của \(\mathbf{B}\).
Nhắc lại rằng chúng ta có thể viết bất cứ vector nào được biến đổi bằng
ma trận \(\mathbf{B}\) dưới dạng tổng có trọng số các cột của ma
trận này, chẳng hạn \(a_1\mathbf{b}_1 + a_2\mathbf{b}_2\). Tổng này
được gọi là \sphinxstyleemphasis{tổ hợp tuyến tính} (\sphinxstyleemphasis{linear combination}). Vì
\(\mathbf{b}_1 = -2\cdot\mathbf{b}_2\), ta có thể viết tổ hợp bất kỳ
của hai cột này mà chỉ dùng \(\mathbf{b}_2\):
\begin{equation}\label{equation:chapter_appendix_math/geometry-linear-algebric-ops_vn:chapter_appendix_math/geometry-linear-algebric-ops_vn:13}
\begin{split}a_1\mathbf{b}_1 + a_2\mathbf{b}_2 = -2a_1\mathbf{b}_2 + a_2\mathbf{b}_2 = (a_2-2a_1)\mathbf{b}_2.\end{split}
\end{equation}


Điều này chỉ ra rằng một trong hai cột là dư thừa vì nó không định nghĩa
một hướng độc nhất trong không gian. Việc này cũng không quá bất ngờ bởi
vì ma trận này đã biến toàn bộ mặt phẳng xuống thành một đường thẳng.
Hơn nữa, điều này có thể được nhận thấy do hai cột trên phụ thuộc tuyến
tính \(\mathbf{b}_1 = -2\cdot\mathbf{b}_2\). Để thấy sự đối xứng
giữa hai vector này, ta sẽ viết dưới dạng
\begin{equation}\label{equation:chapter_appendix_math/geometry-linear-algebric-ops_vn:chapter_appendix_math/geometry-linear-algebric-ops_vn:14}
\begin{split}\mathbf{b}_1  + 2\cdot\mathbf{b}_2 = 0.\end{split}
\end{equation}


Tổng quát, ta sẽ nói rằng: một tập hợp các vector
\(\mathbf{v}_1, \ldots \mathbf{v}_k\) là \sphinxstyleemphasis{phụ thuộc tuyến tính} nếu
tồn tại các hệ số \(a_1, \ldots, a_k\) \sphinxstyleemphasis{không đồng thời bằng không}
sao cho
\begin{equation}\label{equation:chapter_appendix_math/geometry-linear-algebric-ops_vn:chapter_appendix_math/geometry-linear-algebric-ops_vn:15}
\begin{split}\sum_{i=1}^k a_i\mathbf{v_i} = 0.\end{split}
\end{equation}


Trong trường hợp này, ta có thể biểu diễn một vector dưới dạng một tổ
hợp nào đó của các vector khác, điều này khiến cho sự tồn tại của nó trở
nên dư thừa. Bởi vậy, sự phụ thuộc tuyến tính giữa các cột của một ma
trận là một bằng chứng cho thấy ma trận đó đang làm giảm số chiều không
gian. Nếu không có sự phụ thuộc tuyến tính, chúng ta nói rằng các vector
này \sphinxstyleemphasis{độc lập tuyến tính} (\sphinxstyleemphasis{linearly independent}). Nếu các cột của một
ma trận là độc lập tuyến tính, không có việc nén nào xảy ra và phép toán
này có thể đảo ngược (khả nghịch) được.








\subsection{Hạng}
\label{\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:hang}}


Với một ma trận tổng quát \(n\times m\), câu hỏi tự nhiên được đặt
ra là ma trận đó ánh xạ vào không gian có bao nhiêu chiều. Để trả lời
cho câu hỏi này, ta dùng khái niệm \sphinxstyleemphasis{hạng} (\sphinxstyleemphasis{rank}). Trong mục trước,
chúng ta lưu ý rằng một hệ phụ thuộc tuyến tính \sphinxstyleemphasis{nén} không gian xuống
một không gian khác với số chiều thấp hơn. Chúng ta sẽ sử dụng tính chất
này để định nghĩa hạng. Cụ thể, hạng của một ma trận \(\mathbf{A}\)
là số lượng lớn nhất các cột độc lập tuyến tính trong mọi tập con các
cột của ma trận đó. Ví dụ, ma trận
\begin{equation}\label{equation:chapter_appendix_math/geometry-linear-algebric-ops_vn:chapter_appendix_math/geometry-linear-algebric-ops_vn:16}
\begin{split}\mathbf{B} = \begin{bmatrix}
2 & 4 \\ -1 & -2
\end{bmatrix},\end{split}
\end{equation}


có \(\mathrm{rank}(B)=1\) vì hai cột của nó là phụ thuộc tuyến tính
và bản thân mỗi cột là không phụ thuộc tuyến tính. Xét một ví dụ phức
tạp hơn
\begin{equation}\label{equation:chapter_appendix_math/geometry-linear-algebric-ops_vn:chapter_appendix_math/geometry-linear-algebric-ops_vn:17}
\begin{split}\mathbf{C} = \begin{bmatrix}
1& 3 & 0 & -1 & 0 \\
-1 & 0 & 1 & 1 & -1 \\
0 & 3 & 1 & 0 & -1 \\
2 & 3 & -1 & -2 & 1
\end{bmatrix},\end{split}
\end{equation}


Ta có thể chứng minh được \(\mathbf{C}\) có hạng bằng hai, bởi hai
cột đầu tiên là độc lập tuyến tính, trong khi tập hợp ba cột bất kỳ
trong ma trận đều phụ thuộc tuyến tính.



Quá trình được mô tả ở trên rất không hiệu quả. Nó đòi hỏi xét mọi tập
con các cột của một ma trận cho trước, số tập con này tăng theo hàm mũ
khi số cột tăng lên. Sau này chúng ta sẽ thấy một cách hiệu quả hơn để
tính hạng của ma trận, nhưng bây giờ những gì được nói đến ở trên là đủ
để hiểu khái niệm và ý nghĩa của hạng.








\subsection{Tính nghịch đảo (khả nghịch)}
\label{\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:tinh-nghich-dao-kha-nghich}}


Như chúng ta đã thấy ở trên, phép nhân một ma trận có các cột phụ thuộc
tuyến tính là không thể hoàn tác, tức là không tồn tại thao tác đảo nào
có thể khôi phục lại đầu vào. Tuy nhiên, nhân một ma trận hạng đầy đủ
(ví dụ, một ma trận \(\mathbf{A}\) kích thước \(n \times n\) nào
đó với hạng \(n\)), chúng ta luôn có thể hoàn tác nó. Xét ma trận
\begin{equation}\label{equation:chapter_appendix_math/geometry-linear-algebric-ops_vn:chapter_appendix_math/geometry-linear-algebric-ops_vn:18}
\begin{split}\mathbf{I} = \begin{bmatrix}
1 & 0 & \cdots & 0 \\
0 & 1 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & 1
\end{bmatrix}.\end{split}
\end{equation}


đây là ma trận với các phần tử trên đường chéo có giá trị 1 và các phẩn
tử còn lại có giá trị 0. Ma trận này được gọi là ma trận \sphinxstyleemphasis{đơn vị}. �Dữ
liệu sẽ không bị thay đổi khi nhân với ma trận này. Để có một ma trận
hoàn tác những gì ma trận \(\mathbf{A}\) đã làm, ta tìm một ma trận
\(\mathbf{A}^{-1}\) sao cho
\begin{equation}\label{equation:chapter_appendix_math/geometry-linear-algebric-ops_vn:chapter_appendix_math/geometry-linear-algebric-ops_vn:19}
\begin{split}\mathbf{A}^{-1}\mathbf{A} = \mathbf{A}\mathbf{A}^{-1} =  \mathbf{I}.\end{split}
\end{equation}


Nếu coi đây là một hệ phương trình, ta có \(n \times n\) biến (các
giá trị của \(\mathbf{A}^{-1}\)) và \(n \times n\) phương trình
(đẳng thức cần thỏa mãn giữa mỗi giá trị của tích
\(\mathbf{A}^{-1}\mathbf{A}\) và giá trị tương ứng của
\(\mathbf{I}\)) nên nhìn chung hệ phương trình có nghiệm. Thật vậy,
phần tiếp theo sẽ giới thiệu một đại lượng được gọi là \sphinxstyleemphasis{định thức} với
tính chất: nghiệm tồn tại khi đại lượng này khác 0. Ma trận
\(\mathbf{A}^{-1}\) như vậy được gọi là ma trận \sphinxstyleemphasis{nghịch đảo}. Ví dụ,
nếu \(\mathbf{A}\) là ma trận \(2 \times 2\)
\begin{equation}\label{equation:chapter_appendix_math/geometry-linear-algebric-ops_vn:chapter_appendix_math/geometry-linear-algebric-ops_vn:20}
\begin{split}\mathbf{A} = \begin{bmatrix}
a & b \\
c & d
\end{bmatrix},\end{split}
\end{equation}


thì nghịch đảo của ma trận này là
\begin{equation}\label{equation:chapter_appendix_math/geometry-linear-algebric-ops_vn:chapter_appendix_math/geometry-linear-algebric-ops_vn:21}
\begin{split} \frac{1}{ad-bc}  \begin{bmatrix}
d & -b \\
-c & a
\end{bmatrix}.\end{split}
\end{equation}


Việc này có thể kiểm chứng bằng công thức ma trận nghịch đảo trình bày ở
trên.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{M} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{M\PYGZus{}inv} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.5}\PYG{p}{,} \PYG{l+m+mf}{0.5}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{M\PYGZus{}inv}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{M}\PYG{p}{)}
\end{sphinxVerbatim}








\subsubsection{Vấn đề tính toán}
\label{\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:van-de-tinh-toan}}
While the inverse of a matrix is useful in theory, we must say that most
of the time we do not wish to \sphinxstyleemphasis{use} the matrix inverse to solve a
problem in practice. In general, there are far more numerically stable
algorithms for solving linear equations like \textendash{}\textgreater{}

Mặc dù ma trận nghịch đảo khá hữu dụng trong lý thuyết, chúng ta nên
tránh \sphinxstyleemphasis{sử dụng} chúng khi giải quyết các bài toán thực tế. Nhìn chung,
có rất nhiều phương pháp tính toán ổn định hơn trong việc giải các
phương trình tuyến tính dạng
\begin{equation}\label{equation:chapter_appendix_math/geometry-linear-algebric-ops_vn:chapter_appendix_math/geometry-linear-algebric-ops_vn:22}
\begin{split}\mathbf{A}\mathbf{x} = \mathbf{b},\end{split}
\end{equation}


so với việc tính ma trận nghịch đảo và thực hiện phép nhân để có
\begin{equation}\label{equation:chapter_appendix_math/geometry-linear-algebric-ops_vn:chapter_appendix_math/geometry-linear-algebric-ops_vn:23}
\begin{split}\mathbf{x} = \mathbf{A}^{-1}\mathbf{b}.\end{split}
\end{equation}


Giống như việc thực hiện phép chia một số nhỏ có thể dẫn đến sự mất ổn
định tính toán, việc nghịch đảo một ma trận gần với hạng thấp cũng đưa
lại hệ quả tương tự.



Thêm vào đó, thông thường ma trận \(\mathbf{A}\) là ma trận \sphinxstyleemphasis{thưa}
(\sphinxstyleemphasis{sparse}), có nghĩa là nó chỉ chứa một số lượng nhỏ các số khác 0. Nếu
thử một vài ví dụ, chúng ta có thể thấy điều này không có nghĩa ma trận
nghịch đảo cũng là một ma trận thưa. Kể cả khi ma trận A là ma trận
\(1\) triệu nhân \(1\) triệu với chỉ \(5\) triệu giá trị
khác 0 (có nghĩa là chúng ta chỉ cần lưu trữ \(5\) triệu giá trị
đó), ma trận nghịch đảo vẫn hầu như có tất cả các thành phần không âm và
đòi hỏi chúng ta phải lưu trữ 1:raw-latex:\sphinxtitleref{text\{M\}{}`\textasciicircum{}2\$ phần tử—tương
đương với :math:{}`1} nghìn tỉ phần tử!



Mặc dù không đủ thời gian để đi sâu vào các vấn đề tính toán phức tạp
thường gặp khi làm việc với đại số tuyến tính, chúng tôi vẫn mong muốn
có thể cung cấp một vài lưu ý, và quy tắc chung trong thực hành là hạn
chế việc tính nghịch đảo.








\subsection{Định thức}
\label{\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:dinh-thuc}}
The geometric view of linear algebra gives an intuitive way to interpret
a a fundamental quantity known as the \sphinxstyleemphasis{determinant}. Consider the grid
image from before, but now with a highlighted region
(\hyperref[\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:fig-grid-filled}]{Fig.\@ \ref{\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:fig-grid-filled}}}). \textendash{}\textgreater{}

Góc nhìn hình học của đại số tuyến tính cung cấp một cái nhìn trực quan
để diễn giải một đại lượng cơ bản được gọi là \sphinxstyleemphasis{định thức}. Xét hình lưới
trước đây với một vùng được tô màu (\hyperref[\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:fig-grid-filled}]{Fig.\@ \ref{\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:fig-grid-filled}}}).



\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics{{GridTransformFilled}.pdf}
\caption{Ma trận \(\mathbf{A}\) vẫn làm biến dạng lưới. Lần này, tôi muốn
dồn sự chú ý vào điều đã xảy ra với hình vuông được tô màu.}\label{\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:id11}}\label{\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:fig-grid-filled}}\end{figure}



Cùng nhìn vào hình vuông được tô màu. Đây là một hình vuông có diện tích
bằng một với các cạnh được tạo bởi \((0, 1)\) và \((1, 0)\). Sau
khi ma trận \(\mathbf{A}\) biến đổi hình vuông này, ta thấy rằng nó
trở thành một hình bình hành. Không có lý do nào để nói hình bình hành
này có cùng diện tích với hình vuông, và trong trường hợp đặc biệt này
\begin{equation}\label{equation:chapter_appendix_math/geometry-linear-algebric-ops_vn:chapter_appendix_math/geometry-linear-algebric-ops_vn:24}
\begin{split}\mathbf{A} = \begin{bmatrix}
1 & -1 \\
2 & 3
\end{bmatrix},\end{split}
\end{equation}


bạn có thể tính được diện tích hình bình hành bằng \(5\) như một bài
tập hình học tọa độ nhỏ.



Tổng quát, nếu ta có một ma trận
\begin{equation}\label{equation:chapter_appendix_math/geometry-linear-algebric-ops_vn:chapter_appendix_math/geometry-linear-algebric-ops_vn:25}
\begin{split}\mathbf{A} = \begin{bmatrix}
a & b \\
c & d
\end{bmatrix},\end{split}
\end{equation}


với một vài phép tính, ta có thể thấy rằng diện tích của hình bình hành
là \(ad-bc\). Diện tích này được coi là \sphinxstyleemphasis{định thức}.



Cùng kiểm tra nhanh điều này với một đoạn mã ví dụ.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{linalg}\PYG{o}{.}\PYG{n}{det}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}



Không khó để nhận ra rằng biểu thức này có thể bằng không hoặc thậm chí
âm. Khi biểu thức này âm, đó là quy ước thường dùng trong toán học: nếu
ma trận đó “lật” một hình, ta nói diện tính bị đảo dấu. Còn khi định
thức bằng không thì sao?



Xét
\begin{equation}\label{equation:chapter_appendix_math/geometry-linear-algebric-ops_vn:chapter_appendix_math/geometry-linear-algebric-ops_vn:26}
\begin{split}\mathbf{B} = \begin{bmatrix}
2 & 4 \\ -1 & -2
\end{bmatrix}.\end{split}
\end{equation}


Nếu ta tính định thức của ma trận này, ta nhận được
\(2\cdot(-2 ) - 4\cdot(-1) = 0\). Điều này là có lý bởi ma trận
\(\mathbf{B}\) đã nén hình vuông ban đầu xuống thành một đoạn thẳng
với diện tích bằng không. Thật vậy, nén một hình xuống không gian mới
với số chiều thấp hơn là cách duy nhất để có diện tích bằng không sau
phép biến đổi. Do đó chúng ta suy ra được kết quả sau: một ma trận
\(A\) khả nghịch nếu và chỉ nếu định thức khác không.



Hãy tưởng tượng ta có một hình bất kỳ trên mặt phẳng. Ta có thể chia nhỏ
hình này thành một tập hợp các hình vuông nhỏ sao cho diện tính của hình
đó bằng số lượng hình vuông trong cách chia này. Bây giờ nếu ta biến đổi
hình đó bằng một ma trận, ta biến đổi các hình vuông nhỏ thành các hình
bình hành với diện tích bằng với định thức của ma trận. Ta thấy rằng với
bất kỳ hình nào, định thức cho ta một con số (có dấu) mà ma trận co giãn
diện tích của một hình bất kỳ.



Việc tính định thức cho các ma trận lớn có thể phức tạp hơn, nhưng ý
tưởng là như nhau. Định thức vẫn có tính chất rằng ma trận
\(n\times n\) co giãn các khối thể tích trong không gian \(n\)
chiều.








\subsection{Tensors và các Phép Toán Đại Số Tuyến Tính Thông Dụng}
\label{\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:tensors-va-cac-phep-toan-dai-so-tuyen-tinh-thong-dung}}


Khái niệm về tensor đã được giới thiệu ở \sphinxcode{\sphinxupquote{sec\_linear-algebra}}.
Trong mục này, chúng ta sẽ đi sâu hơn vào phép co tensor (tương đương
với phép nhân ma trận), và xem cách chúng có thể cung cấp một cái nhìn
nhất quán như thế nào đối với một số phép toán trên ma trận và vector.



Chúng ta đã biết cách nhân với ma trận và vector như thế nào để biến đổi
dữ liệu. Để tensor trở nên hữu ích, ta cần một định nghĩa tương tự như
thế. Xem lại phép nhân ma trận:
\begin{equation}\label{equation:chapter_appendix_math/geometry-linear-algebric-ops_vn:chapter_appendix_math/geometry-linear-algebric-ops_vn:27}
\begin{split}\mathbf{C} = \mathbf{A}\mathbf{B},\end{split}
\end{equation}


hoặc tương đương
\begin{equation}\label{equation:chapter_appendix_math/geometry-linear-algebric-ops_vn:chapter_appendix_math/geometry-linear-algebric-ops_vn:28}
\begin{split}c_{i, j} = \sum_{k} a_{i, k}b_{k, j}.\end{split}
\end{equation}


Cách thức biểu diễn này có thể lặp lại với tensor. Với tensor, không có
một trường hợp tổng quát để chọn tính tổng theo chỉ số nào. Bởi vậy, ta
cần chỉ ra chính xác những chỉ số nào mà ta muốn tính tổng theo. Ví dụ,
ta có thể xét
\begin{equation}\label{equation:chapter_appendix_math/geometry-linear-algebric-ops_vn:chapter_appendix_math/geometry-linear-algebric-ops_vn:29}
\begin{split}y_{il} = \sum_{jk} x_{ijkl}a_{jk}.\end{split}
\end{equation}


Phép biến đổi này được gọi là một phép \sphinxstyleemphasis{co tensor}. Nó có thể biểu diễn
được các phép biến đổi một cách linh động hơn nhiều so với phép nhân ma
trận đơn thuần.



Để đơn giản cho việc ký hiệu, ta có thể để ý rằng tổng chỉ được tính
theo những chỉ số mà xuất hiện nhiều hơn một lần trong biểu thức. Bởi
vậy, người ta thường làm việc với \sphinxstyleemphasis{ký hiệu Einstein} với quy ước rằng
phép tính tổng sẽ được lấy trên các chỉ số xuất hiện nhiều lần. Từ đó,
ta có một phép biểu diễn ngắn gọn:
\begin{equation}\label{equation:chapter_appendix_math/geometry-linear-algebric-ops_vn:chapter_appendix_math/geometry-linear-algebric-ops_vn:30}
\begin{split}y_{il} = x_{ijkl}a_{jk}.\end{split}
\end{equation}







\subsubsection{Một số ví dụ thông dụng trong Đại Số Tuyến Tính}
\label{\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:mot-so-vi-du-thong-dung-trong-dai-so-tuyen-tinh}}


Hãy xem ta có thể biểu diễn bao nhiêu khái niệm đại số tuyến tính đã học
dưới biểu diễn tensor nén gọn này:


\begin{itemize}
\item {} 
\(\mathbf{v} \cdot \mathbf{w} = \sum_i v_iw_i\)

\item {} 
\(\|\mathbf{v}\|_2^{2} = \sum_i v_iv_i\)

\item {} 
\((\mathbf{A}\mathbf{v})_i = \sum_j a_{ij}v_j\)

\item {} 
\((\mathbf{A}\mathbf{B})_{ik} = \sum_j a_{ij}b_{jk}\)

\item {} 
\(\mathrm{tr}(\mathbf{A}) = \sum_i a_{ii}\)

\end{itemize}



Với cách này, ta có thể thay thế hàng loạt ký hiệu chuyên dụng chỉ với
những biểu diễn tensor ngắn.




\subsubsection{Biểu diễn khi lập trình}
\label{\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:bieu-dien-khi-lap-trinh}}


Tensor cũng có thể được thao tác linh hoạt dưới dạng mã. Như đã thấy ở
\sphinxcode{\sphinxupquote{sec\_linear-algebra}}, ta có thể tạo tensor bằng những cách bên
dưới.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Define tensors}
\PYG{n}{B} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{]}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{7}\PYG{p}{,} \PYG{l+m+mi}{8}\PYG{p}{,} \PYG{l+m+mi}{9}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{11}\PYG{p}{,} \PYG{l+m+mi}{12}\PYG{p}{]}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{A} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{v} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Print out the shapes}
\PYG{n}{A}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{,} \PYG{n}{B}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{,} \PYG{n}{v}\PYG{o}{.}\PYG{n}{shape}
\end{sphinxVerbatim}



Phép tính tổng Einstein đã được lập trình thông qua hàm \sphinxcode{\sphinxupquote{np.einsum}}.
Các chỉ số xuất hiện trong phép tổng Einstein có thể được truyền vào
dưới dạng chuỗi ký tự, theo sau là những tensor để thao tác trên đó. Ví
dụ, để thực hiện phép nhân ma trận, ta có thể sử dụng phép tổng Einstein
ở trên (\(\mathbf{A}\mathbf{v} = a_{ij}v_j\)) và tách ra riêng những
indices để có được cài đặt mong muốn:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Reimplement matrix multiplication}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{einsum}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ij, j \PYGZhy{}\PYGZgt{} i}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{A}\PYG{p}{,} \PYG{n}{v}\PYG{p}{)}\PYG{p}{,} \PYG{n}{A}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{v}\PYG{p}{)}
\end{sphinxVerbatim}



Đây là một ký hiệu cực kỳ linh hoạt. Giả sử ta muốn tính toán một phép
tính thường được ghi một cách truyền thống là
\begin{equation}\label{equation:chapter_appendix_math/geometry-linear-algebric-ops_vn:chapter_appendix_math/geometry-linear-algebric-ops_vn:31}
\begin{split}c_{kl} = \sum_{ij} \mathbf{B}_{ijk}\mathbf{A}_{il}v_j.\end{split}
\end{equation}


nó có thể được thực hiện thông qua phép tổng Einstein như sau:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{np}\PYG{o}{.}\PYG{n}{einsum}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ijk, il, j \PYGZhy{}\PYGZgt{} kl}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{B}\PYG{p}{,} \PYG{n}{A}\PYG{p}{,} \PYG{n}{v}\PYG{p}{)}
\end{sphinxVerbatim}



Cách ký hiệu này vừa dễ đọc và hiệu quả cho chúng ta, tuy nhiên lại khá
rườm rà nếu ta cần tạo ra một phép co tensor tự động bằng cách lập
trình. Vì lý do này, \sphinxcode{\sphinxupquote{einsum}} có một cách ký hiệu thay thế bằng cách
cung cấp các chỉ số nguyên cho mỗi tensor. Ví dụ, cùng một phép co
tensor, có thể viết lại bằng:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{np}\PYG{o}{.}\PYG{n}{einsum}\PYG{p}{(}\PYG{n}{B}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,} \PYG{n}{A}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{,} \PYG{n}{v}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}



Cả hai cách ký hiệu đều biểu diễn phép co tensor một cách chính xác và
hiệu quả.








\subsection{\sphinxstyleemphasis{dịch tiêu đề phía trên}}
\label{\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:dich-tieu-de-phia-tren}}\label{\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:dich-tieu-de-phia-tren-1}}


\sphinxstyleemphasis{dịch đoạn phía trên}



\sphinxstyleemphasis{dịch đoạn phía trên}



\sphinxstyleemphasis{dịch đoạn phía trên}



\sphinxstyleemphasis{dịch đoạn phía trên}



\sphinxstyleemphasis{dịch đoạn phía trên}



\sphinxstyleemphasis{dịch đoạn phía trên}



\sphinxstyleemphasis{dịch đoạn phía trên}




\subsection{\sphinxstyleemphasis{dịch tiêu đề phía trên}}
\label{\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:dich-tieu-de-phia-tren-2}}\label{\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:id1}}


\sphinxstyleemphasis{dịch đoạn phía trên}
\begin{equation}\label{equation:chapter_appendix_math/geometry-linear-algebric-ops_vn:chapter_appendix_math/geometry-linear-algebric-ops_vn:32}
\begin{split}\vec v_1 = \begin{bmatrix}
1 \\ 0 \\ -1 \\ 2
\end{bmatrix}, \qquad \vec v_2 = \begin{bmatrix}
3 \\ 1 \\ 0 \\ 1
\end{bmatrix}?\end{split}
\end{equation}


\sphinxstyleemphasis{dịch đoạn phía trên}



\sphinxstyleemphasis{dịch đoạn phía trên}
\begin{equation}\label{equation:chapter_appendix_math/geometry-linear-algebric-ops_vn:chapter_appendix_math/geometry-linear-algebric-ops_vn:33}
\begin{split}\begin{bmatrix}
2 & 3\\
1 & 2
\end{bmatrix}.\end{split}
\end{equation}


\sphinxstyleemphasis{dịch đoạn phía trên}
\begin{itemize}
\item {} 
\(\left\{\begin{pmatrix}1\\0\\-1\end{pmatrix}, \begin{pmatrix}2\\1\\-1\end{pmatrix}, \begin{pmatrix}3\\1\\1\end{pmatrix}\right\}\)

\item {} 
\(\left\{\begin{pmatrix}3\\1\\1\end{pmatrix}, \begin{pmatrix}1\\1\\1\end{pmatrix}, \begin{pmatrix}0\\0\\0\end{pmatrix}\right\}\)

\item {} 
\(\left\{\begin{pmatrix}1\\1\\0\end{pmatrix}, \begin{pmatrix}0\\1\\-1\end{pmatrix}, \begin{pmatrix}1\\0\\1\end{pmatrix}\right\}\)

\end{itemize}



\sphinxstyleemphasis{dịch đoạn phía trên}



\sphinxstyleemphasis{dịch đoạn phía trên}



\sphinxstyleemphasis{dịch đoạn phía trên}




\subsection{\sphinxstyleemphasis{dịch tiêu đề phía trên}}
\label{\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:id2}}


\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics{{qr_geometry-linear-algebric-ops}.pdf}
\caption{\sphinxstyleemphasis{dịch chú thích ảnh phía trên}}\label{\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:id12}}\end{figure}




\subsubsection{Những người thực hiện}
\label{\detokenize{chapter_appendix_math/geometry-linear-algebric-ops_vn:nhung-nguoi-thuc-hien}}
Bản dịch trong trang này được thực hiện bởi:


\begin{itemize}
\item {} 
Vũ Hữu Tiệp

\end{itemize}


\begin{itemize}
\item {} 
Lê Khắc Hồng Phúc

\end{itemize}


\begin{itemize}
\item {} 
\end{itemize}


\begin{itemize}
\item {} 
\end{itemize}


\begin{itemize}
\item {} 
\end{itemize}


\begin{itemize}
\item {} 
Hoàng Trọng Tuấn

\item {} 
Nguyễn Cảnh Thướng

\end{itemize}


\begin{itemize}
\item {} 
Nguyễn Xuân Tú

\end{itemize}


\begin{itemize}
\item {} 
Phạm Hồng Vinh

\end{itemize}


\begin{itemize}
\item {} 
\end{itemize}


\begin{itemize}
\item {} 
\end{itemize}


\begin{itemize}
\item {} 
Trần Thị Hồng Hạnh

\end{itemize}


\begin{itemize}
\item {} 
Nguyễn Lê Quang Nhật

\end{itemize}


\begin{itemize}
\item {} 
\end{itemize}


\begin{itemize}
\item {} 
\end{itemize}


\begin{itemize}
\item {} 
Phạm Hồng Vinh

\item {} 
Lê Khắc Hồng Phúc

\item {} 
Vũ Hữu Tiệp

\end{itemize}


\begin{itemize}
\item {} 
\end{itemize}


\section{Statistics}
\label{\detokenize{chapter_appendix_math/statistics:statistics}}\label{\detokenize{chapter_appendix_math/statistics:sec-statistics}}\label{\detokenize{chapter_appendix_math/statistics::doc}}
Undoubtedly, to be a top deep learning practitioner, the ability to
train the state-of-the-art and high accurate models is crucial. However,
it is often unclear when improvements are significant, or only the
result of random fluctuations in the training process. To be able to
discuss uncertainty in estimated values, we must learn some statistics.

The earliest reference of \sphinxstyleemphasis{statistics} can be traced back to an Arab
scholar Al-Kindi in the \(9^{\mathrm{th}}\)-century, who gave a
detailed description of how to use statistics and frequency analysis to
decipher encrypted messages. After 800 years, the modern statistics
arose from Germany in 1700s, when the researchers focused on the
demographic and economic data collection and analysis. Today, statistics
is the science subject that concerns the collection, processing,
analysis, interpretation and visualization of data. What is more, the
core theory of statistics has been widely used in the research within
academia, industry, and government.

More specifically, statistics can be divided to \sphinxstyleemphasis{descriptive statistics}
and \sphinxstyleemphasis{statistical inference}. The former focus on summarizing and
illustrating the features of a collection of observed data, which is
referred to as a \sphinxstyleemphasis{sample}. The sample is drawn from a \sphinxstyleemphasis{population},
denotes the total set of similar individuals, items, or events of our
experiment interests. Contrary to descriptive statistics, \sphinxstyleemphasis{statistical
inference} further deduces the characteristics of a population from the
given \sphinxstyleemphasis{samples}, based on the assumptions that the sample distribution
can replicate the population distribution at some degree.

You may wonder: “What is the essential difference between machine
learning and statistics?” Fundamentally speaking, statistics focuses on
the inference problem. This type of problems includes modeling the
relationship between the variables, such as causal inference, and
testing the statistically significance of model parameters, such as A/B
testing. In contrast, machine learning emphasizes on making accurate
predictions, without explicitly programming and understanding each
parameter’s functionality.

In this section, we will introduce three types of statistics inference
methods: evaluating and comparing estimators, conducting hypothesis
tests, and constructing confidence intervals. These methods can help us
infer the characteristics of a given population, i.e., the true
parameter \(\theta\). For brevity, we assume that the true parameter
\(\theta\) of a given population is a scalar value. It is
straightforward to extend to the case where \(\theta\) is a vector
or a tensor, thus we omit it in our discussion.


\subsection{Evaluating and Comparing Estimators}
\label{\detokenize{chapter_appendix_math/statistics:evaluating-and-comparing-estimators}}
In statistics, an \sphinxstyleemphasis{estimator} is a function of given samples used to
estimate the true parameter \(\theta\). We will write
\(\hat{\theta}_n = \hat{f}(x_1, \ldots, x_n)\) for the estimate of
\(\theta\) after observing the samples
\{\(x_1, x_2, \ldots, x_n\)\}.

We’ve seen simple examples of estimators before in section
\sphinxcode{\sphinxupquote{sec\_maximum\_likelihood}}. If you have a number of samples from
a Bernoulli random variable, then the maximum likelihood estimate for
the probability the random variable is one can be obtained by counting
the number of ones observed and dividing by the total number of samples.
Similarly, an exercise asked you to show that the maximum likelihood
estimate of the mean of a Gaussian given a number of samples is given by
the average value of all the samples. These estimators will almost never
give the true value of the parameter, but ideally for a large number of
samples the estimate will be close.

As an example, we show below the true density of a Gaussian random
variable with mean zero and variance one, along with a collection
samples from that Gaussian. We constructed the \(y\) coordinate so
every point is visible and the relationship to the original density is
clearer.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{d2l}
\PYG{k+kn}{from} \PYG{n+nn}{mxnet} \PYG{k+kn}{import} \PYG{n}{np}\PYG{p}{,} \PYG{n}{npx}
\PYG{k+kn}{import} \PYG{n+nn}{random}
\PYG{n}{npx}\PYG{o}{.}\PYG{n}{set\PYGZus{}np}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Sample datapoints and create y coordinate}
\PYG{n}{epsilon} \PYG{o}{=} \PYG{l+m+mf}{0.1}
\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{8675309}\PYG{p}{)}
\PYG{n}{xs} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{300}\PYG{p}{,}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{ys} \PYG{o}{=} \PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{exp}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{xs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{:}\PYG{n}{i}\PYG{p}{]} \PYG{o}{\PYGZhy{}} \PYG{n}{xs}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2} \PYG{o}{/} \PYG{p}{(}\PYG{l+m+mi}{2} \PYG{o}{*} \PYG{n}{epsilon}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{)}
             \PYG{o}{/} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}\PYG{o}{*}\PYG{n}{epsilon}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{)} \PYG{o}{/} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{xs}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{xs}\PYG{p}{)}\PYG{p}{)}\PYG{p}{]}

\PYG{c+c1}{\PYGZsh{} Compute true density}
\PYG{n}{xd} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{min}\PYG{p}{(}\PYG{n}{xs}\PYG{p}{)}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{max}\PYG{p}{(}\PYG{n}{xs}\PYG{p}{)}\PYG{p}{,} \PYG{l+m+mf}{0.01}\PYG{p}{)}
\PYG{n}{yd} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{exp}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{n}{xd}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{)} \PYG{o}{/} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(}\PYG{l+m+mi}{2} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Plot the results}
\PYG{n}{d2l}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{xd}\PYG{p}{,} \PYG{n}{yd}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{density}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{d2l}\PYG{o}{.}\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{xs}\PYG{p}{,} \PYG{n}{ys}\PYG{p}{)}
\PYG{n}{d2l}\PYG{o}{.}\PYG{n}{plt}\PYG{o}{.}\PYG{n}{axvline}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{d2l}\PYG{o}{.}\PYG{n}{plt}\PYG{o}{.}\PYG{n}{axvline}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{xs}\PYG{p}{)}\PYG{p}{,} \PYG{n}{linestyle}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZhy{}\PYGZhy{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{purple}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{d2l}\PYG{o}{.}\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Sample Mean: }\PYG{l+s+si}{\PYGZob{}:.2f\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n+nb}{float}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{xs}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{d2l}\PYG{o}{.}\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

There can be many ways to compute an estimator of a parameter
\(\hat{\theta}_n\). In this section, we introduce three common
methods to evaluate and compare estimators: the mean squared error, the
standard deviation, and statistical bias.


\subsubsection{Mean Squared Error}
\label{\detokenize{chapter_appendix_math/statistics:mean-squared-error}}
Perhaps the simplest metric used to evaluate estimators is the \sphinxstyleemphasis{mean
squared error (MSE)} (or \sphinxstyleemphasis{:math:{}`l\_2{}` loss}) of an estimator can be
defined as
\begin{equation}\label{equation:chapter_appendix_math/statistics:eq_mse_est}
\begin{split}\mathrm{MSE} (\hat{\theta}_n, \theta) = E[(\hat{\theta}_n - \theta)^2].\end{split}
\end{equation}
This allows us to quantify the average squared deviation from the true
value. MSE is always non-negative. If you have read
\sphinxcode{\sphinxupquote{sec\_linear\_regression}}, you will recognize it as the most
commonly used regression loss function. As a measure to evaluate an
estimator, the closer its value to zero, the closer the estimator is
close to the true parameter \(\theta\).


\subsubsection{Statistical Bias}
\label{\detokenize{chapter_appendix_math/statistics:statistical-bias}}
The MSE provides a natural metric, but we can easily imagine multiple
different phenomena that might make it large. Two that we will see are
fundamentally important are the fluctuation in the estimator due to
randomness in the dataset, and systematic error in the estimator due to
the estimation procedure.

First, let’s measure the systematic error. For an estimator
\(\hat{\theta}_n\), the mathematical illustration of \sphinxstyleemphasis{statistical
bias} can be defined as
\begin{equation}\label{equation:chapter_appendix_math/statistics:eq_bias}
\begin{split}\mathrm{bias}(\hat{\theta}_n) = E(\hat{\theta}_n - \theta) = E(\hat{\theta}_n) - \theta.\end{split}
\end{equation}
Note that when \(\mathrm{bias}(\hat{\theta}_n) = 0\), the
expectation of the estimator \(\hat{\theta}_n\) is equal to the true
value of parameter. In this case, we say \(\hat{\theta}_n\) is an
unbiased estimator. In general, an unbiased estimator is better than a
biased estimator since its expectation is the same as the true
parameter.

It is worth being aware, however, that biased estimators are frequently
used in practice. There are cases where unbiased estimators do not exist
without further assumptions, or are intractable to compute. This may
seem like a significant flaw in an estimator, however the majority of
estimators encountered in practice are at least asymptotically unbiased
in the sense that the bias tends to zero as the number of available
samples tends to infinity:
\(\lim_{n \rightarrow \infty} \mathrm{bias}(\hat{\theta}_n) = 0\).


\subsubsection{Variance and Standard Deviation}
\label{\detokenize{chapter_appendix_math/statistics:variance-and-standard-deviation}}
Second, let’s measure the randomness in the estimator. Recall from
\sphinxcode{\sphinxupquote{sec\_random\_variables}}, the \sphinxstyleemphasis{standard deviation} (or \sphinxstyleemphasis{standard
error}) is defined as the squared root of the variance. We may measure
the degree of fluctuation of an estimator by measuring the standard
deviation or variance of that estimator.
\begin{equation}\label{equation:chapter_appendix_math/statistics:eq_var_est}
\begin{split}\sigma_{\hat{\theta}_n} = \sqrt{\mathrm{Var} (\hat{\theta}_n )} = \sqrt{E[(\hat{\theta}_n - E(\hat{\theta}_n))^2]}.\end{split}
\end{equation}
It is important to compare \eqref{equation:chapter_appendix_math/statistics_vn:eq_var_est} to
\eqref{equation:chapter_appendix_math/statistics_vn:eq_mse_est}. In this equation we do not compare to the true
population value \(\theta\), but instead to
\(E(\hat{\theta}_n)\), the expected sample mean. Thus we are not
measuring how far the estimator tends to be from the true value, but
instead we measuring the fluctuation of the estimator itself.


\subsubsection{The Bias-Variance Trade-off}
\label{\detokenize{chapter_appendix_math/statistics:the-bias-variance-trade-off}}
It is intuitively clear that these two components contribute to the mean
squared error. What is somewhat shocking is that we can show that this
is actually a \sphinxstyleemphasis{decomposition} of the mean squared error into two
contributions. That is to say that we can write the mean squared error
as the sum of the variance and the square or the bias.
\begin{equation}\label{equation:chapter_appendix_math/statistics:chapter_appendix_math/statistics:0}
\begin{split}\begin{aligned}
\mathrm{MSE} (\hat{\theta}_n, \theta) &= E[(\hat{\theta}_n - E(\hat{\theta}_n) + E(\hat{\theta}_n) - \theta)^2] \\
 &= E[(\hat{\theta}_n - E(\hat{\theta}_n))^2] + E[(E(\hat{\theta}_n) - \theta)^2] \\
 &= \mathrm{Var} (\hat{\theta}_n) + [\mathrm{bias} (\hat{\theta}_n)]^2.\\
\end{aligned}\end{split}
\end{equation}
We refer the above formula as \sphinxstyleemphasis{bias-variance trade-off}. The mean
squared error can be divided into precisely two sources of error: the
error from high bias and the error from high variance. On the one hand,
the bias error is commonly seen in a simple model (such as a linear
regression model), which cannot extract high dimensional relations
between the features and the outputs. If a model suffers from high bias
error, we often say it is \sphinxstyleemphasis{underfitting} or lack of \sphinxstyleemphasis{generalization} as
introduced in (\sphinxcode{\sphinxupquote{sec\_model\_selection}}). On the flip side, the
other error source—high variance usually results from a too complex
model, which overfits the training data. As a result, an \sphinxstyleemphasis{overfitting}
model is sensitive to small fluctuations in the data. If a model suffers
from high variance, we often say it is \sphinxstyleemphasis{overfitting} and lack of
\sphinxstyleemphasis{flexibility} as introduced in (\sphinxcode{\sphinxupquote{sec\_model\_selection}}).


\subsubsection{Evaluating Estimators in Code}
\label{\detokenize{chapter_appendix_math/statistics:evaluating-estimators-in-code}}
Since the standard deviation of an estimator has been implementing in
MXNet by simply calling \sphinxcode{\sphinxupquote{a.std()}} for a \sphinxcode{\sphinxupquote{ndarray}} “a”, we will skip
it but implement the statistical bias and the mean squared error in
MXNet.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Statistical bias}
\PYG{k}{def} \PYG{n+nf}{stat\PYGZus{}bias}\PYG{p}{(}\PYG{n}{true\PYGZus{}theta}\PYG{p}{,} \PYG{n}{est\PYGZus{}theta}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{est\PYGZus{}theta}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{n}{true\PYGZus{}theta}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Mean squared error}
\PYG{k}{def} \PYG{n+nf}{mse}\PYG{p}{(}\PYG{n}{data}\PYG{p}{,} \PYG{n}{true\PYGZus{}theta}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{square}\PYG{p}{(}\PYG{n}{data} \PYG{o}{\PYGZhy{}} \PYG{n}{true\PYGZus{}theta}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

To illustrate the equation of the bias-variance trade-off, let’s
simulate of normal distribution \(\mathcal{N}(\theta, \sigma^2)\)
with \(10,000\) samples. Here, we use a \(\theta = 1\) and
\(\sigma = 4\). As the estimator is a function of the given samples,
here we use the mean of the samples as an estimator for true
\(\theta\) in this normal distribution
\(\mathcal{N}(\theta, \sigma^2)\) .

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{theta\PYGZus{}true} \PYG{o}{=} \PYG{l+m+mi}{1}
\PYG{n}{sigma} \PYG{o}{=} \PYG{l+m+mi}{4}
\PYG{n}{sample\PYGZus{}length} \PYG{o}{=} \PYG{l+m+mi}{10000}
\PYG{n}{samples} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{n}{theta\PYGZus{}true}\PYG{p}{,} \PYG{n}{sigma}\PYG{p}{,} \PYG{n}{sample\PYGZus{}length}\PYG{p}{)}
\PYG{n}{theta\PYGZus{}est} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{samples}\PYG{p}{)}
\PYG{n}{theta\PYGZus{}est}
\end{sphinxVerbatim}

Let’s validate the trade-off equation by calculating the summation of
the squared bias and the variance of our estimator. First, calculate the
MSE of our estimator.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{mse}\PYG{p}{(}\PYG{n}{samples}\PYG{p}{,} \PYG{n}{theta\PYGZus{}true}\PYG{p}{)}
\end{sphinxVerbatim}

Next, we calculate
\(\mathrm{Var} (\hat{\theta}_n) + [\mathrm{bias} (\hat{\theta}_n)]^2\)
as below. As you can see, the two values agree to numerical precision.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{bias} \PYG{o}{=} \PYG{n}{stat\PYGZus{}bias}\PYG{p}{(}\PYG{n}{theta\PYGZus{}true}\PYG{p}{,} \PYG{n}{theta\PYGZus{}est}\PYG{p}{)}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{square}\PYG{p}{(}\PYG{n}{samples}\PYG{o}{.}\PYG{n}{std}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{square}\PYG{p}{(}\PYG{n}{bias}\PYG{p}{)}
\end{sphinxVerbatim}


\subsection{Conducting Hypothesis Tests}
\label{\detokenize{chapter_appendix_math/statistics:conducting-hypothesis-tests}}
The most commonly encountered topic in statistical inference is
hypothesis testing. While hypothesis testing was popularized in the
early 20th century, the first use can be traced back to John Arbuthnot
in the 1700s. John tracked 80-year birth records in London and concluded
that more men were born than women each year. Following that, the modern
significance testing is the intelligence heritage by Karl Pearson who
invented \(p\)-value and Pearson’s chi-squared test), William Gosset
who is the father of Student’s t-distribution, and Ronald Fisher who
initialed the null hypothesis and the significance test.

A \sphinxstyleemphasis{hypothesis test} is a way of evaluating some evidence against the
default statement about a population. We refer the default statement as
the \sphinxstyleemphasis{null hypothesis} \(H_0\), which we try to reject using the
observed data. Here, we use \(H_0\) as a starting point for the
statistical significance testing. The \sphinxstyleemphasis{alternative hypothesis}
\(H_A\) (or \(H_1\)) is a statement that is contrary to the null
hypothesis. A null hypothesis is often stated in a declarative form
which posits a relationship between variables. It should reflect the
brief as explicit as possible, and be testable by statistics theory.

Imagine you are a chemist. After spending thousands of hours in the lab,
you develop a new medicine which can dramatically improve one’s ability
to understand math. To show its magic power, you need to test it.
Naturally, you may need some volunteers to take the medicine and see
whether it can help them learn math better. How do you get started?

First, you will need carefully random selected two groups of volunteers,
so that there is no difference between their math understanding ability
measured by some metrics. The two groups are commonly referred to as the
test group and the control group. The \sphinxstyleemphasis{test group} (or \sphinxstyleemphasis{treatment
group}) is a group of individuals who will experience the medicine,
while the \sphinxstyleemphasis{control group} represents the group of users who are set
aside as a benchmark, i.e., identical environment setups except taking
this medicine. In this way, the influence of all the variables are
minimized, except the impact of the independent variable in the
treatment.

Second, after a period of taking the medicine, you will need to measure
the two groups’ math understanding by the same metrics, such as letting
the volunteers do the same tests after learning a new math formula.
Then, you can collect their performance and compare the results. In this
case, our null hypothesis will be that there is no difference between
the two groups, and our alternate will be that there is.

This is still not fully formal. There are many details you have to think
of carefully. For example, what is the suitable metrics to test their
math understanding ability? How many volunteers for your test so you can
be confident to claim the effectiveness of your medicine? How long
should you run the test? How do you decided if there is a difference
between the two groups? Do you care about the average performance only,
or do you also the range of variation of the scores. And so on.

In this way, hypothesis testing provides framework for experimental
design and reasoning about certainty in observed results. If we can now
show that the null hypothesis is very unlikely to be true, we may reject
it with confidence.

To complete the story of how to work with hypothesis testing, we need to
now introduce some additional terminology and make some of our concepts
above formal.


\subsubsection{Statistical Significance}
\label{\detokenize{chapter_appendix_math/statistics:statistical-significance}}
The \sphinxstyleemphasis{statistical significance} measures the probability of erroneously
reject the null hypothesis, \(H_0\), when it should not be rejected,
i.e.,
\begin{equation}\label{equation:chapter_appendix_math/statistics:chapter_appendix_math/statistics:1}
\begin{split}\text{statistical significance }= 1 - \alpha = P(\text{reject } H_0 \mid H_0 \text{ is true} ).\end{split}
\end{equation}
It is also referred to as the \sphinxstyleemphasis{type I error} or \sphinxstyleemphasis{false positive}. The
\(\alpha\), is called as the \sphinxstyleemphasis{significance level} and its commonly
used value is \(5\%\), i.e., \(1-\alpha = 95\%\). The level of
statistical significance level can be explained as the level of risk
that we are willing to take, when we reject a true null hypothesis.

\hyperref[\detokenize{chapter_appendix_math/statistics:fig-statistical-significance}]{Fig.\@ \ref{\detokenize{chapter_appendix_math/statistics:fig-statistical-significance}}} shows the the observations’
values and probability of a given normal distribution in a two-sample
hypothesis test. If the observation data point is located outsides the
\(95\%\) threshold, it will be a very unlikely observation under the
null hypothesis assumption. Hence, there might be something wrong with
the null hypothesis and we will reject it.

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics{{statistical_significance}.pdf}
\caption{Statistical significance.}\label{\detokenize{chapter_appendix_math/statistics:id3}}\label{\detokenize{chapter_appendix_math/statistics:fig-statistical-significance}}\end{figure}


\subsubsection{Statistical Power}
\label{\detokenize{chapter_appendix_math/statistics:statistical-power}}
The \sphinxstyleemphasis{statistical power} (or \sphinxstyleemphasis{sensitivity}) measures the probability of
reject the null hypothesis, \(H_0\), when it should be rejected,
i.e.,
\begin{equation}\label{equation:chapter_appendix_math/statistics:chapter_appendix_math/statistics:2}
\begin{split}\text{statistical power }= P(\text{reject } H_0  \mid H_0 \text{ is false} ).\end{split}
\end{equation}
Recall that a \sphinxstyleemphasis{type I error} is error caused by rejecting the null
hypothesis when it is true, whereas a \sphinxstyleemphasis{type II error} is resulted from
failing to reject the null hypothesis when it is false. A type II error
is usually denoted as \(\beta\), and hence the corresponding
statistical power is \(1-\beta\).

Intuitively, statistical power can be interpreted as how likely our test
will detect a real discrepancy of some minimum magnitude at a desired
statistical significance level. \(80\%\) is a commonly used
statistical power threshold. The higher the statistical power, the more
likely we are to detect true differences.

One of the most common uses of statistical power is in determining the
number of samples needed. The probability you reject the null hypothesis
when it is false depends on the degree to which it is false (known as
the \sphinxstyleemphasis{effect size}) and the number of samples you have. As you might
expect, small effect sizes will require a very large number of samples
to be detectable with high probability. While beyond the scope of this
brief appendix to derive in detail, as an example, want to be able to
reject a null hypothesis that our sample came from a mean zero variance
one Gaussian, and we believe that our sample’s mean is actually close to
one, we can do so with acceptable error rates with a sample size of only
\(8\). However, if we think our sample population true mean is close
to \(0.01\), then we’d need a sample size of nearly \(80000\) to
detect the difference.

We can imagine the power as a water filter. In this analogy, a high
power hypothesis test is like a high quality water filtration system
that will reduce harmful substances in the water as much as possible. On
the other hand, a smaller discrepancy is like a low quality water
filter, where some relative small substances may easily escape from the
gaps. Similarly, if the statistical power is not of enough high power,
then the test may not catch the smaller discrepancy.


\subsubsection{Test Statistic}
\label{\detokenize{chapter_appendix_math/statistics:test-statistic}}
A \sphinxstyleemphasis{test statistic} \(T(x)\) is a scalar which summarizes some
characteristic of the sample data. The goal of defining such a statistic
is that it should allow us to distinguish between different
distributions and conduct our hypothesis test. Thinking back to our
chemist example, if we wish to show that one population performs better
than the other, it could be reasonable to take the mean as the test
statistic. Different choices of test statistic can lead to statistical
test with drastically different statistical power.

Often, \(T(X)\) (the distribution of the test statistic under our
null hypothesis) will follow, at least approximately, a common
probability distribution such as a normal distribution when considered
under the null hypothesis. If we can derive explicitly such a
distribution, and then measure our test statistic on our dataset, we can
safely reject the null hypothesis if our statistic is far outside the
range that we would expect. Making this quantitative leads us to the
notion of \(p\)-values.


\subsubsection{\protect\(p\protect\)-value}
\label{\detokenize{chapter_appendix_math/statistics:p-value}}
The \sphinxstyleemphasis{:math:{}`p{}`-value} (or the \sphinxstyleemphasis{probability value}) is the probability
that \(T(X)\) is at least as extreme as the observed test statistic
\(T(x)\) assuming that the null hypothesis is \sphinxstyleemphasis{true}, i.e.,
\begin{equation}\label{equation:chapter_appendix_math/statistics:chapter_appendix_math/statistics:3}
\begin{split}p\text{-value} = P_{H_0}(T(X) \geq T(x)).\end{split}
\end{equation}
If the \(p\)-value is smaller than or equal to a pre-defined and
fixed statistical significance level \(\alpha\), we may reject the
null hypothesis. Otherwise, we will conclude that we are lack of
evidence to reject the null hypothesis. For a given population
distribution, the \sphinxstyleemphasis{region of rejection} will be the interval contained
of all the points which has a \(p\)-value smaller than the
statistical significance level \(\alpha\).


\subsubsection{One-side Test and Two-sided Test}
\label{\detokenize{chapter_appendix_math/statistics:one-side-test-and-two-sided-test}}
Normally there are two kinds of significance test: the one-sided test
and the two-sided test. The \sphinxstyleemphasis{one-sided test} (or \sphinxstyleemphasis{one-tailed test}) is
applicable when the null hypothesis and the alternative hypothesis only
have one direction. For example, the null hypothesis may state that the
true parameter \(\theta\) is less than or equal to a value
\(c\). The alternative hypothesis would be that \(\theta\) is
greater than \(c\). That is, the region of rejection is on only one
side of the sampling distribution. Contrary to the one-sided test, the
\sphinxstyleemphasis{two-sided test} (or \sphinxstyleemphasis{two-tailed test}) is applicable when the region of
rejection is on both sides of the sampling distribution. An example in
this case may have a null hypothesis state that the true parameter
\(\theta\) is equal to a value \(c\). The alternative hypothesis
would be that \(\theta\) is not equal to \(c\).


\subsubsection{General Steps of Hypothesis Testing}
\label{\detokenize{chapter_appendix_math/statistics:general-steps-of-hypothesis-testing}}
After getting familiar with the above concepts, let’s go through the
general steps of hypothesis testing.
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
State the question and establish a null hypotheses \(H_0\).

\item {} 
Set the statistical significance level \(\alpha\) and a
statistical power (\(1 - \beta\)).

\item {} 
Obtain samples through experiments. The number of samples needed will
depend on the statistical power, and the expected effect size.

\item {} 
Calculate the test statistic and the \(p\)-value.

\item {} 
Make the decision to keep or reject the null hypothesis based on the
\(p\)-value and the statistical significance level
\(\alpha\).

\end{enumerate}

To conduct a hypothesis test, we start by defining a null hypothesis and
a level of risk that we are willing to take. Then we calculate the test
statistic of the sample, taking an extreme value of the test statistic
as evidence against the null hypothesis. If the test statistic falls
within the reject region, we may reject the null hypothesis in favor of
the alternative.

Hypothesis testing is applicable in a variety of scenarios such as the
clinical trails and A/B testing.


\subsection{Constructing Confidence Intervals}
\label{\detokenize{chapter_appendix_math/statistics:constructing-confidence-intervals}}
When estimating the value of a parameter \(\theta\), point
estimators like \(\hat \theta\) are of limited utility since they
contain no notion of uncertainty. Rather, it would be far better if we
could produce an interval that would contain the true parameter
\(\theta\) with high probability. If you were interested in such
ideas a century ago, then you would have been excited to read “Outline
of a Theory of Statistical Estimation Based on the Classical Theory of
Probability” by Jerzy Neyman \DUrole{bibtex}{{[}Neyman.1937{]}}, who first introduced
the concept of confidence interval in 1937.

To be useful, a confidence interval should be as small as possible for a
given degree of certainty. Let’s see how to derive it.


\subsubsection{Definition}
\label{\detokenize{chapter_appendix_math/statistics:definition}}
Mathematically, a \sphinxstyleemphasis{confidence interval} for the true parameter
\(\theta\) is an interval \(C_n\) that computed from the sample
data such that
\begin{equation}\label{equation:chapter_appendix_math/statistics:eq_confidence}
\begin{split}P_{\theta} (C_n \ni \theta) \geq 1 - \alpha, \forall \theta.\end{split}
\end{equation}
Here \(\alpha \in (0, 1)\), and \(1 - \alpha\) is called the
\sphinxstyleemphasis{confidence level} or \sphinxstyleemphasis{coverage} of the interval. This is the same
\(\alpha\) as the significance level as we discussed about above.

Note that \eqref{equation:chapter_appendix_math/statistics_vn:eq_confidence} is about variable \(C_n\), not
about the fixed \(\theta\). To emphasize this, we write
\(P_{\theta} (C_n \ni \theta)\) rather than
\(P_{\theta} (\theta \in C_n)\).


\subsubsection{Interpretation}
\label{\detokenize{chapter_appendix_math/statistics:interpretation}}
It is very tempting to interpret a \(95\%\) confidence interval as
an interval where you can be \(95\%\) sure the true parameter lies,
however this is sadly not true. The true parameter is fixed, and it is
the interval that is random. Thus a better interpretation would be to
say that if you generated a large number of confidence intervals by this
procedure, \(95\%\) of the generated intervals would contain the
true parameter.

This may seem pedantic, but it can have real implications for the
interpretation of the results. In particular, we may satisfy
\eqref{equation:chapter_appendix_math/statistics_vn:eq_confidence} by constructing intervals that we are \sphinxstyleemphasis{almost
certain} do not contain the true value, as long as we only do so rarely
enough. We close this section by providing three tempting but false
statements. An in-depth discussion of these points can be found in
\DUrole{bibtex}{{[}Morey.Hoekstra.Rouder.ea.2016{]}}.
\begin{itemize}
\item {} 
\sphinxstylestrong{Fallacy 1}. Narrow confidence intervals mean we can estimate the
parameter precisely.

\item {} 
\sphinxstylestrong{Fallacy 2}. The values inside the confidence interval are more
likely to be the true value than those outside the interval.

\item {} 
\sphinxstylestrong{Fallacy 3}. The probability) that a particular observed
\(95\%\) confidence interval contains the true value is
\(95\%\).

\end{itemize}

Sufficed to say, confidence intervals are subtle objects. However, if
you keep the interpretation clear, they can be powerful tools.


\subsubsection{A Gaussian Example}
\label{\detokenize{chapter_appendix_math/statistics:a-gaussian-example}}
Let’s discuss the most classical example, the confidence interval for
the mean of a Gaussian of unknown mean and variance. Suppose we collect
\(n\) samples \(\{x_i\}_{i=1}^n\) from our Gaussian
\(\mathcal{N}(\mu, \sigma^2)\). We can compute estimators for the
mean and standard deviation by taking
\begin{equation}\label{equation:chapter_appendix_math/statistics:chapter_appendix_math/statistics:4}
\begin{split}\hat\mu_n = \frac{1}{n}\sum_{i=1}^n x_i \;\text{and}\; \hat\sigma^2_n = \frac{1}{n-1}\sum_{i=1}^n (x_i - \hat\mu)^2.\end{split}
\end{equation}
If we now consider the random variable
\begin{equation}\label{equation:chapter_appendix_math/statistics:chapter_appendix_math/statistics:5}
\begin{split}T = \frac{\hat\mu_n - \mu}{\hat\sigma_n/\sqrt{n}},\end{split}
\end{equation}
we obtain a random variable following a well-known distribution called
the \sphinxstyleemphasis{Student’s t-distribution on} \(n-1\) \sphinxstyleemphasis{degrees of freedom}.

This distribution is very well studied, and it is known, for instance,
that as \(n\rightarrow \infty\), it is approximately a standard
Gaussian, and thus by looking up values of the Gaussian c.d.f. in a
table, we may conclude that the value of \(T\) is in the interval
\([-1.96, 1.96]\) at least \(95\%\) of the time. For finite
values of \(n\), the interval needs to be somewhat larger, but are
well known and precomputed in tables.

Thus, we may conclude that for large \(n\),
\begin{equation}\label{equation:chapter_appendix_math/statistics:chapter_appendix_math/statistics:6}
\begin{split}P\left(\frac{\hat\mu_n - \mu}{\hat\sigma_n/\sqrt{n}} \in [-1.96, 1.96]\right) \ge 0.95.\end{split}
\end{equation}
Rearranging this by multiplying both sides by
\(\hat\sigma_n/\sqrt{n}\) and then adding \(\hat\mu_n\), we
obtain
\begin{equation}\label{equation:chapter_appendix_math/statistics:chapter_appendix_math/statistics:7}
\begin{split}P\left(\mu \in \left[\hat\mu_n - 1.96\frac{\hat\sigma_n}{\sqrt{n}}, \hat\mu_n + 1.96\frac{\hat\sigma_n}{\sqrt{n}}\right]\right) \ge 0.95.\end{split}
\end{equation}
Thus we know that we have found our \(95\%\) confidence interval:
\begin{equation}\label{equation:chapter_appendix_math/statistics:eq_gauss_confidence}
\begin{split}\left[\hat\mu_n - 1.96\frac{\hat\sigma_n}{\sqrt{n}}, \hat\mu_n + 1.96\frac{\hat\sigma_n}{\sqrt{n}}\right].\end{split}
\end{equation}
It is safe to say that \eqref{equation:chapter_appendix_math/statistics:eq_gauss_confidence} is one of the most
used formula in statistics. Let’s close our discussion of statistics by
implementing it. For simplicity, we assume we are in the asymptotic
regime. Small values of \(N\) should include the correct value of
\sphinxcode{\sphinxupquote{t\_star}} obtained either programmatically or from a \(t\)-table.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Number of samples}
\PYG{n}{N} \PYG{o}{=} \PYG{l+m+mi}{1000}

\PYG{c+c1}{\PYGZsh{} Sample dataset}
\PYG{n}{samples} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{p}{(}\PYG{n}{N}\PYG{p}{,}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Lookup Students\PYGZsq{}s t\PYGZhy{}distribution c.d.f.}
\PYG{n}{t\PYGZus{}star} \PYG{o}{=} \PYG{l+m+mf}{1.96}

\PYG{c+c1}{\PYGZsh{} Construct interval}
\PYG{n}{mu\PYGZus{}hat} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{samples}\PYG{p}{)}
\PYG{n}{sigma\PYGZus{}hat} \PYG{o}{=} \PYG{n}{samples}\PYG{o}{.}\PYG{n}{std}\PYG{p}{(}\PYG{n}{ddof}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{p}{(}\PYG{n}{mu\PYGZus{}hat} \PYG{o}{\PYGZhy{}} \PYG{n}{t\PYGZus{}star}\PYG{o}{*}\PYG{n}{sigma\PYGZus{}hat}\PYG{o}{/}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(}\PYG{n}{N}\PYG{p}{)}\PYG{p}{,} \PYG{n}{mu\PYGZus{}hat} \PYG{o}{+} \PYG{n}{t\PYGZus{}star}\PYG{o}{*}\PYG{n}{sigma\PYGZus{}hat}\PYG{o}{/}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(}\PYG{n}{N}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}


\subsection{Summary}
\label{\detokenize{chapter_appendix_math/statistics:summary}}\begin{itemize}
\item {} 
Statistics focuses on inference problems, whereas deep learning
emphasizes on making accurate predictions without explicitly
programming and understanding.

\item {} 
There are three common statistics inference methods: evaluating and
comparing estimators, conducting hypothesis tests, and constructing
confidence intervals.

\item {} 
There are three most common estimators: statistical bias, standard
deviation, and mean square error.

\item {} 
A confidence interval is an estimated range of a true population
parameter that we can construct by given the samples.

\item {} 
Hypothesis testing is a way of evaluating some evidence against the
default statement about a population.

\end{itemize}


\subsection{Exercises}
\label{\detokenize{chapter_appendix_math/statistics:exercises}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Let
\(X_1, X_2, \ldots, X_n \overset{\text{iid}}{\sim} \mathrm{Unif}(0, \theta)\),
where “iid” stands for \sphinxstyleemphasis{independent and identically distributed}.
Consider the following estimators of \(\theta\):
\begin{equation}\label{equation:chapter_appendix_math/statistics:chapter_appendix_math/statistics:8}
\begin{split}\hat{\theta} = \max \{X_1, X_2, \ldots, X_n \};\end{split}
\end{equation}\begin{equation}\label{equation:chapter_appendix_math/statistics:chapter_appendix_math/statistics:9}
\begin{split}\tilde{\theta} = 2 \bar{X_n} = \frac{2}{n} \sum_{i=1}^n X_i.\end{split}
\end{equation}\begin{itemize}
\item {} 
Find the statistical bias, standard deviation, and mean square
error of \(\hat{\theta}.\)

\item {} 
Find the statistical bias, standard deviation, and mean square
error of \(\tilde{\theta}.\)

\item {} 
Which estimator is better?

\end{itemize}

\item {} 
For our chemist example in introduction, can you derive the 5 steps
to conduct a two-sided hypothesis testing? Given the statistical
significance level \(\alpha = 0.05\) and the statistical power
\(1 - \beta = 0.8\).

\item {} 
Run the confidence interval code with \(N=2\) and
\(\alpha = 0.5\) for \(100\) independently generated dataset,
and plot the resulting intervals (in this case \sphinxcode{\sphinxupquote{t\_star = 1.0}}). You
will see several very short intervals which are very far from
containing the true mean \(0\). Does this contradict the
interpretation of the confidence interval? Do you feel comfortable
using short intervals to indicate high precision estimates?

\end{enumerate}


\subsection{Discussions\sphinxfootnotemark[38]}
\label{\detokenize{chapter_appendix_math/statistics:discussions}}%
\begin{footnotetext}[38]\sphinxAtStartFootnote
\sphinxnolinkurl{https://discuss.mxnet.io/t/5156}
%
\end{footnotetext}\ignorespaces 
\begin{center}\sphinxincludegraphics{{qr_statistics}.pdf}\end{center}


\section{Những người thực hiện}
\label{\detokenize{chapter_appendix_math/index_vn:nhung-nguoi-thuc-hien}}
Bản dịch trong trang này được thực hiện bởi:


\begin{itemize}
\item {} 
Vũ Hữu Tiệp

\item {} 
Lê Khắc Hồng Phúc

\end{itemize}



\renewcommand{\indexname}{Index}
\printindex
\end{document}