# Giới thiệu
:label:`chap_introduction`

Cho đến gần đây, gần như mọi chương trình máy tính mà chúng tôi tương tác với hàng ngày được mã hóa bởi các nhà phát triển phần mềm từ các nguyên tắc đầu tiên. Nói rằng chúng tôi muốn viết một ứng dụng để quản lý một nền tảng thương mại điện tử. Sau khi huddling xung quanh một bảng trắng trong một vài giờ để suy nghĩ về vấn đề, chúng tôi sẽ đưa ra những nét rộng của một giải pháp làm việc có thể trông giống như thế này: (i) người dùng tương tác với các ứng dụng thông qua một giao diện chạy trong một trình duyệt web hoặc ứng dụng di động; (ii) ứng dụng của chúng tôi tương tác với một công cụ cơ sở dữ liệu cấp thương mại để theo dõi trạng thái của từng người dùng và duy trì hồ sơ về các giao dịch lịch sử; và (iii) ở trung tâm của ứng dụng của chúng tôi, * business logic* (bạn có thể nói, *nãi*) ứng dụng của chúng tôi đánh vần chi tiết về phương pháp hành động thích hợp của chúng tôi program chương trình should take in every mỗi conceivable có thể tưởng tượng circumstance hoàn cảnh. 

Để xây dựng bộ não của ứng dụng của chúng tôi, chúng tôi sẽ phải bước qua mọi trường hợp góc có thể mà chúng tôi dự đoán gặp phải, đưa ra các quy tắc phù hợp. Mỗi lần khách hàng nhấp chuột để thêm một mặt hàng vào giỏ hàng của họ, chúng tôi thêm một mục nhập vào bảng cơ sở dữ liệu giỏ hàng, liên kết ID của người dùng đó với ID của sản phẩm được yêu cầu. Trong khi vài nhà phát triển bao giờ nhận được nó hoàn toàn đúng lần đầu tiên (nó có thể mất một số thử nghiệm chạy để làm việc ra những trở ngại), đối với hầu hết các phần, chúng tôi có thể viết một chương trình như vậy từ nguyên tắc đầu tiên và tự tin khởi động nó 
*trước* bao giờ nhìn thấy một khách hàng thực sự.
Khả năng thiết kế các hệ thống tự động của chúng tôi từ các nguyên tắc đầu tiên thúc đẩy các sản phẩm và hệ thống hoạt động, thường là trong các tình huống mới lạ, là một kỳ công nhận thức đáng chú ý. Và khi bạn có thể đưa ra các giải pháp hoạt động $100\% $ thời gian, bạn không nên sử dụng máy học. 

May mắn thay cho cộng đồng ngày càng tăng của các nhà khoa học máy học, nhiều nhiệm vụ mà chúng tôi muốn tự động hóa không dễ uốn cong đến sự khéo léo của con người. Hãy tưởng tượng xoay quanh bảng trắng với những bộ óc thông minh nhất mà bạn biết, nhưng lần này bạn đang giải quyết một trong những vấn đề sau: 

* Viết một chương trình dự đoán thời tiết ngày mai được đưa ra thông tin địa lý, hình ảnh vệ tinh và một cửa sổ theo dõi thời tiết trong quá khứ.
* Viết một chương trình có trong một câu hỏi, thể hiện bằng văn bản dạng tự do, và trả lời nó một cách chính xác.
* Viết một chương trình đưa ra một hình ảnh có thể xác định tất cả những người mà nó chứa, vẽ phác thảo xung quanh mỗi.
* Viết một chương trình trình bày cho người dùng các sản phẩm mà họ có khả năng thưởng thức nhưng không thể, trong quá trình duyệt tự nhiên, để gặp phải.

Trong mỗi trường hợp này, ngay cả các lập trình viên ưu tú cũng không có khả năng mã hóa các giải pháp từ đầu. Những lý do cho điều này có thể khác nhau. Đôi khi chương trình mà chúng tôi đang tìm kiếm theo một mô hình thay đổi theo thời gian và chúng tôi cần các chương trình của mình để điều chỉnh. Trong các trường hợp khác, mối quan hệ (nói giữa pixel và danh mục trừu tượng) có thể quá phức tạp, đòi hỏi hàng ngàn hoặc hàng triệu tính toán vượt quá sự hiểu biết có ý thức của chúng ta ngay cả khi mắt chúng ta quản lý nhiệm vụ dễ dàng.
*Học máy* là nghiên cứu mạnh mẽ
kỹ thuật mà có thể học hỏi từ kinh nghiệm. Là một thuật toán học máy tích lũy nhiều kinh nghiệm hơn, điển hình dưới dạng dữ liệu quan sát hoặc tương tác với một môi trường, hiệu suất của nó được cải thiện. Tương phản điều này với nền tảng thương mại điện tử xác định của chúng tôi, thực hiện theo cùng một logic kinh doanh, bất kể kinh nghiệm tích lũy bao nhiêu, cho đến khi các nhà phát triển tự học và quyết định rằng đã đến lúc cập nhật phần mềm. Trong cuốn sách này, chúng tôi sẽ dạy cho bạn những nguyên tắc cơ bản của máy học và đặc biệt tập trung vào * deep learning*, một bộ kỹ thuật mạnh mẽ thúc đẩy đổi mới trong các lĩnh vực đa dạng như thị giác máy tính, xử lý ngôn ngữ tự nhiên, chăm sóc sức khỏe và bộ gen. 

## Một ví dụ động lực

Trước khi bắt đầu viết, các tác giả của cuốn sách này, giống như nhiều lực lượng lao động, đã phải trở thành caffein. Chúng tôi nhảy vào xe và bắt đầu lái xe. Sử dụng iPhone, Alex gọi ra “Hey Siri”, đánh thức hệ thống nhận dạng giọng nói của điện thoại. Sau đó Mục chỉ huy “hướng đến quán cà phê Blue Bottle”. Điện thoại nhanh chóng hiển thị phiên mã lệnh của mình. Nó cũng nhận ra rằng chúng tôi đang yêu cầu chỉ đường và khởi chạy ứng dụng Maps (ứng dụng) để đáp ứng yêu cầu của chúng tôi. Sau khi ra mắt, ứng dụng Maps đã xác định được một số tuyến đường. Bên cạnh mỗi tuyến đường, điện thoại hiển thị thời gian vận chuyển dự đoán. Trong khi chúng tôi chế tạo câu chuyện này để thuận tiện sư phạm, nó chứng minh rằng trong khoảng thời gian chỉ vài giây, các tương tác hàng ngày của chúng tôi với một chiếc điện thoại thông minh có thể tham gia vào một số mô hình máy học. 

Hãy tưởng tượng chỉ cần viết một chương trình để trả lời một từ *wake* như “Alexa”, “OK Google”, và “Hey Siri”. Hãy thử tự mình mã hóa nó trong một căn phòng mà không có gì ngoài máy tính và trình soạn thảo mã, như minh họa trong :numref:`fig_wake_word`. Làm thế nào bạn sẽ viết một chương trình như vậy từ các nguyên tắc đầu tiên? Hãy suy nghĩ về nó... vấn đề là khó khăn. Mỗi giây, micrô sẽ thu thập khoảng 44000 mẫu. Mỗi mẫu là một phép đo biên độ của sóng âm. Quy tắc nào có thể ánh xạ đáng tin cậy từ một đoạn âm thanh thô để dự đoán tự tin $\{\text{yes}, \text{no}\}$ về việc liệu đoạn mã có chứa từ thức hay không? Nếu bạn bị mắc kẹt, đừng lo lắng. Chúng tôi cũng không biết làm thế nào để viết một chương trình như vậy từ đầu. Đó là lý do tại sao chúng tôi sử dụng máy học. 

![Identify a wake word.](../img/wake-word.svg)
:label:`fig_wake_word`

Đây là mánh khóe. Thông thường, ngay cả khi chúng ta không biết làm thế nào để nói với một máy tính một cách rõ ràng làm thế nào để ánh xạ từ đầu vào đến đầu ra, dù sao chúng ta vẫn có khả năng tự thực hiện chiến công nhận thức. Nói cách khác, ngay cả khi bạn không biết cách lập trình máy tính để nhận ra từ “Alexa”, bản thân bạn có thể nhận ra nó. Được trang bị khả năng này, chúng ta có thể thu thập một bộ dữ liệu* khổng lồ* chứa các ví dụ về âm thanh và gắn nhãn những người làm và không chứa từ thức dậy. Trong phương pháp học máy, chúng tôi không cố gắng thiết kế một hệ thống
*rõ ràng* để nhận ra các từ thức.
Thay vào đó, chúng tôi xác định một chương trình linh hoạt có hành vi được xác định bởi một số tham số*. Sau đó, chúng tôi sử dụng tập dữ liệu để xác định bộ tham số tốt nhất có thể, những thông số cải thiện hiệu suất của chương trình của chúng tôi đối với một số biện pháp hiệu suất đối với nhiệm vụ quan tâm. 

Bạn có thể nghĩ về các tham số như các nút bấm mà chúng ta có thể xoay, thao tác hành vi của chương trình. Sửa các tham số, chúng tôi gọi chương trình là mô hình **. Tập hợp tất cả các chương trình riêng biệt (ánh xạ đầu vào-đầu ra) mà chúng ta có thể tạo ra chỉ bằng cách thao tác các tham số được gọi là *family* của mô hình. Và meta-chương trình sử dụng tập dữ liệu của chúng tôi để chọn các tham số được gọi là thuật toán *learning *. 

Trước khi chúng ta có thể tiếp tục và tham gia vào thuật toán học tập, chúng ta phải xác định chính xác vấn đề, ghim bản chất chính xác của các đầu vào và đầu ra, và chọn một gia đình mô hình thích hợp. Trong trường hợp này, mô hình của chúng tôi nhận được một đoạn âm thanh dưới dạng * đầu vào* và mô hình tạo ra một lựa chọn trong số $\{\text{yes}, \text{no}\}$ dưới dạng * đầu ra*. Nếu tất cả diễn ra theo kế hoạch, các dự đoán của mô hình thường sẽ chính xác về việc đoạn mã có chứa từ đánh thức hay không. 

Nếu chúng ta chọn đúng dòng mô hình, sẽ tồn tại một cài đặt của các nút bấm sao cho mô hình sẽ cháy “có” mỗi khi nó nghe từ “Alexa”. Bởi vì sự lựa chọn chính xác của từ thức là tùy ý, chúng ta có thể sẽ cần một gia đình người mẫu đủ phong phú rằng, thông qua một thiết lập khác của các nút bấm, nó có thể bắn “có” chỉ khi nghe từ “Apricot”. Chúng tôi hy vọng rằng cùng một gia đình mô hình nên phù hợp để công nhận “Alexa” và công nhận “Apricot” bởi vì chúng có vẻ, trực giác, là những nhiệm vụ tương tự. Tuy nhiên, chúng ta có thể cần một dòng mô hình khác hoàn toàn nếu chúng ta muốn đối phó với các đầu vào hoặc đầu ra khác nhau về cơ bản, hãy nói nếu chúng ta muốn ánh xạ từ hình ảnh sang chú thích, hoặc từ câu tiếng Anh sang câu Trung Quốc. 

Như bạn có thể đoán, nếu chúng ta chỉ cần đặt tất cả các nút một cách ngẫu nhiên, không chắc mô hình của chúng tôi sẽ nhận ra “Alexa”, “Apricot” hoặc bất kỳ từ tiếng Anh nào khác. Trong machine learning, * learning* là quá trình mà chúng tôi khám phá ra cài đặt phù hợp của các nút nhấn ép buộc hành vi mong muốn từ mô hình của chúng tôi. Nói cách khác, chúng tôi * đào tạo* mô hình của chúng tôi với dữ liệu. Như thể hiện trong :numref:`fig_ml_loop`, quá trình đào tạo thường trông như sau: 

1. Bắt đầu với một mô hình được khởi tạo ngẫu nhiên mà không thể làm bất cứ điều gì hữu ích.
1. Lấy một số dữ liệu của bạn (ví dụ: đoạn âm thanh và nhãn $\{\text{yes}, \text{no}\}$ tương ứng).
1. Tinh chỉnh các núm để mô hình hút ít hơn so với những ví dụ đó.
1. Lặp lại Bước 2 và 3 cho đến khi mô hình tuyệt vời.

![A typical training process.](../img/ml-loop.svg)
:label:`fig_ml_loop`

Để tóm tắt, thay vì mã hóa một công cụ nhận dạng từ thức, chúng tôi mã hóa một chương trình có thể * learn* để nhận ra các từ thức, nếu chúng tôi trình bày nó với một tập dữ liệu được dán nhãn lớn. Bạn có thể nghĩ đến hành động này để xác định hành vi của một chương trình bằng cách trình bày nó với một tập dữ liệu dưới dạng *lập trình với dữ liệu*. Điều đó có nghĩa là, chúng ta có thể “lập trình” một máy dò mèo bằng cách cung cấp hệ thống học máy của chúng tôi nhiều ví dụ về mèo và chó. Bằng cách này, máy dò cuối cùng sẽ học cách phát ra một số dương rất lớn nếu nó là một con mèo, một số âm rất lớn nếu nó là chó, và một cái gì đó gần bằng không nếu nó không chắc chắn, và điều này hầu như không làm trầy xước bề mặt của những gì máy học có thể làm. Deep learning, mà chúng tôi sẽ giải thích chi tiết hơn sau này, chỉ là một trong số nhiều phương pháp phổ biến để giải quyết các vấn đề máy học. 

## Các thành phần chính

Trong ví dụ từ thức dậy của chúng tôi, chúng tôi đã mô tả một tập dữ liệu bao gồm đoạn âm thanh và nhãn nhị phân, và chúng tôi đã đưa ra một cảm giác lượn sóng tay về cách chúng tôi có thể đào tạo một mô hình để xấp xỉ một ánh xạ từ đoạn trích đến phân loại. Loại vấn đề này, nơi chúng tôi cố gắng dự đoán một nhãn không xác định được chỉ định dựa trên các đầu vào đã biết được đưa ra một tập dữ liệu bao gồm các ví dụ mà các nhãn được biết đến, được gọi là * học được giám sách*. Đây chỉ là một trong số nhiều loại vấn đề máy học. Sau đó chúng ta sẽ đi sâu vào các vấn đề học máy khác nhau. Đầu tiên, chúng tôi muốn làm sáng tỏ nhiều hơn về một số thành phần cốt lõi sẽ theo dõi chúng tôi xung quanh, bất kể chúng tôi gặp phải vấn đề máy học nào: 

1. *dữ liệu* mà chúng ta có thể học hỏi từ.
1. Một* mô hình* về cách chuyển đổi dữ liệu.
1. Chức năng *mục tiêu * định lượng mô hình đang hoạt động tốt như thế nào (hoặc xấu).
1. *algorithm * để điều chỉnh các tham số của mô hình để tối ưu hóa hàm mục tiêu.

### Dữ liệu

Nó có thể đi mà không nói rằng bạn không thể làm khoa học dữ liệu mà không có dữ liệu. Chúng ta có thể mất hàng trăm trang suy ngẫm những gì chính xác cấu thành dữ liệu, nhưng bây giờ, chúng ta sẽ sai về mặt thực tế và tập trung vào các thuộc tính chính cần quan tâm. Nói chung, chúng tôi quan tâm đến một bộ sưu tập các ví dụ. Để làm việc với dữ liệu một cách hữu ích, chúng ta thường cần phải đưa ra một đại diện số phù hợp. Mỗi *ví dụ* (hoặc *data point*, *data instance*, *sample*) thường bao gồm một tập hợp các thuộc tính gọi là *features* (hoặc *covariates*), từ đó mô hình phải đưa ra dự đoán của nó. Trong các vấn đề học tập được giám sát ở trên, điều cần dự đoán là một thuộc tính đặc biệt được chỉ định là *label* (hoặc *target*). 

Nếu chúng ta đang làm việc với dữ liệu hình ảnh, mỗi bức ảnh riêng lẻ có thể tạo thành một ví dụ, mỗi bức ảnh được biểu thị bằng một danh sách có thứ tự các giá trị số tương ứng với độ sáng của mỗi pixel. Một bức ảnh màu $200\times 200$ sẽ bao gồm $200\times200\times3=120000$ giá trị số, tương ứng với độ sáng của các kênh màu đỏ, xanh lá cây và xanh cho mỗi vị trí không gian. Trong một nhiệm vụ truyền thống khác, chúng ta có thể cố gắng dự đoán liệu bệnh nhân có tồn tại hay không, với một tập hợp các tính năng tiêu chuẩn như tuổi tác, dấu hiệu quan trọng và chẩn đoán. 

Khi mỗi ví dụ được đặc trưng bởi cùng một số giá trị số, chúng ta nói rằng dữ liệu bao gồm các vectơ có độ dài cố định và chúng ta mô tả độ dài không đổi của các vectơ là *dimensionality* của dữ liệu. Như bạn có thể tưởng tượng, chiều dài cố định có thể là một tài sản thuận tiện. Nếu chúng ta muốn đào tạo một mô hình để nhận ra ung thư trong hình ảnh kính hiển vi, các đầu vào có độ dài cố định có nghĩa là chúng ta có ít điều cần lo lắng hơn. 

Tuy nhiên, không phải tất cả dữ liệu đều có thể dễ dàng được biểu diễn dưới dạng 
*chiều dài cố định* vectơ.
Mặc dù chúng ta có thể mong đợi hình ảnh kính hiển vi đến từ thiết bị tiêu chuẩn, nhưng chúng ta không thể mong đợi hình ảnh được khai thác từ Internet để tất cả hiển thị với cùng độ phân giải hoặc hình dạng. Đối với hình ảnh, chúng ta có thể xem xét cắt tất cả chúng đến một kích thước tiêu chuẩn, nhưng chiến lược đó chỉ giúp chúng ta cho đến nay. Chúng tôi có nguy cơ mất thông tin trong các phần cắt ra. Hơn nữa, dữ liệu văn bản chống lại các biểu diễn độ dài cố định thậm chí còn bướng bỉnh hơn. Xem xét các đánh giá của khách hàng còn lại trên các trang thương mại điện tử như Amazon, IMDB và TripAdvisor. Một số là ngắn: “nó bốc mùi!”. Những người khác ramble cho các trang. Một lợi thế lớn của học sâu so với các phương pháp truyền thống là ân sủng so sánh mà các mô hình hiện đại có thể xử lý dữ liệu * chiều dài thay đổi*. 

Nói chung, chúng ta càng có nhiều dữ liệu, công việc của chúng ta càng trở nên dễ dàng hơn. Khi chúng ta có nhiều dữ liệu hơn, chúng ta có thể đào tạo các mô hình mạnh mẽ hơn và ít dựa vào các giả định được hình thành trước. Sự thay đổi chế độ từ (tương đối) nhỏ sang dữ liệu lớn là một đóng góp chính cho sự thành công của học sâu hiện đại. Để thúc đẩy điểm về nhà, nhiều mô hình thú vị nhất trong học sâu không hoạt động mà không có bộ dữ liệu lớn. Một số người khác làm việc trong chế độ dữ liệu nhỏ, nhưng không tốt hơn các cách tiếp cận truyền thống. 

Cuối cùng, nó không đủ để có nhiều dữ liệu và xử lý nó một cách khéo léo. Chúng tôi cần dữ liệu *phải*. Nếu dữ liệu đầy những sai lầm hoặc nếu các tính năng được chọn không dự đoán về số lượng mục tiêu quan tâm, việc học sẽ thất bại. Tình hình bị bắt tốt bởi sáo rỗng:
*rác vào, rác ra*.
Hơn nữa, hiệu suất dự đoán kém không phải là hậu quả tiềm năng duy nhất. Trong các ứng dụng nhạy cảm của học máy, như chính sách dự đoán, sàng lọc tiếp tục và các mô hình rủi ro được sử dụng để cho vay, chúng ta phải đặc biệt cảnh giác với hậu quả của dữ liệu rác. Một chế độ lỗi phổ biến xảy ra trong các bộ dữ liệu nơi một số nhóm người không được đại diện trong dữ liệu đào tạo. Hãy tưởng tượng áp dụng một hệ thống nhận dạng ung thư da trong tự nhiên chưa từng thấy da đen trước đây. Thất bại cũng có thể xảy ra khi dữ liệu không chỉ đơn thuần là đại diện cho một số nhóm mà phản ánh định kiến xã hội. Ví dụ: nếu các quyết định tuyển dụng trong quá khứ được sử dụng để đào tạo một mô hình dự đoán sẽ được sử dụng để sàng lọc sơ yếu lý lịch, thì các mô hình học máy có thể vô tình nắm bắt và tự động hóa các bất công lịch sử. Lưu ý rằng tất cả điều này có thể xảy ra mà không có nhà khoa học dữ liệu tích cực âm mưu, hoặc thậm chí nhận thức được. 

### Mô hình

Hầu hết các máy học liên quan đến việc chuyển đổi dữ liệu theo một số nghĩa. Chúng ta có thể muốn xây dựng một hệ thống chụp ảnh và dự đoán nụ cười. Ngoài ra, chúng ta có thể muốn ăn một tập hợp các bài đọc cảm biến và dự đoán mức độ bình thường so với các bài đọc dị thường như thế nào. Theo mô hình *, chúng tôi biểu thị máy tính toán để nhập dữ liệu của một loại và phun ra các dự đoán của một loại có thể khác. Đặc biệt, chúng tôi quan tâm đến các mô hình thống kê có thể được ước tính từ dữ liệu. Trong khi các mô hình đơn giản hoàn toàn có khả năng giải quyết các vấn đề đơn giản thích hợp, các vấn đề mà chúng tôi tập trung vào trong cuốn sách này kéo dài giới hạn của các phương pháp cổ điển. Học sâu được phân biệt với cách tiếp cận cổ điển chủ yếu bởi tập hợp các mô hình mạnh mẽ mà nó tập trung vào. Các mô hình này bao gồm nhiều biến đổi liên tiếp của dữ liệu được liên kết với nhau từ trên xuống dưới, do đó tên * deep learning*. Trên đường thảo luận về các mô hình sâu sắc, chúng tôi cũng sẽ thảo luận về một số phương pháp truyền thống hơn. 

### Hàm mục tiêu

Trước đó, chúng tôi giới thiệu machine learning là học hỏi từ kinh nghiệm. Bằng cách * learning* ở đây, chúng tôi có nghĩa là cải thiện tại một số nhiệm vụ theo thời gian. Nhưng ai là để nói những gì tạo thành một sự cải thiện? Bạn có thể tưởng tượng rằng chúng tôi có thể đề xuất cập nhật mô hình của chúng tôi và một số người có thể không đồng ý về việc bản cập nhật được đề xuất cấu thành một cải tiến hay suy giảm. 

Để phát triển một hệ thống toán học chính thức của máy học, chúng ta cần phải có các biện pháp chính thức về mức độ tốt (hoặc xấu) mô hình của chúng ta. Trong học máy và tối ưu hóa nói chung hơn, chúng tôi gọi các chức năng * mục tiêu này*. Theo quy ước, chúng ta thường xác định các hàm khách quan để thấp hơn là tốt hơn. Đây chỉ đơn thuần là một quy ước. Bạn có thể thực hiện bất kỳ chức năng nào cao hơn tốt hơn và biến nó thành một chức năng mới giống hệt nhau về chất lượng nhưng thấp hơn là tốt hơn bằng cách lật dấu hiệu. Bởi vì thấp hơn là tốt hơn, các chức năng này đôi khi được gọi
*chức năng mất*.

Khi cố gắng dự đoán các giá trị số, hàm mất mát phổ biến nhất là *squared error*, tức là bình phương của sự khác biệt giữa dự đoán và sự thật mặt đất. Để phân loại, mục tiêu phổ biến nhất là giảm thiểu tỷ lệ lỗi, tức là phần nhỏ các ví dụ mà các dự đoán của chúng ta không đồng ý với sự thật nền tảng. Một số mục tiêu (ví dụ, lỗi bình phương) rất dễ tối ưu hóa. Những người khác (ví dụ, tỷ lệ lỗi) rất khó để tối ưu hóa trực tiếp, do không phân biệt hoặc các biến chứng khác. Trong những trường hợp này, thông thường tối ưu hóa một * thay thế khách thể*. 

Thông thường, hàm mất được xác định đối với các tham số của mô hình và phụ thuộc vào tập dữ liệu. Chúng tôi tìm hiểu các giá trị tốt nhất của các thông số mô hình của chúng tôi bằng cách giảm thiểu tổn thất phát sinh trên một bộ bao gồm một số ví dụ được thu thập để đào tạo. Tuy nhiên, làm tốt trên dữ liệu đào tạo không đảm bảo rằng chúng tôi sẽ làm tốt trên dữ liệu không nhìn thấy. Vì vậy, chúng tôi thường sẽ muốn chia dữ liệu có sẵn thành hai phân vùng: *training dataset* (hoặc *training set*, cho các tham số mô hình phù hợp) và * test dataset* (hoặc * test set*, được tổ chức ra để đánh giá), báo cáo cách mô hình thực hiện trên cả hai chúng. Bạn có thể nghĩ về hiệu suất đào tạo như là giống như một sinh viên 's điểm số trên các kỳ thi thực hành được sử dụng để chuẩn bị cho một số kỳ thi cuối cùng thực tế. Ngay cả khi kết quả là đáng khích lệ, điều đó không đảm bảo thành công trong kỳ thi cuối cùng. Nói cách khác, hiệu suất thử nghiệm có thể đi chệch đáng kể so với hiệu suất đào tạo. Khi một mô hình hoạt động tốt trên bộ đào tạo nhưng không khái quát hóa với dữ liệu không nhìn thấy, chúng tôi nói rằng đó là * overfitting*. Trong điều kiện thực tế, điều này giống như flunking kỳ thi thực sự mặc dù làm tốt trên các kỳ thi thực hành. 

### Thuật toán tối ưu hóa

Khi chúng ta đã có một số nguồn dữ liệu và đại diện, một mô hình và một hàm mục tiêu được xác định rõ, chúng ta cần một thuật toán có khả năng tìm kiếm các tham số tốt nhất có thể để giảm thiểu hàm mất mát. Các thuật toán tối ưu hóa phổ biến cho học sâu dựa trên cách tiếp cận gọi là * gradient descent*. Nói tóm lại, ở mỗi bước, phương pháp này kiểm tra để xem, cho mỗi tham số, theo cách mà bộ đào tạo mất sẽ di chuyển nếu bạn làm phiền tham số đó chỉ là một lượng nhỏ. Sau đó, nó cập nhật tham số theo hướng có thể làm giảm tổn thất. 

## Các loại vấn đề về máy học

Vấn đề từ thức trong ví dụ thúc đẩy của chúng tôi chỉ là một trong số nhiều vấn đề mà machine learning có thể giải quyết. Để thúc đẩy người đọc hơn nữa và cung cấp cho chúng tôi một số ngôn ngữ chung khi chúng tôi nói về nhiều vấn đề hơn trong suốt cuốn sách, trong phần sau, chúng tôi liệt kê một mẫu các vấn đề về máy học. Chúng tôi sẽ liên tục tham khảo các khái niệm đã nói ở trên của chúng tôi như dữ liệu, mô hình và kỹ thuật đào tạo. 

### Học được giám sát

Việc học được giám sát giải quyết nhiệm vụ dự đoán các nhãn được đưa ra các tính năng đầu vào. Mỗi tính năng—cặp nhãn được gọi là một ví dụ. Đôi khi, khi ngữ cảnh rõ ràng, chúng ta có thể sử dụng thuật ngữ *examples* để chỉ một tập hợp các đầu vào, ngay cả khi các nhãn tương ứng không được biết. Mục tiêu của chúng tôi là tạo ra một mô hình ánh xạ bất kỳ đầu vào nào đến dự đoán nhãn. 

Để đưa ra mô tả này trong một ví dụ cụ thể, nếu chúng ta đang làm việc trong chăm sóc sức khỏe, thì chúng ta có thể muốn dự đoán liệu bệnh nhân có bị đau tim hay không. Quan sát này, “đau tim” hoặc “không đau tim”, sẽ là nhãn hiệu của chúng tôi. Các tính năng đầu vào có thể là dấu hiệu quan trọng như nhịp tim, huyết áp tâm trương và huyết áp tâm thu. 

Việc giám sát phát huy tác dụng vì để chọn các tham số, chúng tôi (các giám sát viên) cung cấp cho mô hình một tập dữ liệu bao gồm các ví dụ được dán nhãn, trong đó mỗi ví dụ được khớp với nhãn chân lý mặt đất. Trong thuật ngữ xác suất, chúng tôi thường quan tâm đến việc ước tính xác suất có điều kiện của một nhãn cho các tính năng đầu vào. Mặc dù nó chỉ là một trong số một số mô hình trong machine learning, nhưng việc học được giám sát chiếm phần lớn các ứng dụng thành công của machine learning trong ngành. Một phần, đó là do nhiều nhiệm vụ quan trọng có thể được mô tả rõ ràng là ước tính xác suất của một cái gì đó chưa biết được đưa ra một tập hợp dữ liệu có sẵn cụ thể: 

* Dự đoán ung thư so với ung thư, được đưa ra một hình ảnh chụp cắt lớp vi tính.
* Dự đoán bản dịch chính xác bằng tiếng Pháp, được đưa ra một câu bằng tiếng Anh.
* Dự đoán giá cổ phiếu vào tháng tới dựa trên dữ liệu báo cáo tài chính của tháng này.

Ngay cả với mô tả đơn giản “dự đoán nhãn cho các tính năng đầu vào” học tập giám sát có thể có rất nhiều hình thức và đòi hỏi rất nhiều quyết định mô hình hóa, tùy thuộc vào (trong số các cân nhắc khác) loại, kích thước và số lượng đầu vào và đầu ra. Ví dụ, chúng ta sử dụng các mô hình khác nhau để xử lý các chuỗi có độ dài tùy ý và để xử lý các biểu diễn vectơ có độ dài cố định. Chúng tôi sẽ đến thăm nhiều vấn đề này một cách sâu sắc trong suốt cuốn sách này. 

Không chính thức, quá trình học tập trông giống như sau. Đầu tiên, lấy một bộ sưu tập lớn các ví dụ mà các tính năng được biết đến và chọn từ chúng một tập hợp con ngẫu nhiên, có được các nhãn chân lý nền cho mỗi. Đôi khi các nhãn này có thể là dữ liệu có sẵn đã được thu thập (ví dụ: bệnh nhân có chết trong năm tiếp theo không?) và những lần khác, chúng ta có thể cần sử dụng các chú thích của con người để gắn nhãn dữ liệu, (ví dụ: gán hình ảnh cho các danh mục). Cùng nhau, các đầu vào và nhãn tương ứng này bao gồm bộ đào tạo. Chúng tôi cung cấp tập dữ liệu đào tạo vào một thuật toán học tập được giám sát, một chức năng lấy làm đầu vào một tập dữ liệu và xuất ra một chức năng khác: mô hình đã học. Cuối cùng, chúng ta có thể cung cấp các đầu vào chưa nhìn thấy trước đây cho mô hình đã học, sử dụng đầu ra của nó làm dự đoán của nhãn tương ứng. Quá trình đầy đủ được rút ra trong :numref:`fig_supervised_learning`. 

![Supervised learning.](../img/supervised-learning.svg)
:label:`fig_supervised_learning`

#### Hồi quy

Có lẽ nhiệm vụ học tập được giám sát đơn giản nhất để quấn đầu của bạn là * hồi quy*. Hãy xem xét, ví dụ, một tập hợp dữ liệu thu hoạch từ cơ sở dữ liệu về doanh số bán nhà. Chúng ta có thể xây dựng một bảng, trong đó mỗi hàng tương ứng với một ngôi nhà khác nhau và mỗi cột tương ứng với một số thuộc tính có liên quan, chẳng hạn như cảnh vuông của một ngôi nhà, số phòng ngủ, số lượng phòng tắm, và số phút (đi bộ) đến trung tâm thị trấn. Trong tập dữ liệu này, mỗi ví dụ sẽ là một ngôi nhà cụ thể và vector tính năng tương ứng sẽ là một hàng trong bảng. Nếu bạn sống ở New York hoặc San Francisco, và bạn không phải là Giám đốc điều hành của Amazon, Google, Microsoft hoặc Facebook, (cảnh quay vuông, số phòng ngủ, số phòng tắm, khoảng cách đi bộ) có vector cho ngôi nhà của bạn có thể trông giống như: $[600, 1, 1, 60]$. Tuy nhiên, nếu bạn sống ở Pittsburgh, nó có thể trông giống như $[3000, 4, 3, 10]$ hơn. Các vectơ tính năng như thế này rất cần thiết cho hầu hết các thuật toán machine learning cổ điển. 

Điều gì làm cho một vấn đề trở thành hồi quy thực sự là đầu ra. Nói rằng bạn đang ở trong thị trường cho một ngôi nhà mới. Bạn có thể muốn ước tính giá trị thị trường hợp lý của một ngôi nhà, với một số tính năng như trên. Nhãn, giá bán, là một giá trị số. Khi nhãn lấy các giá trị số tùy ý, chúng tôi gọi đây là vấn đề * regression*. Mục tiêu của chúng tôi là tạo ra một mô hình có dự đoán gần đúng các giá trị nhãn thực tế. 

Rất nhiều vấn đề thực tế là các vấn đề hồi quy được mô tả tốt. Dự đoán đánh giá mà người dùng sẽ gán cho một bộ phim có thể được coi là một vấn đề hồi quy và nếu bạn thiết kế một thuật toán tuyệt vời để hoàn thành kỳ công này trong 2009, bạn có thể đã giành được [1-million-dollar Netflix prize](https://en.wikipedia.org/wiki/Netflix_Prize). Dự đoán thời gian lưu trú cho bệnh nhân trong bệnh viện cũng là một vấn đề hồi quy. Một nguyên tắc tốt của ngón tay cái là bất kỳ * bao nhiêu? * hoặc *bao nhiêu? * vấn đề nên đề nghị hồi quy, chẳng hạn như: 

* Phẫu thuật này sẽ mất bao nhiêu giờ?
* Thị trấn này sẽ có bao nhiêu lượng mưa trong sáu giờ tới?

Ngay cả khi bạn chưa bao giờ làm việc với machine learning trước đây, bạn có thể đã làm việc thông qua một vấn đề hồi quy một cách không chính thức. Hãy tưởng tượng, ví dụ, rằng bạn đã sửa chữa cống rãnh của bạn và nhà thầu của bạn đã dành 3 giờ để loại bỏ gunk khỏi đường ống nước thải của bạn. Sau đó, ông ta gửi cho bạn một hóa đơn 350 đô la. Bây giờ hãy tưởng tượng rằng bạn của bạn đã thuê cùng một nhà thầu trong 2 giờ và anh ta đã nhận được một hóa đơn 250 đô la. Nếu ai đó hỏi bạn mong đợi bao nhiêu vào hóa đơn loại bỏ gunk-removal sắp tới của họ, bạn có thể đưa ra một số giả định hợp lý, chẳng hạn như nhiều giờ làm việc chi phí nhiều đô la hơn. Bạn cũng có thể giả định rằng có một số phí cơ bản và nhà thầu sau đó tính phí mỗi giờ. Nếu những giả định này đúng, sau đó đưa ra hai ví dụ dữ liệu này, bạn đã có thể xác định cấu trúc giá của nhà thầu: 100 đô la mỗi giờ cộng với 50 đô la để xuất hiện tại nhà bạn. Nếu bạn làm theo nhiều điều đó thì bạn đã hiểu ý tưởng cấp cao đằng sau hồi quy tuyến tính. 

Trong trường hợp này, chúng tôi có thể sản xuất các thông số phù hợp chính xác với giá của nhà thầu. Đôi khi điều này là không thể, ví dụ, nếu một số phương sai nợ một vài yếu tố bên cạnh hai tính năng của bạn. Trong những trường hợp này, chúng ta sẽ cố gắng tìm hiểu các mô hình giảm thiểu khoảng cách giữa các dự đoán của chúng tôi và các giá trị quan sát được. Trong hầu hết các chương của chúng tôi, chúng tôi sẽ tập trung vào việc giảm thiểu hàm mất lỗi bình phương. Như chúng ta sẽ thấy sau, sự mất mát này tương ứng với giả định rằng dữ liệu của chúng tôi đã bị hỏng bởi tiếng ồn Gaussian. 

#### Phân loại

Trong khi các mô hình hồi quy là tuyệt vời để giải quyết * bao nhiêu? * câu hỏi, rất nhiều vấn đề không uốn cong thoải mái với mẫu này. Ví dụ: một ngân hàng muốn thêm quét séc vào ứng dụng di động của mình. Điều này sẽ liên quan đến khách hàng chụp ảnh séc bằng máy ảnh của điện thoại thông minh của họ và ứng dụng sẽ cần phải có thể tự động hiểu văn bản nhìn thấy trong hình ảnh. Cụ thể, nó cũng sẽ cần phải hiểu văn bản viết tay để trở nên mạnh mẽ hơn, chẳng hạn như ánh xạ một ký tự viết tay với một trong những ký tự đã biết. Đây là loại * cái nào? * vấn đề được gọi là *phân loại *. Nó được xử lý bằng một tập hợp các thuật toán khác với các thuật toán được sử dụng để hồi quy mặc dù nhiều kỹ thuật sẽ tiếp tục. 

Trong *classification*, chúng tôi muốn mô hình của chúng tôi xem xét các tính năng, ví dụ, các giá trị pixel trong một hình ảnh, và sau đó dự đoán loại ** (chính thức gọi là *class*), trong số một tập hợp các tùy chọn rời rạc, một ví dụ thuộc về. Đối với chữ số viết tay, chúng ta có thể có mười lớp, tương ứng với các chữ số 0 đến 9. Hình thức phân loại đơn giản nhất là khi chỉ có hai lớp, một vấn đề mà chúng ta gọi là *phân loại nhị phân *. Ví dụ, tập dữ liệu của chúng ta có thể bao gồm hình ảnh của động vật và nhãn của chúng ta có thể là các lớp $\mathrm{\{cat, dog\}}$. Trong khi trong hồi quy, chúng tôi tìm kiếm một regressor để xuất ra một giá trị số, trong phân loại, chúng tôi tìm kiếm một phân loại, có đầu ra là gán lớp dự đoán. 

Vì những lý do mà chúng ta sẽ tham gia khi cuốn sách trở nên kỹ thuật hơn, có thể khó tối ưu hóa một mô hình chỉ có thể xuất ra một nhiệm vụ phân loại cứng, ví dụ: “mèo” hoặc “chó”. Trong những trường hợp này, thay vào đó, việc thể hiện mô hình của chúng ta bằng ngôn ngữ xác suất thường dễ dàng hơn nhiều. Cho các tính năng của một ví dụ, mô hình của chúng tôi gán một xác suất cho mỗi lớp có thể. Quay trở lại ví dụ phân loại động vật của chúng tôi trong đó các lớp là $\mathrm{\{cat, dog\}}$, một phân loại có thể thấy một hình ảnh và xuất ra xác suất rằng hình ảnh là một con mèo là 0.9. Chúng ta có thể giải thích con số này bằng cách nói rằng phân loại là 90\% chắc chắn rằng hình ảnh mô tả một con mèo. Độ lớn của xác suất cho lớp dự đoán truyền tải một khái niệm về sự không chắc chắn. Đó không phải là khái niệm duy nhất về sự không chắc chắn và chúng ta sẽ thảo luận về những người khác trong các chương nâng cao hơn. 

Khi chúng ta có nhiều hơn hai lớp có thể, chúng ta gọi vấn đề * phân loại đa lượng*. Các ví dụ phổ biến bao gồm nhận dạng ký tự viết tay $\mathrm{\{0, 1, 2, ... 9, a, b, c, ...\}}$. Trong khi chúng ta tấn công các bài toán hồi quy bằng cách cố gắng giảm thiểu hàm mất lỗi bình phương, hàm tổn thất phổ biến cho các bài toán phân loại được gọi là *cross-entropy*, tên của nó có thể được demystified thông qua một giới thiệu về lý thuyết thông tin trong các chương tiếp theo. 

Lưu ý rằng lớp có khả năng nhất không nhất thiết phải là lớp mà bạn sẽ sử dụng cho quyết định của mình. Giả sử rằng bạn tìm thấy một loại nấm đẹp ở sân sau của bạn như thể hiện trong :numref:`fig_death_cap`. 

![Death cap---do not eat!](../img/death-cap.jpg)
:width:`200px`
:label:`fig_death_cap`

Bây giờ, giả sử rằng bạn đã xây dựng một nhà phân loại và đào tạo nó để dự đoán liệu một loại nấm có độc dựa trên một bức ảnh hay không. Giả sử đầu ra phân loại phát hiện chất độc của chúng tôi rằng xác suất :numref:`fig_death_cap` chứa nắp tử vong là 0,2. Nói cách khác, phân loại là 80\% chắc chắn rằng nấm của chúng tôi không phải là một cái chết nắp. Tuy nhiên, bạn sẽ phải là một kẻ ngốc để ăn nó. Đó là bởi vì lợi ích nhất định của một bữa tối ngon không đáng để có nguy cơ tử vong từ nó 20\%. Nói cách khác, ảnh hưởng của rủi ro không chắc chắn lớn hơn lợi ích cho đến nay. Do đó, chúng ta cần tính toán rủi ro dự kiến mà chúng ta phải chịu như chức năng mất mát, tức là, chúng ta cần nhân xác suất của kết quả với lợi ích (hoặc tác hại) liên quan đến nó. Trong trường hợp này, sự mất mát phát sinh do ăn nấm có thể là $0.2 \times \infty + 0.8 \times 0 = \infty$, trong khi mất việc loại bỏ nó là $0.2 \times 0 + 0.8 \times 1 = 0.8$. Sự thận trọng của chúng tôi đã được chứng minh: như bất kỳ nhà nghiên cứu nấm nào cũng cho chúng tôi biết, nấm vào năm :numref:`fig_death_cap` thực sự là một cái mũ chết. 

Phân loại có thể trở nên phức tạp hơn nhiều so với chỉ phân loại nhị phân, đa phân loại hoặc thậm chí nhiều nhãn. Ví dụ, có một số biến thể của phân loại để giải quyết các hệ thống phân cấp. Hệ thống phân cấp giả định rằng có tồn tại một số mối quan hệ giữa nhiều lớp. Vì vậy, không phải tất cả các lỗi đều bằng nhau - nếu chúng ta phải sai, chúng ta muốn phân loại sai thành một lớp liên quan hơn là đến một lớp xa. Thông thường, điều này được gọi là * phân loại phân học*. Một ví dụ ban đầu là do [Linnaeus](https://en.wikipedia.org/wiki/Carl_Linnaeus), người đã tổ chức các loài động vật theo hệ thống phân cấp. 

Trong trường hợp phân loại động vật, có thể không quá tệ khi nhầm lẫn một con chó xù (giống chó) cho schnauzer (một giống chó khác), nhưng mô hình của chúng tôi sẽ phải trả một hình phạt lớn nếu nó nhầm lẫn một con chó xù cho một con khủng long. Hệ thống phân cấp nào có liên quan có thể phụ thuộc vào cách bạn định sử dụng mô hình. Ví dụ, rắn lục lạc và rắn garter có thể gần trên cây phát sinh loài, nhưng nhầm một con rattler cho một garter có thể gây chết người. 

#### Gắn thẻ

Một số bài toán phân loại phù hợp gọn gàng vào các thiết lập phân loại nhị phân hoặc đa lớp. Ví dụ, chúng ta có thể huấn luyện một phân loại nhị phân bình thường để phân biệt mèo với chó. Với trạng thái hiện tại của tầm nhìn máy tính, chúng ta có thể làm điều này một cách dễ dàng, với các công cụ off-the-shelf. Tuy nhiên, cho dù mô hình của chúng tôi chính xác đến mức nào, chúng ta có thể thấy mình gặp rắc rối khi nhà phân loại gặp phải hình ảnh của * Town Musicians of Bremen*, một câu chuyện cổ tích nổi tiếng của Đức có bốn con vật trong :numref:`fig_stackedanimals`. 

![A donkey, a dog, a cat, and a rooster.](../img/stackedanimals.png)
:width:`300px`
:label:`fig_stackedanimals`

Như bạn có thể thấy, có một con mèo trong :numref:`fig_stackedanimals`, và một con gà trống, chó và một con lừa, với một số cây trong nền. Tùy thuộc vào những gì chúng ta muốn làm với mô hình của chúng ta cuối cùng, coi đây là một vấn đề phân loại nhị phân có thể không có nhiều ý nghĩa. Thay vào đó, chúng ta có thể muốn cung cấp cho người mẫu tùy chọn nói rằng hình ảnh mô tả một con mèo, chó, một con lừa,
*và* một con gà trống.

Vấn đề học cách dự đoán các lớp không loại trừ lẫn nhau được gọi là *phân loại nhiều nhãn mác*. Các vấn đề tự động gắn thẻ thường được mô tả tốt nhất là các vấn đề phân loại đa nhãn. Hãy suy nghĩ về các thẻ mà mọi người có thể áp dụng cho các bài đăng trên blog kỹ thuật, ví dụ: “machine learning”, “technology”, “tiện ích”, “ngôn ngữ lập trình”, “Linux”, “điện toán đám mây”, “AWS”. Một bài viết điển hình có thể có 5—10 thẻ được áp dụng bởi vì các khái niệm này có tương quan. Bài viết về “điện toán đám mây” có khả năng đề cập đến “AWS” và các bài đăng về “machine learning” cũng có thể đối phó với “ngôn ngữ lập trình”. 

Chúng ta cũng phải đối phó với loại vấn đề này khi đối phó với các tài liệu y sinh, nơi gắn thẻ chính xác các bài báo là quan trọng vì nó cho phép các nhà nghiên cứu thực hiện các đánh giá đầy đủ về tài liệu. Tại Thư viện Y học Quốc gia, một số chú thích chuyên nghiệp đi qua mỗi bài viết được lập chỉ mục trong PubMed để liên kết nó với các điều khoản có liên quan từ mesh, một bộ sưu tập khoảng 28000 thẻ. Đây là một quá trình tốn thời gian và các chú thích thường có độ trễ một năm giữa lưu trữ và gắn thẻ. Học máy có thể được sử dụng ở đây để cung cấp các thẻ tạm thời cho đến khi mỗi bài viết có thể có một đánh giá thủ công thích hợp. Thật vậy, trong vài năm, tổ chức BioASQ có [hosted competitions](http://bioasq.org/) để làm chính xác điều này. 

#### Tìm 

Đôi khi chúng ta không chỉ muốn gán từng ví dụ cho một xô hoặc cho một giá trị thực. Trong lĩnh vực truy xuất thông tin, chúng tôi muốn áp đặt một bảng xếp hạng trên một tập hợp các mục. Lấy tìm kiếm trên web cho một ví dụ. Mục tiêu ít hơn để xác định xem một trang cụ thể có liên quan đến truy vấn hay không, mà là một trong rất nhiều kết quả tìm kiếm có liên quan nhất cho một người dùng cụ thể. Chúng tôi thực sự quan tâm đến việc sắp xếp các kết quả tìm kiếm có liên quan và thuật toán học tập của chúng tôi cần tạo ra các tập con có thứ tự của các yếu tố từ một tập hợp lớn hơn. Nói cách khác, nếu chúng ta được yêu cầu tạo ra 5 chữ cái đầu tiên từ bảng chữ cái, có sự khác biệt giữa việc trả về “A B C D E” và “C A B E D”. Ngay cả khi tập kết quả là như nhau, thứ tự trong bộ vấn đề. 

Một giải pháp khả thi cho vấn đề này là lần đầu tiên gán cho mọi yếu tố trong tập hợp một điểm liên quan tương ứng và sau đó lấy các yếu tố được xếp hạng hàng đầu. [PageRank](https://en.wikipedia.org/wiki/PageRank), nước sốt bí mật ban đầu đằng sau công cụ tìm kiếm của Google là một ví dụ ban đầu của một hệ thống tính điểm như vậy nhưng nó đặc biệt ở chỗ nó đã làm không phụ thuộc vào truy vấn thực tế. Ở đây, họ dựa vào một bộ lọc liên quan đơn giản để xác định tập hợp các mục có liên quan và sau đó vào PageRank để đặt hàng những kết quả có chứa thuật ngữ truy vấn. Ngày nay, các công cụ tìm kiếm sử dụng máy học và mô hình hành vi để có được điểm số liên quan phụ thuộc vào truy vấn. Có toàn bộ hội nghị học thuật dành cho môn học này. 

#### Hệ thống Recommender
:label:`subsec_recommender_systems`

Hệ thống giới thiệu là một cài đặt vấn đề khác có liên quan đến tìm kiếm và xếp hạng. Các vấn đề tương tự như mục tiêu là hiển thị một tập hợp các mục có liên quan cho người dùng. Sự khác biệt chính là sự nhấn mạnh vào
*cá nhân hóa*
cho người dùng cụ thể trong bối cảnh của các hệ thống giới thiệu. Ví dụ: đối với các đề xuất phim, trang kết quả cho một người hâm mộ khoa học viễn tưởng và trang kết quả cho một người sành phim hài Peter Sellers có thể khác nhau đáng kể. Các vấn đề tương tự bật lên trong các cài đặt đề xuất khác, ví dụ, cho các sản phẩm bán lẻ, âm nhạc và khuyến nghị tin tức. 

Trong một số trường hợp, khách hàng cung cấp phản hồi rõ ràng cho biết họ thích một sản phẩm cụ thể như thế nào (ví dụ: xếp hạng và đánh giá sản phẩm trên Amazon, IMDb và Goodreads). Trong một số trường hợp khác, họ cung cấp phản hồi ngầm, ví dụ: bằng cách bỏ qua tiêu đề trên danh sách phát, điều này có thể cho thấy sự không hài lòng nhưng có thể chỉ ra rằng bài hát không phù hợp trong ngữ cảnh. Trong các công thức đơn giản nhất, các hệ thống này được đào tạo để ước tính một số điểm, chẳng hạn như đánh giá ước tính hoặc xác suất mua hàng, cho người dùng và một mặt hàng. 

Với một mô hình như vậy, đối với bất kỳ người dùng nhất định nào, chúng ta có thể lấy tập hợp các đối tượng với điểm số lớn nhất, sau đó có thể được đề xuất cho người dùng. Hệ thống sản xuất tiên tiến hơn đáng kể và tính đến hoạt động chi tiết của người dùng và đặc điểm mục khi tính toán điểm số đó. :numref:`fig_deeplearning_amazon` là một ví dụ về sách học sâu được Amazon đề xuất dựa trên các thuật toán cá nhân hóa được điều chỉnh để nắm bắt sở thích của một người. 

![Deep learning books recommended by Amazon.](../img/deeplearning-amazon.jpg)
:label:`fig_deeplearning_amazon`

Mặc dù giá trị kinh tế to lớn của họ, các hệ thống khuyến nghị ngây thơ được xây dựng trên đầu trang của các mô hình dự đoán phải chịu một số sai sót quan niệm nghiêm trọng. Để bắt đầu, chúng tôi chỉ quan sát *phản hồi kiểm duyệt *: người dùng ưu tiên đánh giá những bộ phim mà họ cảm thấy mạnh mẽ. Ví dụ: trên thang điểm năm điểm, bạn có thể nhận thấy rằng các mặt hàng nhận được nhiều xếp hạng năm và một sao nhưng có rất ít xếp hạng ba sao dễ thấy. Hơn nữa, thói quen mua hàng hiện tại thường là kết quả của thuật toán đề xuất hiện tại, nhưng các thuật toán học tập không phải lúc nào cũng tính đến chi tiết này. Do đó, các vòng phản hồi có thể hình thành nơi một hệ thống giới thiệu ưu tiên đẩy một mục sau đó được thực hiện tốt hơn (do mua hàng lớn hơn) và lần lượt được khuyến nghị thường xuyên hơn. Nhiều vấn đề trong số này về cách đối phó với kiểm duyệt, ưu đãi và vòng lặp phản hồi, là những câu hỏi nghiên cứu mở quan trọng. 

#### Học trình tự

Cho đến nay, chúng tôi đã xem xét các vấn đề mà chúng tôi có một số cố định số đầu vào và tạo ra một số cố định của đầu ra. Ví dụ, chúng tôi đã xem xét dự đoán giá nhà từ một tập hợp các tính năng cố định: cảnh vuông, số phòng ngủ, số phòng tắm, thời gian đi bộ đến trung tâm thành phố. Chúng tôi cũng thảo luận về ánh xạ từ một hình ảnh (có kích thước cố định) đến xác suất dự đoán rằng nó thuộc về mỗi lớp cố định hoặc lấy ID người dùng và ID sản phẩm và dự đoán xếp hạng sao. Trong những trường hợp này, một khi chúng tôi cung cấp đầu vào chiều dài cố định của mình vào mô hình để tạo ra một đầu ra, mô hình ngay lập tức quên những gì nó vừa thấy. 

Điều này có thể ổn nếu đầu vào của chúng ta thực sự tất cả đều có cùng kích thước và nếu các đầu vào liên tiếp thực sự không liên quan gì đến nhau. Nhưng làm thế nào chúng ta sẽ đối phó với đoạn video? Trong trường hợp này, mỗi đoạn mã có thể bao gồm một số khung khác nhau. Và đoán của chúng tôi về những gì đang xảy ra trong mỗi khung hình có thể mạnh hơn nhiều nếu chúng ta tính đến các khung trước đó hoặc thành công. Cũng vậy với ngôn ngữ. Một vấn đề học sâu phổ biến là dịch máy: nhiệm vụ nhập câu bằng một số ngôn ngữ nguồn và dự đoán bản dịch của chúng bằng một ngôn ngữ khác. 

Những vấn đề này cũng xảy ra trong y học. Chúng tôi có thể muốn một mô hình để theo dõi bệnh nhân trong phòng chăm sóc đặc biệt và bắn ra cảnh báo nếu nguy cơ tử vong của họ trong 24 giờ tới vượt quá ngưỡng. Chúng tôi chắc chắn sẽ không muốn mô hình này vứt bỏ mọi thứ mà nó biết về lịch sử bệnh nhân mỗi giờ và chỉ đưa ra dự đoán của nó dựa trên các phép đo gần đây nhất. 

Những vấn đề này là một trong những ứng dụng thú vị nhất của machine learning và chúng là các trường hợp của học trình tự *. Chúng yêu cầu một mô hình để ăn các chuỗi đầu vào hoặc phát ra chuỗi đầu ra (hoặc cả hai). Cụ thể,
*trình tự để học trình tự* xem xét các vấn đề
trong đó đầu vào và đầu ra đều là chuỗi có độ dài thay đổi, chẳng hạn như dịch máy và phiên âm văn bản từ bài phát biểu nói. Mặc dù không thể xem xét tất cả các loại biến đổi trình tự, nhưng các trường hợp đặc biệt sau đây là đáng nói đến. 

** Gắn thẻ và phân tích cú pháp**. Điều này liên quan đến việc chú thích một chuỗi văn bản với các thuộc tính.
Nói cách khác, số lượng đầu vào và đầu ra về cơ bản là giống nhau. Ví dụ, chúng ta có thể muốn biết các động từ và chủ thể ở đâu. Ngoài ra, chúng ta có thể muốn biết từ nào là các thực thể được đặt tên. Nói chung, mục tiêu là phân hủy và chú thích văn bản dựa trên các giả định cấu trúc và ngữ pháp để có được một số chú thích. Điều này nghe có vẻ phức tạp hơn so với thực tế. Dưới đây là một ví dụ rất đơn giản về chú thích một câu với các thẻ cho biết từ nào đề cập đến các thực thể được đặt tên (được gắn thẻ là “Ent”).

```text
Tom has dinner in Washington with Sally
Ent  -    -    -     Ent      -    Ent
```

** Nhận dạng giọng nói tự động**. Với nhận dạng giọng nói, trình tự đầu vào
là một bản ghi âm của một loa (hiển thị trong :numref:`fig_speech`), và đầu ra là bảng điểm văn bản của những gì người nói. Thách thức là có nhiều khung âm thanh hơn (âm thanh thường được lấy mẫu ở tốc độ 8kHz hoặc 16kHz) so với văn bản, tức là, không có sự tương ứng 1:1 giữa âm thanh và văn bản, vì hàng ngàn mẫu có thể tương ứng với một từ nói duy nhất. Đây là những chuỗi để sắp xếp các vấn đề học tập trong đó đầu ra ngắn hơn nhiều so với đầu vào. 

![`-D-e-e-p- L-ea-r-ni-ng-` in an audio recording.](../img/speech.png)
:width:`700px`
:label:`fig_speech`

**Text to Speech**. Đây là nghịch đảo của nhận dạng giọng nói tự động.
Nói cách khác, đầu vào là văn bản và đầu ra là một tệp âm thanh. Trong trường hợp này, đầu ra dài hơn nhiều so với đầu vào. Mặc dù con người dễ dàng nhận ra một tệp âm thanh xấu, nhưng điều này không hoàn toàn tầm thường đối với máy tính. 

** Dịch máy**. Không giống như trường hợp nhận dạng giọng nói, nơi tương ứng
đầu vào và đầu ra xảy ra theo cùng một thứ tự (sau khi căn chỉnh), trong dịch máy, đảo ngược thứ tự có thể rất quan trọng. Nói cách khác, trong khi chúng ta vẫn đang chuyển đổi một chuỗi thành chuỗi khác, không phải số lượng đầu vào và đầu ra cũng như thứ tự của các ví dụ dữ liệu tương ứng được giả định là giống nhau. Hãy xem xét ví dụ minh họa sau đây về xu hướng đặc biệt của người Đức để đặt các động từ ở cuối câu.

```text
German:           Haben Sie sich schon dieses grossartige Lehrwerk angeschaut?
English:          Did you already check out this excellent tutorial?
Wrong alignment:  Did you yourself already this excellent tutorial looked-at?
```

Nhiều vấn đề liên quan bật lên trong các nhiệm vụ học tập khác. Ví dụ, xác định thứ tự mà người dùng đọc một trang web là một vấn đề phân tích bố cục hai chiều. Các vấn đề đối thoại thể hiện tất cả các loại biến chứng bổ sung, trong đó việc xác định những gì cần nói tiếp theo đòi hỏi phải tính đến kiến thức trong thế giới thực và trạng thái trước đó của cuộc trò chuyện trên khoảng cách thời gian dài. Đây là những lĩnh vực nghiên cứu tích cực. 

### Học tập không được giám sát và tự giám sát

Tất cả các ví dụ cho đến nay đều liên quan đến việc học được giám sát, tức là, các tình huống mà chúng tôi cung cấp cho mô hình một tập dữ liệu khổng lồ chứa cả các tính năng và giá trị nhãn tương ứng. Bạn có thể nghĩ về người học được giám sát là có một công việc cực kỳ chuyên biệt và một ông chủ cực kỳ tầm thường. Ông chủ đứng trên vai của bạn và cho bạn biết chính xác phải làm gì trong mọi tình huống cho đến khi bạn học cách lập bản đồ từ các tình huống đến hành động. Làm việc cho một ông chủ như vậy nghe có vẻ khá khập khiễng. Mặt khác, thật dễ dàng để làm hài lòng ông chủ này. Bạn chỉ cần nhận ra mô hình càng nhanh càng tốt và bắt chước hành động của họ. 

Theo một cách hoàn toàn ngược lại, có thể là bực bội khi làm việc cho một ông chủ không biết họ muốn bạn làm gì. Tuy nhiên, nếu bạn có kế hoạch trở thành một nhà khoa học dữ liệu, bạn nên làm quen với nó. Ông chủ có thể chỉ đưa cho bạn một bãi dữ liệu khổng lồ và nói với bạn để * làm một số khoa học dữ liệu với nó! * Điều này nghe có vẻ mơ hồ vì nó là. Chúng tôi gọi lớp vấn đề này* học không được giám sách*, và loại và số câu hỏi chúng tôi có thể hỏi chỉ bị giới hạn bởi sự sáng tạo của chúng tôi. Chúng tôi sẽ giải quyết các kỹ thuật học tập không được giám sát trong các chương sau. Để kích thích sự thèm ăn của bạn bây giờ, chúng tôi mô tả một vài trong số các câu hỏi sau đây bạn có thể hỏi. 

* Can we find a small nhỏ bé numbercon số of prototypenguyên mẫu
that accurately chính xác summarize tóm tắt the data dữ liệu? Với một bộ ảnh, chúng ta có thể nhóm chúng thành ảnh phong cảnh, hình ảnh của chó, trẻ sơ sinh, mèo và đỉnh núi không? Tương tự như vậy, với một bộ sưu tập các hoạt động duyệt web của người dùng, chúng ta có thể nhóm chúng thành những người dùng có hành vi tương tự không? Vấn đề này thường được gọi là *cùng*.
* Chúng ta có thể tìm thấy một số lượng nhỏ các tham số
mà nắm bắt chính xác các thuộc tính có liên quan của dữ liệu? Quỹ đạo của một quả bóng được mô tả khá tốt bằng vận tốc, đường kính và khối lượng của quả bóng. Thợ may đã phát triển một số lượng nhỏ các thông số mô tả hình dạng cơ thể con người khá chính xác với mục đích phù hợp với quần áo. Những vấn đề này được gọi là *ước tính không gian con*. Nếu sự phụ thuộc là tuyến tính, nó được gọi là * phân tích thành phần chính*.
* Có một đại diện của các đối tượng (cấu trúc tùy ý)
trong không gian Euclide như vậy mà tính chất tượng trưng có thể được phù hợp tốt? Điều này có thể được sử dụng để mô tả các thực thể và quan hệ của họ, chẳng hạn như “Rome” $-$ “Italy” $+$ “Pháp” $=$ “Paris”.
* Có mô tả về nguyên nhân gốc rễ
of much of the data dữ liệu that we observe quan sát? Ví dụ, nếu chúng ta có dữ liệu nhân khẩu học về giá nhà, ô nhiễm, tội phạm, vị trí, giáo dục và tiền lương, chúng ta có thể khám phá ra cách chúng liên quan đơn giản dựa trên dữ liệu thực nghiệm không? Các trường liên quan đến *causality* và *mô hình đồ họa xác suất* giải quyết vấn đề này.
* Một sự phát triển gần đây quan trọng và thú vị khác trong việc học không được giám sát
là sự ra đời của * generative adversarial networks*. Những điều này cho chúng ta một cách thủ tục để tổng hợp dữ liệu, thậm chí dữ liệu có cấu trúc phức tạp như hình ảnh và âm thanh. Các cơ chế thống kê cơ bản là các thử nghiệm để kiểm tra xem dữ liệu thực và giả có giống nhau hay không. 

As a formhình thức of unsupervised không giám sát learning học tập,
*tự giám sát*
tận dụng dữ liệu không được dán nhãn để cung cấp sự giám sát trong đào tạo, chẳng hạn như bằng cách dự đoán một số phần giữ lại của dữ liệu bằng cách sử dụng các phần khác. Đối với văn bản, chúng ta có thể đào tạo các mô hình để “điền vào khoảng trống” bằng cách dự đoán các từ được che giấu ngẫu nhiên bằng cách sử dụng các từ xung quanh (ngữ cảnh) của chúng trong thể lớn mà không cần bất kỳ nỗ lực ghi nhãn :cite:`Devlin.Chang.Lee.ea.2018`! Đối với hình ảnh, chúng tôi có thể đào tạo các mô hình để cho biết vị trí tương đối giữa hai vùng bị cắt của cùng một hình ảnh :cite:`Doersch.Gupta.Efros.2015`. Trong hai ví dụ về việc học tự giám sát này, các mô hình đào tạo để dự đoán các từ có thể và vị trí tương đối đều là nhiệm vụ phân loại (từ việc học được giám sát). 

### Tương tác với môi trường

Cho đến nay, chúng ta chưa thảo luận về dữ liệu thực sự đến từ đâu, hoặc những gì thực sự xảy ra khi một mô hình máy học tạo ra một đầu ra. Đó là bởi vì việc học được giám sát và học tập không được giám sát không giải quyết những vấn đề này một cách rất tinh vi. Trong cả hai trường hợp, chúng tôi lấy trước một đống dữ liệu lớn, sau đó thiết lập các máy nhận dạng mẫu của chúng tôi chuyển động mà không bao giờ tương tác với môi trường một lần nữa. Bởi vì tất cả các quá trình học tập diễn ra sau khi thuật toán bị ngắt kết nối khỏi môi trường, điều này đôi khi được gọi là *học ngoại tuyết*. Đối với việc học được giám sát, quá trình này bằng cách xem xét thu thập dữ liệu từ một môi trường trông giống như :numref:`fig_data_collection`. 

![Collecting data for supervised learning from an environment.](../img/data-collection.svg)
:label:`fig_data_collection`

Sự đơn giản của việc học ngoại tuyến này có sự quyến rũ của nó. Nhược điểm là chúng ta có thể lo lắng về việc nhận dạng mô hình trong sự cô lập, mà không bị phân tâm từ những vấn đề khác. Nhưng nhược điểm là công thức vấn đề khá hạn chế. Nếu bạn tham vọng hơn, hoặc nếu bạn lớn lên đọc loạt Robot của Asimov, thì bạn có thể tưởng tượng các bot thông minh nhân tạo có khả năng không chỉ đưa ra dự đoán mà còn thực hiện các hành động trên thế giới. Chúng tôi muốn suy nghĩ về thông minh* đại lý*, không chỉ là các mô hình dự đoán. Điều này có nghĩa là chúng ta cần suy nghĩ về việc lựa chọn *actions*, không chỉ đưa ra dự đoán. Hơn nữa, không giống như dự đoán, các hành động thực sự ảnh hưởng đến môi trường. Nếu chúng ta muốn đào tạo một đại lý thông minh, chúng ta phải tính đến cách hành động của nó có thể ảnh hưởng đến các quan sát trong tương lai của tác nhân. 

Xem xét sự tương tác với một môi trường mở ra một tập hợp các câu hỏi mô hình hóa mới. Sau đây chỉ là một vài ví dụ. 

* Môi trường có nhớ những gì chúng ta đã làm trước đây không?
* Môi trường có muốn giúp chúng tôi, ví dụ: người dùng đọc văn bản vào bộ nhận dạng giọng nói không?
* Môi trường có muốn đánh bại chúng ta, tức là, một thiết lập đối thủ như lọc thư rác (chống lại kẻ gửi thư rác) hoặc chơi một trò chơi (so với đối thủ)?
* Môi trường không quan tâm?
* Môi trường có động lực dịch chuyển không? Ví dụ: dữ liệu trong tương lai luôn giống với quá khứ hay các mẫu thay đổi theo thời gian, một cách tự nhiên hoặc để đáp ứng với các công cụ tự động của chúng tôi?

Câu hỏi cuối cùng này đặt ra vấn đề của *dịch chuyển phân phố*, khi dữ liệu đào tạo và kiểm tra khác nhau. Đó là một vấn đề mà hầu hết chúng ta đã trải qua khi tham gia các kỳ thi được viết bởi một giảng viên, trong khi bài tập về nhà được sáng tác bởi các trợ lý giảng dạy của ông. Tiếp theo, chúng tôi sẽ mô tả ngắn gọn học tập củng cố, một thiết lập xem xét rõ ràng tương tác với một môi trường. 

### Học tăng cường

Nếu bạn quan tâm đến việc sử dụng máy học để phát triển một tác nhân tương tác với môi trường và thực hiện các hành động, thì có lẽ bạn sẽ tập trung vào * học tăng cường *. Điều này có thể bao gồm các ứng dụng cho robot, hệ thống đối thoại và thậm chí để phát triển trí tuệ nhân tạo (AI) cho trò chơi điện tử.
*Học tăng cường sâu*, áp dụng
học sâu để củng cố các vấn đề học tập, đã tăng lên phổ biến. Đột phá sâu Q-mạng đánh bại con người tại các trò chơi Atari chỉ sử dụng đầu vào trực quan, và chương trình AlphaGo đã truất ngôi nhà vô địch thế giới tại trò chơi board game Go là hai ví dụ nổi bật. 

Học củng cố đưa ra một tuyên bố rất chung chung về một vấn đề, trong đó một tác nhân tương tác với một môi trường trong một loạt các bước thời gian. Tại mỗi bước thời gian, tác nhân nhận được một số *quan sách* từ môi trường và phải chọn một *hành động* sau đó được truyền trở lại môi trường thông qua một số cơ chế (đôi khi được gọi là bộ truyền động). Cuối cùng, đại lý nhận được phần thưởng từ môi trường. Quá trình này được minh họa trong :numref:`fig_rl-environment`. Các tác nhân sau đó nhận được một quan sát tiếp theo, và chọn một hành động tiếp theo, và như vậy. Hành vi của một đại lý học tập củng cố được điều chỉnh bởi một chính sách. Nói tóm lại, một *policy* chỉ là một chức năng ánh xạ từ quan sát môi trường đến hành động. Mục tiêu của việc học củng cố là tạo ra một chính sách tốt. 

![The interaction between reinforcement learning and an environment.](../img/rl-environment.svg)
:label:`fig_rl-environment`

Thật khó để nói quá mức tổng quát của khung học tập củng cố. Ví dụ, chúng ta có thể đưa bất kỳ vấn đề học tập được giám sát nào như một vấn đề học tập củng cố. Giả sử chúng tôi đã có một vấn đề phân loại. Chúng ta có thể tạo ra một đại lý học tập tăng cường với một hành động tương ứng với mỗi lớp. Sau đó chúng ta có thể tạo ra một môi trường cho một phần thưởng chính xác bằng với chức năng mất từ vấn đề học tập được giám sát ban đầu. 

Điều đó đang được nói, việc học củng cố cũng có thể giải quyết nhiều vấn đề mà việc học được giám sát không thể. Ví dụ, trong việc học được giám sát, chúng tôi luôn mong đợi rằng đầu vào đào tạo được liên kết với nhãn chính xác. Nhưng trong học tập củng cố, chúng tôi không cho rằng đối với mỗi quan sát môi trường cho chúng ta biết hành động tối ưu. Nói chung, chúng tôi chỉ nhận được một số phần thưởng. Hơn nữa, môi trường thậm chí có thể không cho chúng ta biết hành động nào dẫn đến phần thưởng. 

Xem xét ví dụ như các trò chơi của cờ vua. Tín hiệu phần thưởng thực sự duy nhất đến ở cuối trò chơi khi chúng ta giành chiến thắng, mà chúng ta có thể gán phần thưởng là 1, hoặc khi chúng ta thua, mà chúng ta có thể gán phần thưởng là -1. Vì vậy, người học củng cố phải giải quyết vấn đề *tín dục*: xác định hành động nào cần ghi nhận hoặc đổ lỗi cho một kết quả. Điều tương tự cũng xảy ra đối với một nhân viên được khuyến mãi vào ngày 11 tháng 10. Chương trình khuyến mãi đó có thể phản ánh một số lượng lớn các hành động được lựa chọn tốt trong năm trước. Nhận được nhiều chương trình khuyến mãi trong tương lai đòi hỏi phải tìm ra những hành động nào trên đường đi dẫn đến chương trình khuyến mãi. 

Người học củng cố cũng có thể phải đối phó với vấn đề quan sát từng phần. Đó là, quan sát hiện tại có thể không cho bạn biết mọi thứ về trạng thái hiện tại của bạn. Nói rằng một robot làm sạch thấy mình bị mắc kẹt trong một trong nhiều tủ quần áo giống hệt nhau trong một ngôi nhà. Suy ra vị trí chính xác (và do đó trạng thái) của robot có thể yêu cầu xem xét các quan sát trước đó của nó trước khi vào tủ quần áo. 

Cuối cùng, tại bất kỳ thời điểm nào, người học củng cố có thể biết về một chính sách tốt, nhưng có thể có nhiều chính sách tốt hơn khác mà đại lý chưa bao giờ thử. Người học củng cố phải liên tục lựa chọn để * khai thác * chiến lược tốt nhất hiện nay được biết đến như một chính sách, hoặc để * khám phát* không gian của các chiến lược, có khả năng từ bỏ một số phần thưởng ngắn hạn để đổi lấy kiến thức. 

Vấn đề học tập củng cố chung là một bối cảnh rất chung chung. Các hành động ảnh hưởng đến các quan sát tiếp theo. Phần thưởng chỉ được quan sát tương ứng với các hành động đã chọn. Môi trường có thể được quan sát đầy đủ hoặc một phần. Kế toán cho tất cả sự phức tạp này cùng một lúc có thể hỏi quá nhiều các nhà nghiên cứu. Hơn nữa, không phải mọi vấn đề thực tế thể hiện tất cả sự phức tạp này. Kết quả là, các nhà nghiên cứu đã nghiên cứu một số trường hợp đặc biệt của các vấn đề học tập củng cố. 

Khi môi trường được quan sát đầy đủ, chúng tôi gọi vấn đề học tập củng cố là quy trình quyết định * Markov*. Khi trạng thái không phụ thuộc vào các hành động trước đó, chúng ta gọi vấn đề là vấn đề tên cướp theo ngữ cảnh *. *. Khi không có trạng thái, chỉ là một tập hợp các hành động có sẵn với phần thưởng ban đầu chưa biết, vấn đề này là vấn đề cướp đa vũ cổ* cổ điển*. 

## Rễ

Chúng tôi vừa xem xét một tập hợp nhỏ các vấn đề mà machine learning có thể giải quyết. Đối với một tập hợp các vấn đề máy học đa dạng, deep learning cung cấp các công cụ mạnh mẽ để giải quyết chúng. Mặc dù nhiều phương pháp học sâu là những phát minh gần đây, ý tưởng cốt lõi của lập trình với dữ liệu và mạng thần kinh (tên của nhiều mô hình học sâu) đã được nghiên cứu trong nhiều thế kỷ. Trên thực tế, con người đã tổ chức mong muốn phân tích dữ liệu và dự đoán kết quả trong tương lai lâu dài và phần lớn khoa học tự nhiên có nguồn gốc từ việc này. Ví dụ, bản phân bố Bernoulli được đặt theo tên [[Jacob Bernoulli (1655—1705)](https://en.wikipedia.org/wiki/Jacob_Bernoulli), và bản phân bố Gaussian được phát hiện bởi [Carl Friedrich Gauss (1777—1855)](https://en.wikipedia.org/wiki/Carl_Friedrich_Gauss). Ông đã phát minh ra, ví dụ, thuật toán ô vuông ít trung bình nhất, mà vẫn được sử dụng ngày nay cho vô số vấn đề từ tính toán bảo hiểm đến chẩn đoán y tế. Những công cụ này đã tạo ra một cách tiếp cận thực nghiệm trong khoa học tự nhiên - ví dụ, định luật Ohm liên quan đến dòng điện và điện áp trong một điện trở được mô tả hoàn hảo bởi một mô hình tuyến tính. 

Ngay cả trong thời trung cổ, các nhà toán học đã có một trực giác quan tâm về ước tính. Ví dụ, cuốn sách hình học của [[Jacob Köbel (1460—1533)](https://www.maa.org/press/periodicals/convergence/mathematical-treasures-jacob-kobels-geometry) minh họa trung bình chiều dài của 16 chân nam trưởng thành để có được chiều dài chân trung bình. 

![Estimating the length of a foot.](../img/koebel.jpg)
:width:`500px`
:label:`fig_koebel`

:numref:`fig_koebel` minh họa cách thức ước tính này hoạt động. 16 người đàn ông trưởng thành được yêu cầu xếp hàng liên tiếp, khi rời khỏi nhà thờ. Chiều dài tổng hợp của chúng sau đó được chia cho 16 để có được ước tính cho những gì bây giờ là 1 foot. “Thuật toán” này sau đó được cải tiến để đối phó với đôi chân sai mạnh—2 người đàn ông có bàn chân ngắn nhất và dài nhất lần lượt được gửi đi, trung bình chỉ hơn phần còn lại. Đây là một trong những ví dụ sớm nhất về ước tính trung bình được cắt tỉa. 

Thống kê thực sự cất cánh với việc thu thập và tính sẵn có của dữ liệu. Một trong những người khổng lồ của nó, [Ronald Fisher (1890—1962)](https://en.wikipedia.org/wiki/Ronald_Fisher), đóng góp đáng kể cho lý thuyết của nó và cũng là các ứng dụng của nó trong di truyền học. Nhiều thuật toán của ông (như phân tích phân biệt đối xử tuyến tính) và công thức (như ma trận thông tin Fisher) vẫn đang được sử dụng thường xuyên ngày nay. Trên thực tế, ngay cả bộ dữ liệu Iris mà Fisher phát hành vào năm 1936 vẫn còn được sử dụng đôi khi để minh họa cho các thuật toán học máy. Ông cũng là một người đề xuất ưu sinh, điều này nên nhắc nhở chúng ta rằng việc sử dụng khoa học dữ liệu đáng ngờ về mặt đạo đức đã lâu dài và lâu dài một lịch sử như việc sử dụng sản xuất của nó trong công nghiệp và khoa học tự nhiên. 

Ảnh hưởng thứ hai đối với học máy đến từ lý thuyết thông tin của [Claude Shannon (1916—2001)](https://en.wikipedia.org/wiki/Claude_Shannon) và lý thuyết tính toán qua [Alan Turing (1912—1954)](https://en.wikipedia.org/wiki/Alan_Turing). Turing đặt ra câu hỏi “máy móc có thể nghĩ không?” trong bài báo nổi tiếng của mình * Computing Machinery and Intelligence* :cite:`Turing.1950`. Trong những gì ông mô tả là thử nghiệm Turing, một cỗ máy có thể được coi là *thông minh* nếu người đánh giá của con người khó phân biệt giữa các câu trả lời với một máy và một con người dựa trên tương tác văn bản. 

Một ảnh hưởng khác có thể được tìm thấy trong khoa học thần kinh và tâm lý học. Rốt cuộc, con người thể hiện rõ hành vi thông minh. Do đó, chỉ hợp lý để hỏi liệu người ta có thể giải thích và có thể đảo ngược khả năng này. Một trong những thuật toán lâu đời nhất lấy cảm hứng từ thời trang này được xây dựng bởi [Donald Hebb (1904—1985)](https://en.wikipedia.org/wiki/Donald_O._Hebb). Trong cuốn sách đột phá của mình * Tổ chức hành vi* :cite:`Hebb.Hebb.1949`, ông khẳng định rằng các tế bào thần kinh học bằng cách củng cố tích cực. Điều này được biết đến như là quy tắc học tập Hebbian. Đây là nguyên mẫu của thuật toán học perceptron của Rosenblatt và nó đã đặt nền móng của nhiều thuật toán gốc gradient ngẫu nhiên làm nền tảng học sâu ngày nay: củng cố hành vi mong muốn và giảm bớt hành vi không mong muốn để có được cài đặt tốt của các thông số trong mạng thần kinh. 

Cảm hứng sinh học là những gì đã cho * mạng thần kinh * tên của họ. Trong hơn một thế kỷ (có niên đại từ các mô hình của Alexander Bain, 1873 và James Sherrington, 1890), các nhà nghiên cứu đã cố gắng lắp ráp các mạch tính toán giống với mạng lưới các tế bào thần kinh tương tác. Theo thời gian, việc giải thích sinh học đã trở nên ít theo nghĩa đen hơn nhưng cái tên bị mắc kẹt. Tại trái tim của nó, nói dối một vài nguyên tắc chính có thể được tìm thấy trong hầu hết các mạng hiện nay: 

* Sự xen kẽ của các đơn vị xử lý tuyến tính và phi tuyến, thường được gọi là * lớp*.
* Việc sử dụng quy tắc chuỗi (còn được gọi là *backpropagation*) để điều chỉnh các tham số trong toàn bộ mạng cùng một lúc.

Sau những tiến bộ nhanh chóng ban đầu, nghiên cứu trong các mạng thần kinh mệt mỏi từ khoảng năm 1995 cho đến năm 2005. Điều này chủ yếu là do hai lý do. Đầu tiên, đào tạo một mạng là tính toán rất tốn kém. Trong khi bộ nhớ truy cập ngẫu nhiên rất phong phú vào cuối thế kỷ qua, sức mạnh tính toán rất khan hiếm. Thứ hai, các bộ dữ liệu tương đối nhỏ. Trên thực tế, tập dữ liệu Iris của Fisher từ năm 1932 là một công cụ phổ biến để kiểm tra hiệu quả của các thuật toán. Tập dữ liệu MNIST với 60000 chữ số viết tay của nó được coi là rất lớn. 

Với sự khan hiếm của dữ liệu và tính toán, các công cụ thống kê mạnh mẽ như phương pháp hạt nhân, cây quyết định và mô hình đồ họa tỏ ra vượt trội theo kinh nghiệm. Không giống như các mạng thần kinh, họ không cần nhiều tuần để đào tạo và cung cấp kết quả có thể dự đoán được với những đảm bảo lý thuyết mạnh mẽ. 

## Con đường đến sâu học

Phần lớn điều này đã thay đổi với sự sẵn có sẵn của một lượng lớn dữ liệu, do World Wide Web, sự ra đời của các công ty phục vụ hàng trăm triệu người dùng trực tuyến, phổ biến các cảm biến giá rẻ, chất lượng cao, lưu trữ dữ liệu giá rẻ (định luật Kryder) và tính toán giá rẻ (định luật Moore), đặc biệt là dạng GPU, ban đầu được thiết kế để chơi game trên máy tính. Đột nhiên các thuật toán và mô hình dường như không khả thi tính toán trở nên có liên quan (và ngược lại). Điều này được minh họa tốt nhất trong :numref:`tab_intro_decade`. 

:Dataset so với bộ nhớ máy tính và sức mạnh tính toán 

|Decade|Dataset|Memory|Floating point calculations per second|
|:--|:-|:-|:-|
|1970|100 (Iris)|1 KB|100 KF (Intel 8080)|
|1980|1 K (House prices in Boston)|100 KB|1 MF (Intel 80186)|
|1990|10 K (optical character recognition)|10 MB|10 MF (Intel 80486)|
|2000|10 M (web pages)|100 MB|1 GF (Intel Core)|
|2010|10 G (advertising)|1 GB|1 TF (Nvidia C2050)|
|2020|1 T (social network)|100 GB|1 PF (Nvidia DGX-2)|
:label:`tab_intro_decade`

Rõ ràng là bộ nhớ truy cập ngẫu nhiên đã không theo kịp sự tăng trưởng của dữ liệu. Đồng thời, sự gia tăng sức mạnh tính toán đã vượt xa dữ liệu có sẵn. Điều này có nghĩa là các mô hình thống kê cần phải trở nên hiệu quả hơn bộ nhớ (điều này thường đạt được bằng cách thêm phi tuyến tính) trong khi đồng thời có thể dành nhiều thời gian hơn để tối ưu hóa các thông số này, do ngân sách tính toán tăng lên. Do đó, điểm ngọt ngào trong máy học và thống kê đã chuyển từ các mô hình tuyến tính (tổng quát) và phương pháp hạt nhân sang các mạng thần kinh sâu. Đây cũng là một trong những lý do tại sao nhiều trụ cột của học sâu, chẳng hạn như nhận thức đa lớp :cite:`McCulloch.Pitts.1943`, mạng thần kinh phức tạp :cite:`LeCun.Bottou.Bengio.ea.1998`, bộ nhớ ngắn hạn dài :cite:`Hochreiter.Schmidhuber.1997` và Q-Learning :cite:`Watkins.Dayan.1992`, về cơ bản là “phát hiện lại” trong thập kỷ qua, sau khi đặt tương đối không hoạt động cho considerable đáng kể time. 

Sự tiến bộ gần đây trong các mô hình thống kê, ứng dụng, và thuật toán đôi khi đã được so sánh với vụ nổ Cambri: một khoảnh khắc tiến bộ nhanh chóng trong quá trình tiến hóa của các loài. Thật vậy, nhà nước của nghệ thuật không chỉ là một hậu quả đơn thuần của các nguồn lực có sẵn, áp dụng cho các thuật toán hàng thập kỷ cũ. Lưu ý rằng danh sách dưới đây hầu như không làm trầy xước bề mặt của những ý tưởng đã giúp các nhà nghiên cứu đạt được tiến bộ to lớn trong thập kỷ qua. 

* Các phương pháp mới để kiểm soát công suất, chẳng hạn như * dropout* :cite:`Srivastava.Hinton.Krizhevsky.ea.2014`, đã giúp giảm thiểu nguy cơ quá mức. Điều này đạt được bằng cách áp dụng tiếng ồn tiêm :cite:`Bishop.1995` trên toàn mạng thần kinh, thay thế trọng lượng bằng các biến ngẫu nhiên cho mục đích đào tạo.
* Các cơ chế chú ý đã giải quyết một vấn đề thứ hai đã cản trở số liệu thống kê trong hơn một thế kỷ: làm thế nào để tăng bộ nhớ và độ phức tạp của một hệ thống mà không làm tăng số lượng các tham số có thể học được. Các nhà nghiên cứu đã tìm thấy một giải pháp thanh lịch bằng cách sử dụng những gì chỉ có thể được xem như một cấu trúc con trỏ có thể học được :cite:`Bahdanau.Cho.Bengio.2014`. Thay vì phải nhớ toàn bộ một chuỗi văn bản, ví dụ, đối với dịch máy trong một biểu diễn chiều cố định, tất cả những gì cần được lưu trữ là một con trỏ đến trạng thái trung gian của quá trình dịch thuật. Điều này cho phép tăng độ chính xác đáng kể cho các chuỗi dài, vì mô hình không còn cần phải nhớ toàn bộ chuỗi trước khi bắt đầu tạo ra một chuỗi mới.
* Thiết kế đa giai đoạn, ví dụ, thông qua các mạng bộ nhớ :cite:`Sukhbaatar.Weston.Fergus.ea.2015` và lập trình viên thần kinh :cite:`Reed.De-Freitas.2015` cho phép các mô hình thống kê mô tả các phương pháp lặp đi lặp lại để lập luận. Những công cụ này cho phép một trạng thái nội bộ của mạng thần kinh sâu được sửa đổi nhiều lần, do đó thực hiện các bước tiếp theo trong một chuỗi lý luận, tương tự như cách một bộ xử lý có thể sửa đổi bộ nhớ cho một tính toán.
* Một phát triển quan trọng khác là phát minh ra các mạng đối thủ thế hệ :cite:`Goodfellow.Pouget-Abadie.Mirza.ea.2014`. Theo truyền thống, các phương pháp thống kê để ước tính mật độ và các mô hình tạo tập trung vào việc tìm kiếm các phân phối xác suất thích hợp và (thường là gần đúng) thuật toán lấy mẫu từ chúng. Kết quả là, các thuật toán này phần lớn bị giới hạn bởi sự thiếu linh hoạt vốn có trong các mô hình thống kê. Sự đổi mới quan trọng trong các mạng đối thủ thế hệ là thay thế bộ lấy mẫu bằng một thuật toán tùy ý với các tham số khác biệt. Chúng sau đó được điều chỉnh theo cách mà người phân biệt đối xử (hiệu quả là thử nghiệm hai mẫu) không thể phân biệt giả với dữ liệu thực. Thông qua khả năng sử dụng các thuật toán tùy ý để tạo ra dữ liệu, nó đã mở ra ước tính mật độ cho nhiều kỹ thuật khác nhau. Ví dụ về Zebras phi nước đại :cite:`Zhu.Park.Isola.ea.2017` và của những người nổi tiếng giả mạo phải đối mặt :cite:`Karras.Aila.Laine.ea.2017` đều là lời khai cho sự tiến bộ này. Ngay cả những người vẽ tranh nghiệp dư cũng có thể tạo ra những hình ảnh thực tế dựa trên các bản phác thảo mô tả cách bố trí của một cảnh trông giống như :cite:`Park.Liu.Wang.ea.2019`.
* Trong nhiều trường hợp, một GPU duy nhất là không đủ để xử lý một lượng lớn dữ liệu có sẵn để đào tạo. Trong thập kỷ qua, khả năng xây dựng các thuật toán đào tạo song song và phân tán đã được cải thiện đáng kể. Một trong những thách thức chính trong việc thiết kế các thuật toán có thể mở rộng là việc làm việc tối ưu hóa học tập sâu, gốc gradient ngẫu nhiên, dựa vào các minibatches dữ liệu tương đối nhỏ sẽ được xử lý. Đồng thời, các lô nhỏ hạn chế hiệu quả của GPU. Do đó, đào tạo trên 1024 GPU với kích thước minibatch, giả sử 32 hình ảnh mỗi lô lên tới một minibatch tổng hợp khoảng 32000 hình ảnh. Công việc gần đây, đầu tiên là Li :cite:`Li.2017`, và sau đó là :cite:`You.Gitman.Ginsburg.2017` và :cite:`Jia.Song.He.ea.2018` đã đẩy kích thước lên đến 64000 quan sát, giảm thời gian đào tạo cho mô hình ResNet-50 trên bộ dữ liệu ImageNet xuống chưa đầy 7 phút. Để so sánh, thời gian đào tạo ban đầu được đo theo thứ tự ngày.
* Khả năng tính toán song song cũng đã đóng góp khá quan trọng để tiến bộ trong học tập củng cố, ít nhất là bất cứ khi nào mô phỏng là một lựa chọn. Điều này đã dẫn đến những tiến bộ đáng kể trong các máy tính đạt được hiệu suất siêu phàm trong các game Go, Atari, Starcraft, và trong các mô phỏng vật lý (ví dụ, sử dụng MuJoCo). Xem ví dụ, :cite:`Silver.Huang.Maddison.ea.2016` để biết mô tả về cách đạt được điều này trong AlphaGo. Tóm lại, việc học tăng cường hoạt động tốt nhất nếu có nhiều (trạng thái, hành động, phần thưởng) gấp ba lần, tức là, bất cứ khi nào có thể thử nhiều thứ để tìm hiểu cách chúng liên quan đến nhau. Mô phỏng cung cấp một đại lộ như vậy.
* Các khuôn khổ học sâu đã đóng một vai trò quan trọng trong việc phổ biến các ý tưởng. Thế hệ khung đầu tiên cho phép mô hình hóa dễ dàng bao gồm [Caffe](https://github.com/BVLC/caffe), [Torch](https://github.com/torch) và [Theano](https://github.com/Theano/Theano). Nhiều bài báo tinh dịch được viết bằng các công cụ này. Đến bây giờ, chúng đã được thay thế bởi [TensorFlow](https://github.com/tensorflow/tensorflow) (thường được sử dụng thông qua API cấp cao [Keras](https://github.com/keras-team/keras)), [CNTK](https://github.com/Microsoft/CNTK), [Caffe 2](https://github.com/caffe2/caffe2), và [Apache MXNet](https://github.com/apache/incubator-mxnet). Thế hệ công cụ thứ ba, cụ thể là các công cụ bắt buộc để học sâu, được cho là dẫn đầu bởi [Chainer](https://github.com/chainer/chainer), sử dụng một cú pháp tương tự như Python NumPy để mô tả các mô hình. Ý tưởng này đã được thông qua bởi cả [PyTorch](https://github.com/pytorch/pytorch), [Gluon API](https://github.com/apache/incubator-mxnet) của MXNet và [Jax](https://github.com/google/jax).

Việc phân chia lao động giữa các nhà nghiên cứu hệ thống xây dựng các công cụ tốt hơn và các nhà mô hình thống kê xây dựng mạng thần kinh tốt hơn đã đơn giản hóa mọi thứ rất nhiều. Ví dụ, đào tạo một mô hình hồi quy logistic tuyến tính từng là một vấn đề bài tập về nhà không tầm thường, xứng đáng để cung cấp cho sinh viên máy học mới tiến sĩ tại Đại học Carnegie Mellon vào năm 2014. Đến bây giờ, nhiệm vụ này có thể được thực hiện với ít hơn 10 dòng mã, đặt nó vững chắc vào nắm bắt các lập trình viên. 

## Câu chuyện thành công

AI có một lịch sử lâu dài của việc cung cấp kết quả sẽ rất khó để thực hiện nếu không. Ví dụ, các hệ thống phân loại thư sử dụng nhận dạng ký tự quang học đã được triển khai từ những năm 1990. Đây là, sau tất cả, nguồn gốc của tập dữ liệu MNIST nổi tiếng của các chữ số viết tay. Điều tương tự cũng áp dụng cho việc đọc séc cho tiền gửi ngân hàng và ghi điểm tín dụng của người nộp đơn. Các giao dịch tài chính được kiểm tra gian lận tự động. Điều này tạo thành xương sống của nhiều hệ thống thanh toán thương mại điện tử, chẳng hạn như PayPal, Stripe, AliPay, WeChat, Apple, Visa và MasterCard. Các chương trình máy tính cho cờ vua đã cạnh tranh trong nhiều thập kỷ. Máy học nguồn cấp dữ liệu tìm kiếm, khuyến nghị, cá nhân hóa và xếp hạng trên Internet. Nói cách khác, học máy rất phổ biến, mặc dù thường bị che giấu khỏi tầm nhìn. 

Chỉ gần đây AI mới ở trong ánh đèn sân khấu, chủ yếu là do các giải pháp cho các vấn đề được coi là khó chữa trước đây và có liên quan trực tiếp đến người tiêu dùng. Nhiều tiến bộ như vậy được quy cho học sâu. 

* Các trợ lý thông minh, chẳng hạn như Siri của Apple, Alexa của Amazon và trợ lý của Google, có thể trả lời các câu hỏi nói với mức độ chính xác hợp lý. Điều này bao gồm các nhiệm vụ quan trọng như bật công tắc đèn (một lợi ích cho người khuyết tật) để thực hiện các cuộc hẹn của thợ cắt tóc và cung cấp hộp thoại hỗ trợ điện thoại. Đây có thể là dấu hiệu đáng chú ý nhất cho thấy AI đang ảnh hưởng đến cuộc sống của chúng ta.
* Một thành phần quan trọng trong trợ lý kỹ thuật số là khả năng nhận dạng giọng nói chính xác. Dần dần độ chính xác của các hệ thống như vậy đã tăng lên đến mức chúng đạt đến độ chẵn lẻ của con người đối với một số ứng dụng nhất định :cite:`Xiong.Wu.Alleva.ea.2018`.
* Nhận dạng đối tượng tương tự như vậy đã đi một chặng đường dài. Ước tính đối tượng trong một bức tranh là một nhiệm vụ khá khó khăn trong năm 2010. Trên các nhà nghiên cứu điểm chuẩn ImageNet từ NEC Labs và Đại học Illinois tại Urbana-Champaign đã đạt được tỷ lệ lỗi top-5 là 28% :cite:`Lin.Lv.Zhu.ea.2010`. Đến năm 2017, tỷ lệ lỗi này đã giảm xuống còn 2,25% :cite:`Hu.Shen.Sun.2018`. Tương tự như vậy, kết quả tuyệt đẹp đã đạt được để xác định các loài chim hoặc chẩn đoán ung thư da.
* Trò chơi từng là một pháo đài của trí thông minh của con người. Bắt đầu từ TD-Gammon, một chương trình chơi backgammon bằng cách sử dụng học tăng cường chênh lệch thời gian, tiến trình thuật toán và tính toán đã dẫn đến các thuật toán cho một loạt các ứng dụng. Không giống như backgammon, cờ vua có một không gian trạng thái phức tạp hơn nhiều và tập hợp các hành động. DeepBlue đánh bại Garry Kasparov sử dụng song song lớn, phần cứng chuyên dụng và tìm kiếm hiệu quả thông qua cây trò chơi :cite:`Campbell.Hoane-Jr.Hsu.2002`. Đi vẫn còn khó khăn hơn, do không gian trạng thái khổng lồ của nó. AlphaGo đạt đến độ chẵn lẻ của con người vào năm 2015, sử dụng học sâu kết hợp với lấy mẫu cây Monte Carlo :cite:`Silver.Huang.Maddison.ea.2016`. Thách thức trong Poker là không gian nhà nước lớn và nó không được quan sát đầy đủ (chúng tôi không biết thẻ của đối thủ). Libratus vượt quá hiệu suất của con người trong Poker sử dụng chiến lược có cấu trúc hiệu quả :cite:`Brown.Sandholm.2017`. Điều này minh họa sự tiến bộ ấn tượng trong các trò chơi và thực tế là các thuật toán tiên tiến đóng một phần quan trọng trong đó.
* Một dấu hiệu khác của sự tiến bộ trong AI là sự ra đời của ô tô và xe tải tự lái. Mặc dù quyền tự chủ hoàn toàn chưa hoàn toàn nằm trong tầm tay, nhưng tiến bộ tuyệt vời đã được thực hiện theo hướng này, với các công ty như Tesla, NVIDIA và các sản phẩm vận chuyển Waymo cho phép ít nhất một phần tự chủ. Điều làm cho quyền tự chủ đầy đủ trở nên khó khăn là lái xe thích hợp đòi hỏi khả năng nhận thức, lý luận và kết hợp các quy tắc vào một hệ thống. Hiện nay, học sâu được sử dụng chủ yếu trong khía cạnh thị giác máy tính của những vấn đề này. Phần còn lại được điều chỉnh rất nhiều bởi các kỹ sư.

Một lần nữa, danh sách trên hầu như không làm trầy xước bề mặt của nơi máy học đã ảnh hưởng đến các ứng dụng thực tế. Ví dụ, robot, hậu cần, sinh học tính toán, vật lý hạt và thiên văn học nợ một số tiến bộ ấn tượng nhất gần đây của họ ít nhất là trong các phần của học máy. Do đó, học máy đang trở thành một công cụ phổ biến cho các kỹ sư và nhà khoa học. 

Thông thường, câu hỏi về ngày tận thế AI, hoặc điểm kỳ dị AI đã được nêu ra trong các bài viết phi kỹ thuật về AI. Nỗi sợ hãi là bằng cách nào đó hệ thống học máy sẽ trở nên tình cảm và quyết định độc lập với các lập trình viên (và thạc sĩ) của họ về những thứ ảnh hưởng trực tiếp đến sinh kế của con người. Ở một mức độ nào đó, AI đã ảnh hưởng đến sinh kế của con người một cách ngay lập tức: tính tín dụng được đánh giá tự động, các phi công tự động chủ yếu điều hướng phương tiện, quyết định về việc có cấp bảo lãnh sử dụng dữ liệu thống kê làm đầu vào hay không. Phù phiếm hơn, chúng ta có thể yêu cầu Alexa bật máy pha cà phê. 

May mắn thay, chúng ta còn xa một hệ thống AI có cảm giác sẵn sàng thao túng những người tạo ra con người (hoặc đốt cà phê của họ). Đầu tiên, các hệ thống AI được thiết kế, đào tạo và triển khai theo một cách cụ thể, hướng đến mục tiêu. Mặc dù hành vi của họ có thể tạo ra ảo giác về trí thông minh chung, nhưng đó là sự kết hợp của các quy tắc, heuristics và các mô hình thống kê làm nền tảng cho thiết kế. Thứ hai, hiện tại các công cụ cho * trí tuệ tổng quát nhân tạo* đơn giản là không tồn tại có thể cải thiện bản thân, lý do về bản thân và có thể sửa đổi, mở rộng và cải thiện kiến trúc của riêng họ trong khi cố gắng giải quyết các nhiệm vụ chung. 

Một mối quan tâm cấp bách hơn nhiều là cách AI đang được sử dụng trong cuộc sống hàng ngày của chúng ta. Có khả năng nhiều nhiệm vụ menial được thực hiện bởi các tài xế xe tải và trợ lý cửa hàng có thể và sẽ được tự động hóa. Robot nông trại có thể sẽ giảm chi phí cho canh tác hữu cơ nhưng chúng cũng sẽ tự động hóa các hoạt động thu hoạch. Giai đoạn này của cuộc cách mạng công nghiệp có thể có hậu quả sâu sắc đối với những phần lớn của xã hội, vì tài xế xe tải và trợ lý cửa hàng là một số công việc phổ biến nhất ở nhiều quốc gia. Hơn nữa, các mô hình thống kê, khi áp dụng mà không cần chăm sóc có thể dẫn đến thiên vị chủng tộc, giới tính hoặc tuổi tác và nêu ra những lo ngại hợp lý về sự công bằng về thủ tục nếu tự động thúc đẩy các quyết định do hậu quả. Điều quan trọng là đảm bảo rằng các thuật toán này được sử dụng cẩn thận. Với những gì chúng ta biết ngày nay, điều này gây ra cho chúng ta một mối quan tâm cấp bách hơn nhiều so với tiềm năng của siêu trí thông minh ác độc để tiêu diệt nhân loại. 

## các điểm

Cho đến nay, chúng ta đã nói về máy học một cách rộng rãi, vừa là một nhánh của AI vừa là cách tiếp cận với AI. Mặc dù deep learning là một tập hợp con của machine learning, nhưng tập hợp các thuật toán và ứng dụng chóng mặt gây khó khăn cho việc đánh giá các thành phần cụ thể cho học sâu có thể là gì. Điều này khó khăn như cố gắng pin xuống các thành phần cần thiết cho pizza vì hầu hết mọi thành phần đều có thể thay thế. 

Như chúng tôi đã mô tả, machine learning có thể sử dụng dữ liệu để tìm hiểu các biến đổi giữa đầu vào và đầu ra, chẳng hạn như chuyển đổi âm thanh thành văn bản trong nhận dạng giọng nói. Khi làm như vậy, thường cần phải đại diện cho dữ liệu theo cách phù hợp với các thuật toán để biến đổi các biểu diễn như vậy thành đầu ra.
*Học sâu* là * sâu* theo nghĩa chính xác
rằng các mô hình của nó học nhiều * lớp* biến đổi, trong đó mỗi lớp cung cấp biểu diễn ở một cấp độ. Ví dụ, các lớp gần đầu vào có thể đại diện cho các chi tiết cấp thấp của dữ liệu, trong khi các lớp gần đầu ra phân loại có thể đại diện cho các khái niệm trừu tượng hơn được sử dụng để phân biệt đối xử. Vì học đại diện *nhằm mục đích tìm ra bản thân đại diện, học sâu có thể được gọi là học đại diện đa cấp. 

Các vấn đề mà chúng ta đã thảo luận cho đến nay, chẳng hạn như học từ tín hiệu âm thanh thô, giá trị pixel thô của hình ảnh hoặc ánh xạ giữa các câu có độ dài tùy ý và các đối tác của chúng bằng ngoại ngữ, là những vấn đề mà học sâu vượt trội và nơi các phương pháp học máy truyền thống chùn bước. Nó chỉ ra rằng các mô hình nhiều lớp này có khả năng giải quyết dữ liệu nhận thức cấp thấp theo cách mà các công cụ trước đây không thể. Có thể cho là điểm chung quan trọng nhất trong các phương pháp học sâu là việc sử dụng * đào tạo end-to-end*. Đó là, thay vì lắp ráp một hệ thống dựa trên các thành phần được điều chỉnh riêng lẻ, người ta xây dựng hệ thống và sau đó điều chỉnh hiệu suất của chúng cùng nhau. Ví dụ, trong tầm nhìn máy tính các nhà khoa học sử dụng để tách quá trình kỹ thuật *tính năng* khỏi quá trình xây dựng các mô hình học máy. Máy dò cạnh Canny :cite:`Canny.1987` và máy trích xuất tính năng SIFT của Lowe :cite:`Lowe.2004` trị vì tối cao trong hơn một thập kỷ như các thuật toán để lập bản đồ hình ảnh thành các vectơ tính năng. Trong những ngày đã qua, phần quan trọng của việc áp dụng máy học vào những vấn đề này bao gồm việc đưa ra các cách được thiết kế thủ công để chuyển đổi dữ liệu thành một số hình thức phù hợp với các mô hình nông. Thật không may, chỉ có rất ít mà con người có thể thực hiện bằng sự khéo léo so với một đánh giá nhất quán trên hàng triệu lựa chọn được thực hiện tự động bởi một thuật toán. Khi học sâu tiếp quản, các bộ chiết xuất tính năng này được thay thế bằng các bộ lọc được điều chỉnh tự động, mang lại độ chính xác vượt trội. 

Do đó, một lợi thế chính của học sâu là nó thay thế không chỉ các mô hình nông ở cuối các đường ống học tập truyền thống, mà còn là quá trình kỹ thuật tính năng tốn nhiều lao động. Hơn nữa, bằng cách thay thế phần lớn tiền xử lý miền cụ thể, deep learning đã loại bỏ nhiều ranh giới mà trước đây tách tầm nhìn máy tính, nhận dạng giọng nói, xử lý ngôn ngữ tự nhiên, tin học y tế và các lĩnh vực ứng dụng khác, cung cấp một bộ công cụ thống nhất để giải quyết đa dạng vấn đề. 

Ngoài đào tạo từ đầu đến cuối, chúng tôi đang trải qua một sự chuyển đổi từ mô tả thống kê tham số sang các mô hình hoàn toàn không tham số. Khi dữ liệu khan hiếm, người ta cần dựa vào việc đơn giản hóa các giả định về thực tế để có được các mô hình hữu ích. Khi dữ liệu dồi dào, điều này có thể được thay thế bằng các mô hình không tham số phù hợp với thực tế chính xác hơn. Ở một mức độ nào đó, điều này phản ánh sự tiến bộ mà vật lý trải qua vào giữa thế kỷ trước với sự sẵn có của máy tính. Thay vì giải các xấp xỉ tham số về cách điện tử hoạt động bằng tay, giờ đây người ta có thể dùng đến các mô phỏng số của các phương trình vi phân từng phần liên quan. Điều này đã dẫn đến các mô hình chính xác hơn nhiều, mặc dù thường xuyên với chi phí giải thích. 

Một điểm khác biệt khác đối với công việc trước đây là việc chấp nhận các giải pháp tối ưu, giải quyết các vấn đề tối ưu hóa phi tuyến không lồi và sẵn sàng thử mọi thứ trước khi chứng minh chúng. Chủ nghĩa kinh nghiệm mới được tìm thấy này trong việc đối phó với các vấn đề thống kê, kết hợp với một dòng tài năng nhanh chóng đã dẫn đến tiến bộ nhanh chóng của các thuật toán thực tế, mặc dù trong nhiều trường hợp với chi phí sửa đổi và tái phát minh các công cụ tồn tại trong nhiều thập kỷ. 

Cuối cùng, cộng đồng deep learning tự hào về việc chia sẻ các công cụ trên các ranh giới học thuật và doanh nghiệp, phát hành nhiều thư viện tuyệt vời, mô hình thống kê và các mạng được đào tạo làm nguồn mở. Theo tinh thần này, các máy tính xách tay hình thành cuốn sách này có sẵn miễn phí để phân phối và sử dụng. Chúng tôi đã làm việc chăm chỉ để giảm bớt các rào cản tiếp cận để mọi người tìm hiểu về học sâu và chúng tôi hy vọng rằng độc giả của chúng tôi sẽ được hưởng lợi từ việc này. 

## Tóm tắt

* Machine learning nghiên cứu cách các hệ thống máy tính có thể tận dụng trải nghiệm (thường là dữ liệu) để cải thiện hiệu suất tại các tác vụ cụ thể. Nó kết hợp các ý tưởng từ thống kê, khai thác dữ liệu và tối ưu hóa. Thông thường, nó được sử dụng như một phương tiện triển khai các giải pháp AI.
* Là một lớp học máy học, việc học đại diện tập trung vào cách tự động tìm ra cách thể hiện dữ liệu thích hợp. Deep learning là học đại diện đa cấp thông qua việc học nhiều lớp biến đổi.
* Deep learning thay thế không chỉ các mô hình nông ở cuối các đường ống học máy truyền thống, mà còn là quá trình kỹ thuật tính năng tốn nhiều lao động. 
* Phần lớn tiến bộ gần đây trong học sâu đã được kích hoạt bởi sự phong phú của dữ liệu phát sinh từ các cảm biến giá rẻ và các ứng dụng quy mô Internet, và bởi sự tiến bộ đáng kể trong tính toán, chủ yếu là thông qua GPU.
* Tối ưu hóa toàn bộ hệ thống là một thành phần quan trọng trong việc đạt được hiệu suất cao. Sự sẵn có của các khung học sâu hiệu quả đã giúp thiết kế và triển khai điều này dễ dàng hơn đáng kể.

## Bài tập

1. Những phần nào của mã mà bạn hiện đang viết có thể được “học”, tức là, được cải thiện bằng cách học và tự động xác định các lựa chọn thiết kế được thực hiện trong mã của bạn? Mã của bạn có bao gồm các lựa chọn thiết kế heuristic không?
1. Những vấn đề mà bạn gặp phải có nhiều ví dụ về cách giải quyết chúng, nhưng không có cách nào cụ thể để tự động hóa chúng? Đây có thể là những ứng cử viên chính để sử dụng deep learning.
1. Xem sự phát triển của AI như một cuộc cách mạng công nghiệp mới, mối quan hệ giữa các thuật toán và dữ liệu là gì? Nó có tương tự như động cơ hơi nước và than? Sự khác biệt cơ bản là gì?
1. Bạn có thể áp dụng phương pháp đào tạo từ đầu đến cuối ở đâu khác, chẳng hạn như trong :numref:`fig_ml_loop`, vật lý, kỹ thuật và kinh tế học?

[Discussions](https://discuss.d2l.ai/t/22)
